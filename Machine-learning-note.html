<!-- build time:Sat Mar 13 2021 11:25:23 GMT+0800 (GMT+08:00) --><!DOCTYPE html><html class="theme-next pisces use-motion" lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="theme-color" content="#222"><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script>(adsbygoogle=window.adsbygoogle||[]).push({google_ad_client:"ca-pub-2723999557244339",enable_page_level_ads:!0})</script><script async custom-element="amp-auto-ads" src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js"></script><script src="//cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><link href="//cdn.jsdelivr.net/npm/pace-js@1.0.2/themes/blue/pace-theme-minimal.css" rel="stylesheet"><script>!function(e,t,o,c,i,a,n){e.DaoVoiceObject=i,e[i]=e[i]||function(){(e[i].q=e[i].q||[]).push(arguments)},e[i].l=1*new Date,a=t.createElement(o),n=t.getElementsByTagName(o)[0],a.async=1,a.src=c,a.charset="utf-8",n.parentNode.insertBefore(a,n)}(window,document,"script",("https:"==document.location.protocol?"https:":"http:")+"//widget.daovoice.io/widget/0f81ff2f.js","daovoice"),daovoice("init",{app_id:"6f8389c1"}),daovoice("update")</script><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="https://fonts.googleapis.com/css?family=EB+Garamond:400,400i,700,700i|Noto+Serif+SC:400,500,700&display=swap&subset=chinese-simplified" rel="stylesheet"><link href="//cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css"><link rel="apple-touch-icon" sizes="180x180" href="https://cdn.jsdelivr.net/gh/Qikaile/cdn/img/apple-touch-icon.png?v=5.1.4"><link rel="icon" type="image/png" sizes="32x32" href="https://cdn.jsdelivr.net/gh/Qikaile/cdn/img/favicon-32x32.png?v=5.1.4"><link rel="icon" type="image/png" sizes="16x16" href="https://cdn.jsdelivr.net/gh/Qikaile/cdn/img/favicon-16x16.png?v=5.1.4"><meta name="keywords" content="机器学习,"><link rel="alternate" href="/atom.xml" title="天镜云生" type="application/atom+xml"><meta name="description" content="机器学习概述什么是机器学习机器学习是从数据中自动分析获得规律(模型)，并利用规律对未知数据进行预测。机器学习算法分类监督学习目标值：类别—分类问题KNN、贝叶斯分类、决策树与随机森林、逻辑回归目标值：连续型数据—回归问题线性回归、岭回归无监督学习目标值：无聚类K-means机器学习开发流程1）获取数据2）数据处理3）特征工程4）机器学习算法训练—模型5）模型评估6）应用特征工程数据集数据———数据"><meta property="og:type" content="article"><meta property="og:title" content="机器学习课程"><meta property="og:url" content="https://qikaile.us/Machine-learning-note.html"><meta property="og:site_name" content="天镜云生"><meta property="og:description" content="机器学习概述什么是机器学习机器学习是从数据中自动分析获得规律(模型)，并利用规律对未知数据进行预测。机器学习算法分类监督学习目标值：类别—分类问题KNN、贝叶斯分类、决策树与随机森林、逻辑回归目标值：连续型数据—回归问题线性回归、岭回归无监督学习目标值：无聚类K-means机器学习开发流程1）获取数据2）数据处理3）特征工程4）机器学习算法训练—模型5）模型评估6）应用特征工程数据集数据———数据"><meta property="og:image" content="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9B%BE%E5%BA%93/1fd68fd47f13030f88eecd632db3a4db.png"><meta property="og:image" content="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9B%BE%E5%BA%93/%E8%A1%A8%E6%A0%BC1.JPG"><meta property="og:image" content="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9B%BE%E5%BA%93/Figure_1.png"><meta property="og:image" content="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9B%BE%E5%BA%93/2.JPG"><meta property="og:image" content="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9B%BE%E5%BA%93/123.jpg"><meta property="og:image" content="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9B%BE%E5%BA%93/%E7%94%B5%E5%BD%B1%E7%B1%BB%E5%9E%8B%E5%88%86%E6%9E%90.png"><meta property="og:image" content="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9B%BE%E5%BA%93/%E7%94%B5%E5%BD%B1%E8%B7%9D%E7%A6%BB%E8%AE%A1%E7%AE%97.png"><meta property="og:image" content="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9B%BE%E5%BA%93/%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81%E8%BF%87%E7%A8%8B.png"><meta property="og:image" content="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9B%BE%E5%BA%93/1234.JPG"><meta property="og:image" content="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9B%BE%E5%BA%93/8d16d796670bb4901c7a4c13ca3aa1fa.jpg"><meta property="og:image" content="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9B%BE%E5%BA%93/12.png"><meta property="og:image" content="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9B%BE%E5%BA%93/13.png"><meta property="og:image" content="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9B%BE%E5%BA%93/Figure_1-1583391690020.png"><meta property="og:image" content="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9B%BE%E5%BA%93/%E9%93%B6%E8%A1%8C%E8%B4%B7%E6%AC%BE%E6%95%B0%E6%8D%AE.png"><meta property="og:image" content="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9B%BE%E5%BA%93/%E6%8D%95%E8%8E%B7.JPG"><meta property="og:image" content="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9B%BE%E5%BA%93/image-20200302161335608.png"><meta property="og:image" content="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9B%BE%E5%BA%93/%E9%9D%9E%E7%BA%BF%E6%80%A7%E5%85%B3%E7%B3%BB.png"><meta property="og:image" content="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9B%BE%E5%BA%93/%E6%8D%9F%E5%A4%B1%E8%A1%8C%E6%95%B0%E6%B1%82%E8%A7%A31.png"><meta property="og:image" content="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9B%BE%E5%BA%93/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E5%85%AC%E5%BC%8F.png"><meta property="og:image" content="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9B%BE%E5%BA%93/%E5%8D%95%E5%8F%98%E9%87%8F%E7%9A%84%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D.png"><meta property="og:image" content="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9B%BE%E5%BA%93/%E5%A4%9A%E5%8F%98%E9%87%8F%E7%9A%84%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D.png"><meta property="og:image" content="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9B%BE%E5%BA%93/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%BC%98%E5%8C%96%E5%8A%A8%E6%80%81%E5%9B%BE.gif"><meta property="og:image" content="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9B%BE%E5%BA%93/Figure_1-1583217802407.png"><meta property="og:image" content="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9B%BE%E5%BA%93/Figure_19.png"><meta property="og:image" content="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9B%BE%E5%BA%93/%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B%E5%92%8C%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E5%AF%B9%E6%AF%94.png"><meta property="og:image" content="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9B%BE%E5%BA%93/031938497051596.png"><meta property="og:image" content="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9B%BE%E5%BA%93/20190720210756791.png"><meta property="og:image" content="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9B%BE%E5%BA%93/20190720205912608.png"><meta property="og:image" content="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9B%BE%E5%BA%93/20190720210220248.png"><meta property="og:image" content="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9B%BE%E5%BA%93/20190720212516689.png"><meta property="og:image" content="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9B%BE%E5%BA%93/20181103202221367.jpg"><meta property="og:image" content="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9B%BE%E5%BA%93/20181103202237148.jpg"><meta property="og:image" content="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9B%BE%E5%BA%93/Figure_1-1583378255882.png"><meta property="og:image" content="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9B%BE%E5%BA%93/image-20200304105302691.png"><meta property="og:image" content="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9B%BE%E5%BA%93/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E8%BF%90%E7%AE%97%E8%BF%87%E7%A8%8B.png"><meta property="og:image" content="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9B%BE%E5%BA%93/%E5%8D%95%E4%B8%AA%E6%8D%9F%E5%A4%B1%E8%A7%A3%E9%87%8A.png"><meta property="og:image" content="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9B%BE%E5%BA%93/%E6%8D%9F%E5%A4%B1%E8%AE%A1%E7%AE%97%E8%BF%87%E7%A8%8B.png"><meta property="og:image" content="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9B%BE%E5%BA%93/%E7%B2%BE%E7%A1%AE%E7%8E%87.png"><meta property="og:image" content="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9B%BE%E5%BA%93/%E5%8F%AC%E5%9B%9E%E7%8E%87.png"><meta property="og:image" content="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9B%BE%E5%BA%93/%E7%B2%BE%E7%A1%AE%E7%8E%87%E4%B8%8E%E5%8F%AC%E5%9B%9E%E7%8E%87%E7%90%86%E8%A7%A3.png"><meta property="og:image" content="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9B%BE%E5%BA%93/20180531113257203.png"><meta property="og:image" content="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9B%BE%E5%BA%93/20190611144159116.png"><meta property="og:image" content="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9B%BE%E5%BA%93/Figure_111.png"><meta property="og:image" content="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9B%BE%E5%BA%93/20180531115939413.png"><meta property="og:image" content="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9B%BE%E5%BA%93/ROC.png"><meta property="og:image" content="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9B%BE%E5%BA%93/Figure_11.png"><meta property="og:image" content="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9B%BE%E5%BA%93/20180309191310524.png"><meta property="og:image" content="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9B%BE%E5%BA%93/20180309191506788.png"><meta property="og:image" content="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9B%BE%E5%BA%93/K-means%E5%A6%82%E4%BD%95%E8%81%9A%E7%B1%BB%E6%95%88%E6%9E%9C.png"><meta property="og:image" content="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9B%BE%E5%BA%93/K-means%E8%BF%87%E7%A8%8B%E5%88%86%E6%9E%90.png"><meta property="og:image" content="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9B%BE%E5%BA%93/%E8%BD%AE%E5%BB%93%E7%B3%BB%E6%95%B0%E5%88%86%E6%9E%90.png"><meta property="og:image" content="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9B%BE%E5%BA%93/Figure_15.png"><meta property="article:published_time" content="2020-03-27T16:00:00.000Z"><meta property="article:modified_time" content="2021-01-20T10:56:02.825Z"><meta property="article:author" content="TJYS"><meta property="article:tag" content="机器学习"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9B%BE%E5%BA%93/1fd68fd47f13030f88eecd632db3a4db.png"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Pisces",version:"5.1.4",sidebar:{position:"left",display:"post",offset:12,b2t:!1,scrollpercent:!0,onmobile:!0},fancybox:!1,tabs:!0,motion:{enable:!0,async:!1,transition:{post_block:"flipYIn",post_header:"perspectiveRightIn",post_body:"perspectiveLeftIn",coll_header:"perspectiveDownIn",sidebar:"perspectiveUpIn"}},duoshuo:{userId:"0",author:"博主"},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><link rel="canonical" href="https://qikaile.us/Machine-learning-note.html"><script></script><title>机器学习课程 | 天镜云生</title><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/qikaile/cdn@1.1/js/bubble.js"></script><meta name="generator" content="Hexo 4.2.0"></head><body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans"><script type="text/javascript" src="//libs.baidu.com/jquery/1.8.3/jquery.min.js"></script><script type="text/javascript">var windowWidth=$(window).width();windowWidth>480&&document.write('<script type="text/javascript" src="https://cdn.jsdelivr.net/gh/qikaile/cdn@1.1/js/snow.js"><\/script>')</script><amp-auto-ads type="adsense" data-ad-client="ca-pub-7008389072962799"></amp-auto-ads><div class="container sidebar-position-left page-post-detail"><div class="headband"></div><a href="https://github.com/Qikaile" target="_blank" rel="noopener external nofollow noreferrer" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513;color:#fff;position:absolute;top:0;border:0;right:0" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">天镜云生</span> <span class="logo-line-after"><i></i></span></a></div><h1 class="site-subtitle" itemprop="description">云生的个人博客</h1></div><div class="site-nav-toggle"><button><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>归档</a></li><li class="menu-item menu-item-开往"><a href="https://travellings.now.sh/" target="_blank" rel="section noopener external nofollow noreferrer"><i class="menu-item-icon fa fa-fw fa-fas fa-paper-plane"></i><br>开往</a></li><li class="menu-item menu-item-友链"><a href="/links/" rel="section"><i class="menu-item-icon fa fa-fw fa-fas fa-link"></i><br>友链</a></li><li class="menu-item menu-item-shuoshuo"><a href="/shuoshuo/" rel="section"><i class="menu-item-icon fa fa-fw fa-fa-commenting-o fa-commenting"></i><br>说说</a></li><li class="menu-item menu-item-bookmark"><a href="/bookmark/" rel="section"><i class="menu-item-icon fa fa-fw fa-book"></i><br>书签</a></li><li class="menu-item menu-item-sitemap"><a href="/sitemap.xml" rel="section"><i class="menu-item-icon fa fa-fw fa-sitemap"></i><br>站点地图</a></li><li class="menu-item menu-item-search"><a href="javascript:;" rel="external nofollow noreferrer" class="popup-trigger"><i class="menu-item-icon fa fa-search fa-fw"></i><br>搜索</a></li></ul><div class="site-search"><div class="popup search-popup local-search-popup"><div class="local-search-header clearfix"><span class="search-icon"><i class="fa fa-search"></i> </span><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span><div class="local-search-input-wrapper"><input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input"></div></div><div id="local-search-result"></div></div></div></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="https://qikaile.us/Machine-learning-note.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="TJYS"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="天镜云生"></span><header class="post-header"><h2 class="post-title" itemprop="name headline">机器学习课程</h2><div class="post-meta"><span class="post-time"><i class="fa fa-thumb-tack"></i> <font color="7D26CD">置顶</font> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-newspaper-o jingping">精品</i> </span><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><time title="创建于" itemprop="dateCreated datePublished" datetime="2020-03-28T00:00:00+08:00">2020-03-28 </time></span><span class="post-category"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span> </a></span></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/Machine-learning-note.html#comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/Machine-learning-note.html" itemprop="commentCount"></span> </a></span><span class="post-wordcount"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">字数&#58;</span> <span title="字数">29k </span><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">时长 &asymp;</span> <span title="时长">126 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><h2 id="机器学习概述"><a href="#机器学习概述" class="headerlink" title="机器学习概述"></a>机器学习概述</h2><h3 id="什么是机器学习"><a href="#什么是机器学习" class="headerlink" title="什么是机器学习"></a>什么是机器学习</h3><p>机器学习是从<strong><span style="color:red">数据</span></strong>中自动分析获得<strong><span style="color:red">规律(模型)</span></strong>，并利用规律对<strong><span style="color:red">未知数据进行预测</span></strong>。</p><h3 id="机器学习算法分类"><a href="#机器学习算法分类" class="headerlink" title="机器学习算法分类"></a>机器学习算法分类</h3><h6 id="监督学习"><a href="#监督学习" class="headerlink" title="监督学习"></a>监督学习</h6><p>目标值：类别—分类问题</p><p>KNN、贝叶斯分类、决策树与随机森林、逻辑回归</p><p>目标值：连续型数据—回归问题</p><p>线性回归、岭回归</p><h6 id="无监督学习"><a href="#无监督学习" class="headerlink" title="无监督学习"></a>无监督学习</h6><p>目标值：无</p><p>聚类K-means</p><p><img src="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/机器学习图库/1fd68fd47f13030f88eecd632db3a4db.png" alt="img"></p><h3 id="机器学习开发流程"><a href="#机器学习开发流程" class="headerlink" title="机器学习开发流程"></a>机器学习开发流程</h3><p>1）获取数据</p><p>2）数据处理</p><p>3）特征工程</p><p>4）机器学习算法训练—模型</p><p>5）模型评估</p><p>6）应用</p><h2 id="特征工程"><a href="#特征工程" class="headerlink" title="特征工程"></a>特征工程</h2><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>数据———数据集的构成———特征值 + 目标值</p><h4 id="可用数据集"><a href="#可用数据集" class="headerlink" title="可用数据集"></a>可用数据集</h4><p>Kaggle特点：1、大数据竞赛平台 2、80万科学家 3、真实数据 4、数据量巨大</p><p>UCI特点：1、收录了360个数据集 2、覆盖科学、生活、经济等领域 3、数据量几十万</p><p>scikit-learn特点：1、数据量较小 2、方便学习</p><p>网址：</p><p>Kaggle网址：<a href="https://www.kaggle.com/datasets" target="_blank" rel="noopener external nofollow noreferrer">https://www.kaggle.com/datasets</a></p><p>UCI数据集网址： <a href="http://archive.ics.uci.edu/ml/" target="_blank" rel="noopener external nofollow noreferrer">http://archive.ics.uci.edu/ml/</a></p><p>scikit-learn网址：<a href="http://scikit-learn.org/stable/datasets/index.html#datasets" target="_blank" rel="noopener external nofollow noreferrer">http://scikit-learn.org/stable/datasets/index.html#datasets</a></p><h4 id="sklearn数据集"><a href="#sklearn数据集" class="headerlink" title="sklearn数据集"></a>sklearn数据集</h4><p>load_*小规模的数据集</p><p>fetch_*大规模的数据集</p><p>Bunch类型</p><p>数据集划分—model_selection.train_test_split()</p><ul><li><p>训练数据：用于训练，构建模型</p></li><li><p>测试数据：在模型检验时使用，用于评估模型是否有效</p></li></ul><h3 id="特征工程介绍"><a href="#特征工程介绍" class="headerlink" title="特征工程介绍"></a>特征工程介绍</h3><p>特征工程是将原始数据转换为更好地代表预测模型的潜在问题的特征的过程，从而提高了对未知数据的模型准确性。</p><p>sklearn 特征工程</p><p>Scikit-learn包含的内容：分类、聚类、回归、特征工程、模型选择和调优。</p><p>pandas 数据清洗、数据处理</p><p><strong><span style="color:red">特征处理</span></strong>是特征工程的核心部分，包括<strong><span style="color:red">特征提取</span></strong>、<strong><span style="color:red">数据预处理</span></strong>、<strong><span style="color:red">特征选择</span></strong>、<strong><span style="color:red">特征降维</span></strong>等。</p><h3 id="特征提取"><a href="#特征提取" class="headerlink" title="特征提取"></a>特征提取</h3><p>特征提取包括将任意数据（如文本或图像）转换为可用于机器学习的数字特征。<br>注：特征值化是为了计算机更好的去理解数据</p><p><strong>包：sklearn.feature_extraction</strong></p><p><strong>字典特征提取</strong></p><ul><li><p>应用DictVectorizer实现对类别特征进行数值化、离散化</p><p>sklearn.feature_extraction.DictVectorizer(sparse=True,…)<br>DictVectorizer.fit_transform(X) X:字典或者包含字典的迭代器返回值：返回sparse矩阵<br>DictVectorizer.inverse_transform(X) X:array数组或者sparse矩阵 返回值:转换之前数据格式<br>DictVectorizer.get_feature_names() 返回类别名称<br><strong>应用：</strong></p></li></ul><p>代码：</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="false" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#对字典类型的数据进行特征抽取</span></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction <span class="keyword">import</span> DictVectorizer</span><br><span class="line">data = [&#123;<span class="string">'city'</span>: <span class="string">'北京'</span>,<span class="string">'temperature'</span>:<span class="number">100</span>&#125;, &#123;<span class="string">'city'</span>: <span class="string">'上海'</span>,<span class="string">'temperature'</span>:<span class="number">60</span>&#125;, &#123;<span class="string">'city'</span>: <span class="string">'深圳'</span>,<span class="string">'temperature'</span>:<span class="number">30</span>&#125;]</span><br><span class="line"><span class="comment"># 1、实例化一个转换器类</span></span><br><span class="line">transfer = DictVectorizer(sparse=<span class="literal">False</span>)</span><br><span class="line"><span class="comment"># 2、调用fit_transform</span></span><br><span class="line">data = transfer.fit_transform(data)</span><br><span class="line">print(<span class="string">"返回的结果:\n"</span>, data)</span><br><span class="line"><span class="comment"># 打印特征名字</span></span><br><span class="line">print(<span class="string">"特征名字：\n"</span>, transfer.get_feature_names())</span><br></pre></td></tr></table></figure></div><p>输出结果：</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="false" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">返回的结果:</span><br><span class="line"> [[  <span class="number">0.</span>   <span class="number">1.</span>   <span class="number">0.</span> <span class="number">100.</span>]</span><br><span class="line"> [  <span class="number">1.</span>   <span class="number">0.</span>   <span class="number">0.</span>  <span class="number">60.</span>]</span><br><span class="line"> [  <span class="number">0.</span>   <span class="number">0.</span>   <span class="number">1.</span>  <span class="number">30.</span>]]</span><br><span class="line">特征名字：</span><br><span class="line"> [<span class="string">'city=上海'</span>, <span class="string">'city=北京'</span>, <span class="string">'city=深圳'</span>, <span class="string">'temperature'</span>]</span><br></pre></td></tr></table></figure></div><p><strong>文本特征提取</strong></p><ul><li><p><strong>独热编码</strong>（One-HotEncoding）</p></li><li><p>应用CountVectorizer实现对文本特征进行数值化</p></li><li><p>应用TfidfVectorizer(TF-IDF)实现对文本特征进行数值化</p><p>sklearn.feature_extraction.text.CountVectorizer(stop_words=[])</p><p>返回词频矩阵<br>CountVectorizer.fit_transform(X) X:文本或者包含文本字符串的可迭代对象 返回值：返回sparse矩阵<br>CountVectorizer.inverse_transform(X) X:array数组或者sparse矩阵 返回值:转换之前数据格<br>CountVectorizer.get_feature_names() 返回值:单词列表<br>sklearn.feature_extraction.text.TfidfVectorizer</p><ul><li>TF-IDF的主要思想是：如果<strong>某个词或短语在一篇文章中出现的概率高，并且在其他文章中很少出现</strong>，则认为此词或者短语具有很好的类别区分能力，适合用来分类。</li><li><strong>TF-IDF作用：用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。</strong></li></ul></li></ul><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="false" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfVectorizer</span><br><span class="line"><span class="comment"># 对于文本数据，进行特征抽取</span></span><br><span class="line">tf = TfidfVectorizer()</span><br><span class="line">x_train = tf.fit_transform(x_train)<span class="comment">#20类新闻分类数据集</span></span><br><span class="line"><span class="comment">#这里打印出来的列表是：训练集当中的所有不同词的组成的一个列表</span></span><br><span class="line">print(tf.get_feature_names())</span><br><span class="line"><span class="comment"># print(x_train.toarray())</span></span><br><span class="line">x_test = tf.transform(x_test) <span class="comment"># 不需要fit_transform</span></span><br></pre></td></tr></table></figure></div><ul><li><strong>应用</strong></li></ul><p>代码：</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="false" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> CountVectorizer,TfidfVectorizer</span><br><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cut_word</span><span class="params">(text)</span>:</span></span><br><span class="line">    <span class="comment"># 用jieba对中文字符串进行分词</span></span><br><span class="line">    text = <span class="string">" "</span>.join(list(jieba.cut(text)))</span><br><span class="line">    <span class="keyword">return</span> text</span><br><span class="line"><span class="comment">#对中文进行特征抽取</span></span><br><span class="line">data = [<span class="string">"一种还是一种今天很残酷，明天更残酷，后天很美好，但绝对大部分是死在明天晚上，所以每个人不要放弃今天。"</span>,</span><br><span class="line">            <span class="string">"我们看到的从很远星系来的光是在几百万年之前发出的，这样当我们看到宇宙时，我们是在看它的过去。"</span>,</span><br><span class="line">            <span class="string">"如果只用一种方式了解某样事物，你就不会真正了解它。了解事物真正含义的秘密取决于如何将其与我们所了解的事物相联系。"</span>,</span><br><span class="line">            <span class="string">"life is short,i like like python"</span>, <span class="string">"life is too long,i dislike python"</span>]</span><br><span class="line"><span class="comment"># 将原始数据转换成分好词的形式</span></span><br><span class="line">text_list = []</span><br><span class="line"><span class="keyword">for</span> sent <span class="keyword">in</span> data:</span><br><span class="line">	text_list.append(cut_word(sent))</span><br><span class="line">print(text_list)</span><br><span class="line"><span class="comment"># 1、实例化一个转换器类</span></span><br><span class="line">transfer = CountVectorizer()</span><br><span class="line"><span class="comment">#transfer = TfidfVectorizer()</span></span><br><span class="line"><span class="comment"># 2、调用fit_transform</span></span><br><span class="line">data = transfer.fit_transform(text_list)</span><br><span class="line">print(<span class="string">"文本特征抽取的结果：\n"</span>, data.toarray())</span><br><span class="line">print(<span class="string">"返回特征名字：\n"</span>, transfer.get_feature_names())</span><br></pre></td></tr></table></figure></div><p>输出结果：</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="false" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">'一种 还是 一种 今天 很 残酷 ， 明天 更 残酷 ， 后天 很 美好 ， 但 绝对 大部分 是 死 在 明天 晚上 ， 所以 每个 人 不要 放弃 今天 。'</span>, <span class="string">'我们 看到 的 从 很 远 星系 来 的 光是在 几百万年 之前 发出 的 ， 这样 当 我们 看到 宇宙 时 ， 我们 是 在 看 它 的 过去 。'</span>, <span class="string">'如果 只用 一种 方式 了解 某样 事物 ， 你 就 不会 真正 了解 它 。 了解 事物 真正 含义 的 秘密 取决于 如何 将 其 与 我们 所 了解 的 事物 相 联系 。'</span>, <span class="string">'life   is   short , i   like   like   python'</span>, <span class="string">'life   is   too   long , i   dislike   python'</span>]</span><br><span class="line">文本特征抽取的结果：</span><br><span class="line"> [[<span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">2</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">2</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">2</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">2</span> <span class="number">1</span></span><br><span class="line">  <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">3</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span></span><br><span class="line">  <span class="number">2</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span>]</span><br><span class="line"> [<span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">4</span> <span class="number">3</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span></span><br><span class="line">  <span class="number">0</span> <span class="number">2</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">2</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span></span><br><span class="line">  <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span></span><br><span class="line">  <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span>]]</span><br><span class="line">返回特征名字：</span><br><span class="line"> [<span class="string">'dislike'</span>, <span class="string">'is'</span>, <span class="string">'life'</span>, <span class="string">'like'</span>, <span class="string">'long'</span>, <span class="string">'python'</span>, <span class="string">'short'</span>, <span class="string">'too'</span>, <span class="string">'一种'</span>, <span class="string">'不会'</span>, <span class="string">'不要'</span>, <span class="string">'之前'</span>, <span class="string">'了解'</span>, <span class="string">'事物'</span>, <span class="string">'今天'</span>, <span class="string">'光是在'</span>, <span class="string">'几百万年'</span>, <span class="string">'发出'</span>, <span class="string">'取决于'</span>, <span class="string">'只用'</span>, <span class="string">'后天'</span>, <span class="string">'含义'</span>, <span class="string">'大部分'</span>, <span class="string">'如何'</span>, <span class="string">'如果'</span>, <span class="string">'宇宙'</span>, <span class="string">'我们'</span>, <span class="string">'所以'</span>, <span class="string">'放弃'</span>, <span class="string">'方式'</span>, <span class="string">'明天'</span>, <span class="string">'星系'</span>, <span class="string">'晚上'</span>, <span class="string">'某样'</span>, <span class="string">'残酷'</span>, <span class="string">'每个'</span>, <span class="string">'看到'</span>, <span class="string">'真正'</span>, <span class="string">'秘密'</span>, <span class="string">'绝对'</span>, <span class="string">'美好'</span>, <span class="string">'联系'</span>, <span class="string">'过去'</span>, <span class="string">'还是'</span>, <span class="string">'这样'</span>]</span><br></pre></td></tr></table></figure></div><p><strong>图像特征提取</strong>（深度学习将介绍）</p><h3 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h3><h4 id="去除唯一属性"><a href="#去除唯一属性" class="headerlink" title="去除唯一属性"></a>去除唯一属性</h4><p>唯一属性通常是一些id属性，这些属性并不能刻画样本自身的分布规律，所以简单地删除这些属性即可。</p><h4 id="处理缺失值"><a href="#处理缺失值" class="headerlink" title="处理缺失值"></a>处理缺失值</h4><p>缺失值处理的三种方法：直接使用含有缺失值的特征；删除含有缺失值的特征（该方法在包含缺失值的属性含有大量缺失值而仅仅包含极少量有效值时是有效的）；缺失值补全。</p><p>常见的缺失值补全方法：均值插补、同类均值插补、建模预测、高维映射、多重插补、极大似然估计、压缩感知和矩阵补全。</p><p>（1）<strong>均值插补</strong></p><p>如果样本属性的距离是可度量的，则使用该属性有效值的平均值来插补缺失的值；</p><p>如果的距离是不可度量的，则使用该属性有效值的众数来插补缺失的值。<strong>如果使用众数插补，出现数据倾斜会造成什么影响？</strong></p><p>（2）<strong>同类均值插补</strong></p><p>首先将样本进行分类，然后以该类中样本的均值来插补缺失值。</p><p><strong>（3）建模预测</strong></p><p>将缺失的属性作为预测目标来预测，将数据集按照是否含有特定属性的缺失值分为两类，利用现有的机器学习算法对待预测数据集的缺失值进行预测。</p><p>该方法的根本的缺陷是如果其他属性和缺失属性无关，则预测的结果毫无意义；但是若预测结果相当准确，则说明这个缺失属性是没必要纳入数据集中的；一般的情况是介于两者之间。</p><p><strong>（4）高维映射</strong></p><p>将属性映射到高维空间，采用独热码编码（one-hot）技术。将包含K个离散取值范围的属性值扩展为K+1个属性值，若该属性值缺失，则扩展后的第K+1个属性值置为1。</p><p>这种做法是最精确的做法，保留了所有的信息，也未添加任何额外信息，若预处理时把所有的变量都这样处理，会大大增加数据的维度。这样做的好处是完整保留了原始数据的全部信息、不用考虑缺失值；缺点是计算量大大提升，且只有在样本量非常大的时候效果才好。</p><p>（5）<strong>多重插补</strong>（MultipleImputation，MI）</p><p>多重插补认为待插补的值是随机的，实践上通常是估计出待插补的值，再加上不同的噪声，形成多组可选插补值，根据某种选择依据，选取最合适的插补值。</p><p>（6）<strong>压缩感知和矩阵补全</strong></p><p>（7）<strong>手动插补</strong></p><p>插补处理只是将未知值补以我们的主观估计值，不一定完全符合客观事实。在许多情况下，根据对所在领域的理解，手动对缺失值进行插补的效果会更好。</p><h4 id="特征编码"><a href="#特征编码" class="headerlink" title="特征编码"></a>特征编码</h4><p>（1）<strong>标签处理</strong></p><p>通常我们会把字符型的标签转换成数值型的</p><p>代码：</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="false" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df = pd.DataFrame([</span><br><span class="line">            [<span class="string">'green'</span>, <span class="string">'M'</span>, <span class="number">10.1</span>, <span class="string">'class1'</span>], </span><br><span class="line">            [<span class="string">'red'</span>, <span class="string">'L'</span>, <span class="number">13.5</span>, <span class="string">'class2'</span>], </span><br><span class="line">            [<span class="string">'blue'</span>, <span class="string">'XL'</span>, <span class="number">15.3</span>, <span class="string">'class1'</span>]])</span><br><span class="line">df.columns = [<span class="string">'color'</span>, <span class="string">'size'</span>, <span class="string">'prize'</span>, <span class="string">'class label'</span>]</span><br><span class="line"><span class="comment">#标签处理</span></span><br><span class="line">class_mapping = &#123;label:idx <span class="keyword">for</span> idx,label <span class="keyword">in</span> enumerate(set(df[<span class="string">'class label'</span>]))&#125;</span><br><span class="line">df[<span class="string">'class label'</span>] = df[<span class="string">'class label'</span>].map(class_mapping)</span><br><span class="line">print(df)</span><br><span class="line">print(<span class="string">'-----------------------------------'</span>)</span><br><span class="line">size_mapping = &#123;</span><br><span class="line">           <span class="string">'XL'</span>: <span class="number">3</span>,</span><br><span class="line">           <span class="string">'L'</span>: <span class="number">2</span>,</span><br><span class="line">           <span class="string">'M'</span>: <span class="number">1</span>&#125;</span><br><span class="line"></span><br><span class="line">df[<span class="string">'size'</span>] = df[<span class="string">'size'</span>].map(size_mapping)</span><br><span class="line">print(df)</span><br></pre></td></tr></table></figure></div><p>输出结果:</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="false" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">   color size  prize  <span class="class"><span class="keyword">class</span> <span class="title">label</span></span></span><br><span class="line"><span class="class">0  <span class="title">green</span>    <span class="title">M</span>   10.1            1</span></span><br><span class="line"><span class="class">1    <span class="title">red</span>    <span class="title">L</span>   13.5            0</span></span><br><span class="line"><span class="class">2   <span class="title">blue</span>   <span class="title">XL</span>   15.3            1</span></span><br><span class="line"><span class="class">-----------------------------------</span></span><br><span class="line"><span class="class">   <span class="title">color</span>  <span class="title">size</span>  <span class="title">prize</span>  <span class="title">class</span> <span class="title">label</span></span></span><br><span class="line"><span class="class">0  <span class="title">green</span>     1   10.1            1</span></span><br><span class="line"><span class="class">1    <span class="title">red</span>     2   13.5            0</span></span><br><span class="line"><span class="class">2   <span class="title">blue</span>     3   15.3            1</span></span><br></pre></td></tr></table></figure></div><p>（2）<strong>二值化</strong></p><p>二值化的过程是将数值型的属性转换为布尔值的属性，设定一个阈值作为划分属性值为0和1的分隔点。</p><p>使用preproccessing库的Binarizer类对数据进行二值化的代码如下：</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="false" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> Binarizer</span><br><span class="line"><span class="comment">#二值化，阈值设置为3，返回值为二值化后的数据</span></span><br><span class="line">Binarizer(threshold=<span class="number">3</span>).fit_transform(X)  <span class="comment">#X=iris.data（鸢尾花）数据集</span></span><br></pre></td></tr></table></figure></div><p>（3）<strong>scikit DictVectorizer</strong></p><p>代码：</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="false" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df = pd.DataFrame([</span><br><span class="line">            [<span class="string">'green'</span>, <span class="string">'M'</span>, <span class="number">10.1</span>, <span class="string">'class1'</span>], </span><br><span class="line">            [<span class="string">'red'</span>, <span class="string">'L'</span>, <span class="number">13.5</span>, <span class="string">'class2'</span>], </span><br><span class="line">            [<span class="string">'blue'</span>, <span class="string">'XL'</span>, <span class="number">15.3</span>, <span class="string">'class1'</span>]])</span><br><span class="line">df.columns = [<span class="string">'color'</span>, <span class="string">'size'</span>, <span class="string">'prize'</span>, <span class="string">'class label'</span>]</span><br><span class="line">print(df)</span><br><span class="line">print(<span class="string">'----------------------------------------------------------------------'</span>)</span><br><span class="line"><span class="comment">#print(df.transpose().to_dict().values())</span></span><br><span class="line"><span class="comment">#print('----------------------------------------------------------------------')</span></span><br><span class="line">feature = df.iloc[:, :<span class="number">-1</span>]</span><br><span class="line">print(feature)</span><br><span class="line">print(<span class="string">'----------------------------------------------------------------------'</span>)</span><br><span class="line"><span class="comment"># ②对于x转换成字典数据</span></span><br><span class="line">feature=feature.to_dict(orient=<span class="string">"records"</span>)</span><br><span class="line"><span class="comment">#feature=feature.transpose().to_dict().values() #对所有的数据都做了映射</span></span><br><span class="line"><span class="comment">#使用 DictVectorizer将得到特征的字典</span></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction <span class="keyword">import</span> DictVectorizer</span><br><span class="line">dvec = DictVectorizer(sparse=<span class="literal">False</span>)</span><br><span class="line">X = dvec.fit_transform(feature)</span><br><span class="line">print(dvec.get_feature_names())</span><br><span class="line">print(<span class="string">'----------------------------------------------------------------------'</span>)</span><br><span class="line">print(X)</span><br><span class="line">print(<span class="string">'----------------------------------------------------------------------'</span>)</span><br><span class="line"><span class="comment">#可以调用 get_feature_names 来返回新的列的名字，其中0和1就代表是不是这个属性.</span></span><br><span class="line">data=pd.DataFrame(X, columns=dvec.get_feature_names())</span><br><span class="line">print(data)</span><br></pre></td></tr></table></figure></div><p>输出结果:</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="false" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">   color size  prize <span class="class"><span class="keyword">class</span> <span class="title">label</span></span></span><br><span class="line"><span class="class">0  <span class="title">green</span>    <span class="title">M</span>   10.1      <span class="title">class1</span></span></span><br><span class="line"><span class="class">1    <span class="title">red</span>    <span class="title">L</span>   13.5      <span class="title">class2</span></span></span><br><span class="line"><span class="class">2   <span class="title">blue</span>   <span class="title">XL</span>   15.3      <span class="title">class1</span></span></span><br><span class="line"><span class="class">----------------------------------------------------------------------</span></span><br><span class="line"><span class="class">   <span class="title">color</span> <span class="title">size</span>  <span class="title">prize</span></span></span><br><span class="line"><span class="class">0  <span class="title">green</span>    <span class="title">M</span>   10.1</span></span><br><span class="line"><span class="class">1    <span class="title">red</span>    <span class="title">L</span>   13.5</span></span><br><span class="line"><span class="class">2   <span class="title">blue</span>   <span class="title">XL</span>   15.3</span></span><br><span class="line"><span class="class">----------------------------------------------------------------------</span></span><br><span class="line">['color=blue', 'color=green', 'color=red', 'prize', 'size=L', 'size=M', 'size=XL']</span><br><span class="line">----------------------------------------------------------------------</span><br><span class="line">[[ <span class="number">0.</span>   <span class="number">1.</span>   <span class="number">0.</span>  <span class="number">10.1</span>  <span class="number">0.</span>   <span class="number">1.</span>   <span class="number">0.</span> ]</span><br><span class="line"> [ <span class="number">0.</span>   <span class="number">0.</span>   <span class="number">1.</span>  <span class="number">13.5</span>  <span class="number">1.</span>   <span class="number">0.</span>   <span class="number">0.</span> ]</span><br><span class="line"> [ <span class="number">1.</span>   <span class="number">0.</span>   <span class="number">0.</span>  <span class="number">15.3</span>  <span class="number">0.</span>   <span class="number">0.</span>   <span class="number">1.</span> ]]</span><br><span class="line">----------------------------------------------------------------------</span><br><span class="line">   color=blue  color=green  color=red  prize  size=L  size=M  size=XL</span><br><span class="line"><span class="number">0</span>         <span class="number">0.0</span>          <span class="number">1.0</span>        <span class="number">0.0</span>   <span class="number">10.1</span>     <span class="number">0.0</span>     <span class="number">1.0</span>      <span class="number">0.0</span></span><br><span class="line"><span class="number">1</span>         <span class="number">0.0</span>          <span class="number">0.0</span>        <span class="number">1.0</span>   <span class="number">13.5</span>     <span class="number">1.0</span>     <span class="number">0.0</span>      <span class="number">0.0</span></span><br><span class="line"><span class="number">2</span>         <span class="number">1.0</span>          <span class="number">0.0</span>        <span class="number">0.0</span>   <span class="number">15.3</span>     <span class="number">0.0</span>     <span class="number">0.0</span>      <span class="number">1.0</span></span><br></pre></td></tr></table></figure></div><p>（4）<strong>独热编码</strong>（One-HotEncoding）</p><p>独热编码采用N位状态寄存器来对N个可能的取值进行编码，每个状态都由独立的寄存器来表示，并且在任意时刻只有其中一位有效。</p><p>独热编码的优点：能够处理非数值属性；在一定程度上扩充了特征；编码后的属性是稀疏的，存在大量的零元分量。</p><p>代码：</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="false" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df = pd.DataFrame([</span><br><span class="line">            [<span class="string">'green'</span>, <span class="string">'M'</span>, <span class="number">10.1</span>, <span class="string">'class1'</span>], </span><br><span class="line">            [<span class="string">'red'</span>, <span class="string">'L'</span>, <span class="number">13.5</span>, <span class="string">'class2'</span>], </span><br><span class="line">            [<span class="string">'blue'</span>, <span class="string">'XL'</span>, <span class="number">15.3</span>, <span class="string">'class1'</span>]])</span><br><span class="line">df.columns = [<span class="string">'color'</span>, <span class="string">'size'</span>, <span class="string">'prize'</span>, <span class="string">'class label'</span>]</span><br><span class="line"><span class="comment">#OneHotEncoder 必须使用整数作为输入，所以得先使用scikit LabelEncoder处理一下</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder</span><br><span class="line">class_le = LabelEncoder()</span><br><span class="line">df[<span class="string">'class label'</span>] = class_le.fit_transform(df[<span class="string">'class label'</span>])</span><br><span class="line">print(df)</span><br><span class="line">print(<span class="string">'-----------------------------------'</span>)</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> OneHotEncoder</span><br><span class="line">ohe = OneHotEncoder(sparse=<span class="literal">False</span>)</span><br><span class="line">X = ohe.fit_transform(df[[<span class="string">'color'</span>]].values)</span><br><span class="line">print(X)</span><br></pre></td></tr></table></figure></div><p>输出结果:</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="false" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">   color size  prize  <span class="class"><span class="keyword">class</span> <span class="title">label</span></span></span><br><span class="line"><span class="class">0  <span class="title">green</span>    <span class="title">M</span>   10.1            0</span></span><br><span class="line"><span class="class">1    <span class="title">red</span>    <span class="title">L</span>   13.5            1</span></span><br><span class="line"><span class="class">2   <span class="title">blue</span>   <span class="title">XL</span>   15.3            0</span></span><br><span class="line"><span class="class">-----------------------------------</span></span><br><span class="line"><span class="class">[[0. 1. 0.]</span></span><br><span class="line"><span class="class"> [0. 0. 1.]</span></span><br><span class="line"><span class="class"> [1. 0. 0.]]</span></span><br></pre></td></tr></table></figure></div><p>注：Pandas库中同样有类似的操作，使用get_dummies也可以得到相应的特征</p><p>（5）<strong>pandas get_dummies</strong></p><p>代码：</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="false" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df = pd.DataFrame([</span><br><span class="line">            [<span class="string">'green'</span>,<span class="number">10.1</span>], </span><br><span class="line">            [<span class="string">'red'</span>, <span class="number">13.5</span>], </span><br><span class="line">            [<span class="string">'blue'</span>,<span class="number">15.3</span>]])</span><br><span class="line">df.columns = [<span class="string">'color'</span>, <span class="string">'prize'</span>]</span><br><span class="line">df=pd.get_dummies(df)</span><br><span class="line">print(df)</span><br></pre></td></tr></table></figure></div><p>输出结果:</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="false" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">   prize  color_blue  color_green  color_red</span><br><span class="line"><span class="number">0</span>   <span class="number">10.1</span>           <span class="number">0</span>            <span class="number">1</span>          <span class="number">0</span></span><br><span class="line"><span class="number">1</span>   <span class="number">13.5</span>           <span class="number">0</span>            <span class="number">0</span>          <span class="number">1</span></span><br><span class="line"><span class="number">2</span>   <span class="number">15.3</span>           <span class="number">1</span>            <span class="number">0</span>          <span class="number">0</span></span><br></pre></td></tr></table></figure></div><h4 id="无量纲化、正则化"><a href="#无量纲化、正则化" class="headerlink" title="无量纲化、正则化"></a>无量纲化、正则化</h4><h6 id="无量纲化"><a href="#无量纲化" class="headerlink" title="无量纲化"></a>无量纲化</h6><p>数据标准化是将样本的属性缩放到某个指定的范围。</p><p><strong>标准化</strong>：基于原始数据的均值（mean）和标准差（standarddeviation）进行数据的标准化。</p><p>要求 均值$\mu = 0$ 和标准差 $\sigma = 1$</p><p>公式表达为：</p><script type="math/tex;mode=display">\begin{equation} z = \frac{x - \mu}{\sigma}\end{equation}</script><p>使用preproccessing库的StandardScaler类对数据进行标准化的代码如下：</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="false" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler </span><br><span class="line"><span class="comment">#标准化，返回值为标准化后的数据</span></span><br><span class="line">ss=StandardScaler()</span><br><span class="line">X1=ss.fit_transform(X) <span class="comment">#X=iris.data（鸢尾花）数据集</span></span><br></pre></td></tr></table></figure></div><p><strong>归一化</strong>：处理后的所有特征的值都会被压缩到 0到1区间上.这样做还可以抑制离群值对结果的影响.</p><p>公式表达为：</p><script type="math/tex;mode=display">{\begin{equation} X_{norm} = \frac{X - X_{min}}{X_{max}-X_{min}} \end{equation}}</script><p>使用preproccessing库的MinMaxScaler类对数据进行区间缩放的代码如下：</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="false" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler</span><br><span class="line"><span class="comment">#区间缩放，返回值为缩放到[0, 1]区间的数据</span></span><br><span class="line">mms=MinMaxScaler()</span><br><span class="line">X2=mms.fit_transform(X) <span class="comment">#X=iris.data（鸢尾花）数据集</span></span><br></pre></td></tr></table></figure></div><h6 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h6><p>正则化的过程是将每个样本缩放到单位范数（每个样本的范数为1），如果后面要使用如二次型（点积）或者其它核方法计算两个样本之间的相似性这个方法会很有用。</p><p>该方法主要应用于文本分类和聚类中。</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="false" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> Normalizer</span><br><span class="line">ss=Normalizer()</span><br><span class="line">X3=ss.fit_transform(X) <span class="comment">#X=iris.data（鸢尾花）数据集</span></span><br></pre></td></tr></table></figure></div><h6 id="标准化与正则化的区别"><a href="#标准化与正则化的区别" class="headerlink" title="标准化与正则化的区别"></a>标准化与正则化的区别</h6><p>简单来说，<strong><span style="color:red">标准化是依照特征矩阵的列处理数据</span></strong>，其通过求z-score的方法，将样本的特征值转换到同一量纲下。<strong><span style="color:red">正则化是依照特征矩阵的行处理数据</span></strong>，其目的在于样本向量在点乘运算或其他核函数计算相似性时，拥有统一的标准，也就是说都转化为“单位向量”。</p><h3 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h3><p>特征选择：数据中包含冗余或相关变量（或特征、属性、指标），旨在从原有特征中找出主要特征。</p><p>包：<strong>sklearn.feature_selection</strong></p><p>当数据预处理完成后，我们需要选择有意义的特征输入机器学习的算法和模型进行训练。通常来说，从两个方面考虑来选择特征：</p><ul><li><strong>特征是否发散</strong>：如果一个<strong><span style="color:red">特征不发散，例如方差接近于0</span></strong>，也就是说样本在这个特征上基本上没有差异，<strong><span style="color:red">这个特征对于样本的区分并没有什么用</span></strong>。</li><li><strong>特征与目标的相关性</strong>：这点比较显见，<strong><span style="color:red">与目标相关性高的特征，应当优选选择</span></strong>。除移除低方差法外，本文介绍的其他方法均从相关性考虑。</li></ul><p><strong>根据特征选择的形式又可以将特征选择方法分为3种：</strong></p><h4 id="Filter"><a href="#Filter" class="headerlink" title="Filter"></a>Filter</h4><p>Filter：过滤法，按照发散性或者相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，选择特征。</p><p><strong>方差选择法</strong>：低方差特征过滤</p><p>代码：</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="false" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> VarianceThreshold</span><br><span class="line">X = [[<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>]]</span><br><span class="line">sel = VarianceThreshold(threshold=(<span class="number">.8</span> * (<span class="number">1</span> - <span class="number">.8</span>)))</span><br><span class="line">X1=sel.fit_transform(X)</span><br><span class="line">print(X1)</span><br></pre></td></tr></table></figure></div><p>输出结果:</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="false" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">0</span> <span class="number">1</span>]</span><br><span class="line"> [<span class="number">1</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">0</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">1</span> <span class="number">1</span>]</span><br><span class="line"> [<span class="number">1</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">1</span> <span class="number">1</span>]]</span><br></pre></td></tr></table></figure></div><p>果然, VarianceThreshold 移除了第一列特征，第一列中特征值为0的概率达到了5/6.</p><p><strong>相关系数</strong>：特征与特征之间的相关程度（<strong><span style="color:red">与目标相关性高的特征，应当优选选择</span></strong>）</p><ul><li><p>对于<strong><span style="color:red">分类问题(y离散)</span></strong>，可采用：</p><p><strong><span style="color:#00f">卡方检验</span></strong>，<em>f_classif</em>, <em>mutual_info_classif</em>，<strong><span style="color:#00f">互信息</span></strong></p></li><li><p>对于<strong><span style="color:red">回归问题(y连续)</span></strong>，可采用：<br><strong><span style="color:#00f">皮尔森相关系数</span></strong>，<strong><span style="color:#00f"><em>f_regression</em></span></strong>, <em>mutual_info_regression</em>，最大信息系数</p><p><strong>卡方(Chi2)检验</strong></p><p>​ 经典的卡方检验是检验定性自变量对定性因变量的相关性。假设自变量有N种取值，因变量有M种取值，考虑自变量等于i且因变量等于j的样本频数的观察值与期望的差距，构建统计量：</p><script type="math/tex;mode=display">\chi^{2}=\sum \frac{(A-E)^{2}}{E}</script><p>假设有两个分类变量X和Y，它们的值域分别为{x1, x2}和{y1, y2}，其样本频数列联表为：</p></li></ul><p><img src="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/机器学习图库/表格1.JPG" alt></p><p>经典的卡方检验是检验定性自变量对定性因变量的相关性，针对分类问题。比如，我们可以对样本进行一次chi2测试来选择最佳的两项特征：</p><p>代码：</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="false" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectKBest</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> chi2</span><br><span class="line">iris = load_iris()</span><br><span class="line">X, y = iris.data, iris.target</span><br><span class="line">print(X.shape)</span><br><span class="line">X_new = SelectKBest(chi2, k=<span class="number">2</span>).fit_transform(X, y)</span><br><span class="line">print(X_new.shape)</span><br></pre></td></tr></table></figure></div><p>输出结果:</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="false" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(<span class="number">150</span>, <span class="number">4</span>)</span><br><span class="line">(<span class="number">150</span>, <span class="number">2</span>)</span><br></pre></td></tr></table></figure></div><p><strong>Pearson相关系数 (Pearson Correlation)</strong></p><p>​ 皮尔森相关系数是一种最简单的，能帮助理解特征和响应变量之间关系的方法，该方法衡量的是变量之间的线性相关性，结果的取值区间为[-1，1]，-1表示完全的负相关，+1表示完全的正相关，0表示没有线性相关。</p><p>​ Pearson Correlation速度快、易于计算，经常在拿到数据(经过清洗和特征提取之后的)之后第一时间就执行。Scipy的 pearsonr 方法能够同时计算相关系数和p-value.</p><p>代码：</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="false" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> pearsonr</span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line">size = <span class="number">300</span></span><br><span class="line">x = np.random.normal(<span class="number">0</span>, <span class="number">1</span>, size)</span><br><span class="line"><span class="comment"># pearsonr(x, y)的输入为特征矩阵和目标向量</span></span><br><span class="line">print(<span class="string">"Lower noise"</span>, pearsonr(x, x + np.random.normal(<span class="number">0</span>, <span class="number">1</span>, size)))</span><br><span class="line">print(<span class="string">"Higher noise"</span>, pearsonr(x, x + np.random.normal(<span class="number">0</span>, <span class="number">10</span>, size)))</span><br></pre></td></tr></table></figure></div><p>输出结果：</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="false" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#输出为二元组(sorce, p-value)的数组</span></span><br><span class="line">Lower noise (<span class="number">0.7182483686213842</span>, <span class="number">7.324017312997672e-49</span>)</span><br><span class="line">Higher noise (<span class="number">0.05796429207933815</span>, <span class="number">0.31700993885325246</span>)</span><br></pre></td></tr></table></figure></div><p>这个例子中，我们比较了变量在加入噪音之前和之后的差异。当噪音比较小的时候，相关性很强，p-value很低。</p><p><strong>实例分析：股票的财务指标相关性计算</strong></p><p>分析</p><ul><li><p>两两特征之间进行相关性计算</p><p>代码：</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="false" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> pearsonr</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">data = pd.read_csv(<span class="string">"factor_returns.csv"</span>)</span><br><span class="line">factor = [<span class="string">'pe_ratio'</span>, <span class="string">'pb_ratio'</span>, <span class="string">'market_cap'</span>, <span class="string">'return_on_asset_net_profit'</span>, <span class="string">'du_return_on_equity'</span>, <span class="string">'ev'</span>,<span class="string">'earnings_per_share'</span>, <span class="string">'revenue'</span>, <span class="string">'total_expense'</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(factor)):</span><br><span class="line">	<span class="keyword">for</span> j <span class="keyword">in</span> range(i, len(factor) - <span class="number">1</span>):</span><br><span class="line">		print(<span class="string">"指标%s与指标%s之间的相关性大小为%f"</span> % (factor[i], factor[j + <span class="number">1</span>], pearsonr(data[factor[i]], data[factor[j + <span class="number">1</span>]])[<span class="number">0</span>]))</span><br></pre></td></tr></table></figure></div><p>输出结果：（展示部分数据结果）</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="false" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">指标pe_ratio与指标pb_ratio之间的相关性大小为<span class="number">-0.004389</span></span><br><span class="line">指标pe_ratio与指标market_cap之间的相关性大小为<span class="number">-0.068861</span></span><br><span class="line">………………………………………………………………………………………………………………………………………………………………</span><br><span class="line">指标return_on_asset_net_profit与指标du_return_on_equity之间的相关性大小为<span class="number">0.818697</span></span><br><span class="line">指标return_on_asset_net_profit与指标ev之间的相关性大小为<span class="number">-0.101225</span></span><br><span class="line">………………………………………………………………………………………………………………………………………………………………</span><br><span class="line">指标revenue与指标total_expense之间的相关性大小为<span class="number">0.995845</span></span><br></pre></td></tr></table></figure></div><p>从中我们得出</p></li><li><p>指标revenue与指标total_expense之间的相关性大小为0.995845</p></li><li><p>指标return_on_asset_net_profit与指标du_return_on_equity之间的相关性大小为0.818697</p><p>我们也可以通过画图来观察结果</p><p>代码：</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="false" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.figure(figsize=(<span class="number">20</span>, <span class="number">8</span>), dpi=<span class="number">100</span>)</span><br><span class="line">plt.scatter(data[<span class="string">'revenue'</span>], data[<span class="string">'total_expense'</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></div><p><img src="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/机器学习图库/Figure_1.png" alt="Figure_1"></p><p>注：特征与特征之间相关性很高：1）选取其中一个；2）权重加权求和；3）主成分分析</p><p>Scikit-learn提供的 <a href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_regression.html" target="_blank" rel="noopener external nofollow noreferrer">f_regrssion</a> 方法能够批量计算特征的f_score和p-value，非常方便，参考sklearn的 <a href="http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html" target="_blank" rel="noopener external nofollow noreferrer">pipeline</a></p><p><strong>Pearson相关系数的一个明显缺陷是，作为特征排序机制，</strong>他只对线性关系敏感。如果关系是非线性的，即便两个变量具有一一对应的关系，Pearson相关性也可能会接近0。例如：</p><p>代码：</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="false" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = np.random.uniform(<span class="number">-1</span>, <span class="number">1</span>, <span class="number">100000</span>)</span><br><span class="line"><span class="keyword">print</span> pearsonr(x, x**<span class="number">2</span>)[<span class="number">0</span>]</span><br></pre></td></tr></table></figure></div><p>输出结果：</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="false" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">-0.00230804707612</span></span><br></pre></td></tr></table></figure></div><p>更多类似的例子参考 <a href="http://upload.wikimedia.org/wikipedia/commons/thumb/d/d4/Correlation_examples2.svg/506px-Correlation_examples2.svg.png" target="_blank" rel="noopener external nofollow noreferrer">sample plots</a> 。另外，如果仅仅根据相关系数这个值来判断的话，有时候会具有很强的误导性，如 <a href="http://www.matrix67.com/blog/archives/2308" target="_blank" rel="noopener external nofollow noreferrer">Anscombe’s quartet</a> ，最好把数据可视化出来，以免得出错误的结论。</p><p><strong>互信息和最大信息系数 (Mutual information and maximal information coefficient (MIC)</strong></p><p>经典的互信息（互信息为随机变量X与Y之间的互信息$I(X;Y)$为单个事件之间互信息的数学期望）也是评价定性自变量对定性因变量的相关性的，互信息计算公式如下：</p><script type="math/tex;mode=display">I(X ; Y)=\sum_{x \in X} \sum_{y \in Y} p(x, y) \log \frac{p(x, y)}{p(x) p(y)}</script><p>互信息直接用于特征选择其实不是太方便：</p><p>1、它不属于度量方式，也没有办法归一化，在不同数据及上的结果无法做比较；</p><p>2、对于连续变量的计算不是很方便（X和Y都是集合，x，y都是离散的取值），通常变量需要先离散化，而互信息的结果对离散化的方式很敏感。</p><p>最大信息系数克服了这两个问题。它首先寻找一种最优的离散化方式，然后把互信息取值转换成一种度量方式，取值区间在[0，1]。 <a href="http://minepy.readthedocs.io/en/latest/" target="_blank" rel="noopener external nofollow noreferrer">minepy</a> 提供了MIC功能。</p><p>反过头来看y=x^2这个例子，MIC算出来的互信息值为1(最大的取值)。</p><p>代码：</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="false" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> minepy <span class="keyword">import</span> MINE</span><br><span class="line">m = MINE()</span><br><span class="line">x = np.random.uniform(<span class="number">-1</span>, <span class="number">1</span>, <span class="number">10000</span>)</span><br><span class="line">m.compute_score(x, x**<span class="number">2</span>)</span><br><span class="line">print(m.mic())</span><br></pre></td></tr></table></figure></div><p>输出结果：</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="false" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1.0000000000000009</span></span><br></pre></td></tr></table></figure></div><p>MIC的统计能力遭到了 <a href="http://statweb.stanford.edu/~tibs/reshef/comment.pdf" target="_blank" rel="noopener external nofollow noreferrer">一些质疑</a> ，当零假设不成立时，MIC的统计就会受到影响。在有的数据集上不存在这个问题，但有的数据集上就存在这个问题。</p><p><strong>距离相关系数 (Distance Correlation)</strong></p><p>距离相关系数是为了克服Pearson相关系数的弱点而生的。在$X$和$X^2$这个例子中，即便Pearson相关系数是0，我们也不能断定这两个变量是独立的（有可能是非线性相关）；但如果距离相关系数是0，那么我们就可以说这两个变量是独立的。</p><p>R的 energy 包里提供了距离相关系数的实现，另外这是 <a href="https://gist.github.com/josef-pkt/2938402" target="_blank" rel="noopener external nofollow noreferrer">Python gist</a> 的实现。</p><p>尽管有 MIC 和 距离相关系数在了，但当变量之间的关系接近线性相关的时候，Pearson相关系数仍然是不可替代的。<br>第一，Pearson相关系数计算速度快，这在处理大规模数据的时候很重要。<br>第二，Pearson相关系数的取值区间是[-1，1]，而MIC和距离相关系数都是[0，1]。这个特点使得Pearson相关系数能够表征更丰富的关系，符号表示关系的正负，绝对值能够表示强度。当然，Pearson相关性有效的前提是两个变量的变化关系是单调的。</p><p><strong>基于模型的特征排序 (Model based ranking)</strong></p><p>这种方法的思路是直接使用你要用的机器学习算法，<strong>针对每个单独的特征和响应变量建立预测模型。</strong>假如特征和响应变量之间的关系是<strong>非线性的</strong>，可以用基于树的方法(决策树、随机森林)、或者扩展的线性模型等。基于树的方法比较易于使用，因为他们对非线性关系的建模比较好，并且不需要太多的调试。但要注意过拟合问题，因此树的深度最好不要太大，再就是<strong>运用交叉验证</strong>。</p><p>在波士顿房价数据集上使用sklearn的随机森林回归给出一个单变量选择的例子(这里使用了交叉验证)：</p><p>代码：</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="false" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score, ShuffleSplit</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_boston</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestRegressor</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># Load boston housing dataset as an example</span></span><br><span class="line">boston = load_boston()</span><br><span class="line">X = boston[<span class="string">"data"</span>]</span><br><span class="line">Y = boston[<span class="string">"target"</span>]</span><br><span class="line">names = boston[<span class="string">"feature_names"</span>]</span><br><span class="line">rf = RandomForestRegressor(n_estimators=<span class="number">20</span>, max_depth=<span class="number">4</span>)</span><br><span class="line">scores = []</span><br><span class="line"><span class="comment"># 单独采用每个特征进行建模，并进行交叉验证</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(X.shape[<span class="number">1</span>]):</span><br><span class="line">    score = cross_val_score(rf, X[:, i:i+<span class="number">1</span>], Y, scoring=<span class="string">"r2"</span>,  cv=ShuffleSplit(len(X), <span class="number">3</span>, <span class="number">.3</span>)) <span class="comment"># 注意X[:, i]和X[:, i:i+1]的区别</span></span><br><span class="line">    scores.append((format(np.mean(score), <span class="string">'.3f'</span>), names[i]))</span><br><span class="line">print(sorted(scores, reverse=<span class="literal">True</span>))</span><br></pre></td></tr></table></figure></div><p>输出结果：</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="false" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[(<span class="string">'-8.082'</span>, <span class="string">'TAX'</span>), (<span class="string">'-6.871'</span>, <span class="string">'CHAS'</span>), (<span class="string">'-6.420'</span>, <span class="string">'RM'</span>), (<span class="string">'-6.315'</span>, <span class="string">'DIS'</span>), (<span class="string">'-4.833'</span>, <span class="string">'INDUS'</span>), (<span class="string">'-4.816'</span>, <span class="string">'AGE'</span>), (<span class="string">'-4.742'</span>, <span class="string">'LSTAT'</span>), (<span class="string">'-4.638'</span>, <span class="string">'RAD'</span>), (<span class="string">'-3.411'</span>, <span class="string">'NOX'</span>), (<span class="string">'-3.123'</span>, <span class="string">'CRIM'</span>), (<span class="string">'-26.603'</span>, <span class="string">'PTRATIO'</span>), (<span class="string">'-12.284'</span>, <span class="string">'B'</span>), (<span class="string">'-1.995'</span>, <span class="string">'ZN'</span>)]</span><br></pre></td></tr></table></figure></div></li></ul><h4 id="Wrapper"><a href="#Wrapper" class="headerlink" title="Wrapper"></a>Wrapper</h4><p>Wrapper：包装法，根据目标函数（通常是预测效果评分），每次选择若干特征，或者排除若干特征。</p><p>主要方法有：recursive feature elimination algorithm(递归特征消除算法)</p><h4 id="Embedded"><a href="#Embedded" class="headerlink" title="Embedded"></a>Embedded</h4><p>Embedded：嵌入法，先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。类似于Filter方法，但是是通过训练来确定特征的优劣。比如，Lasso和RF（随机森林）都有各自的特征选择方法。</p><p>注：使用SelectFromModel选择特征</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="false" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectFromMode</span><br></pre></td></tr></table></figure></div><h3 id="特征降维"><a href="#特征降维" class="headerlink" title="特征降维"></a>特征降维</h3><p>二维数组 此处的降维：降低特征的个数 效果：特征与特征之间不相关</p><p><strong><span style="color:red">降维</span></strong>是指在某种限定条件下，<strong><span style="color:red">降低随机变量（特征）个数</span></strong>，得到<strong><span style="color:red">一组“不相关”主变量</span></strong>的过程。</p><p>当特征选择完成后，可以直接训练模型了，但是可能由于特征矩阵过大，导致计算量大，训练时间长的问题，因此降低特征矩阵维度也是必不可少的。常见的降维方法：主成分分析法（PCA）和线性判别分析（LDA），<strong><span style="color:orange">线性判别分析本身也是一个分类模型</span></strong>。PCA和LDA有很多的相似点，<strong><span style="color:orange">其本质是要将原始的样本映射到维度更低的样本空间中</span></strong>，但是PCA和LDA的映射目标不一样：<strong><span style="color:#00f">PCA是为了让映射后的样本具有最大的发散性；而LDA是为了让映射后的样本有最好的分类性能</span></strong>。所以说PCA是一种无监督的降维方法，而LDA是一种有监督的降维方法。</p><h4 id="主成分分析法（PCA）"><a href="#主成分分析法（PCA）" class="headerlink" title="主成分分析法（PCA）"></a>主成分分析法（PCA）</h4><p>基本思想：构造原变量的一系列线性组合形成几个综合指标，以<strong><span style="color:red">去除数据的相关性</span></strong>，并使低维数据最大程度保持原始高维数据的方差信息。</p><p><strong>主成分个数的确定：</strong></p><ol><li>贡献率：第i个主成分的方差在全部方差中所占比重，反映第i个主成分所提取的总信息的份额。</li><li>累计贡献率：前k个主成分在全部方差中所占比重</li><li>主成分个数的确定：累计贡献率&gt;0.85</li></ol><p><strong>相关系数矩阵or协方差阵？</strong><br>当涉及变量的量纲不同或取值范围相差较大的指标时，应考虑从相关系数矩阵出发进行主成分分析；（相关系数矩阵消除了量纲的影响。）对同度量或取值范围相差不大的数据，从协方差阵出发。</p><p>使用decomposition库的PCA类选择特征的代码如下：</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="false" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line"><span class="comment">#主成分分析法，返回降维后的数据</span></span><br><span class="line"><span class="comment">#参数n_components为主成分数目</span></span><br><span class="line">PCA(n_components=<span class="number">2</span>).fit_transform(X) <span class="comment">#X=iris.data（鸢尾花）数据集</span></span><br></pre></td></tr></table></figure></div><p>n_components：</p><ul><li>小数：表示保留百分之多少的信息</li><li>整数：减少到多少特征</li></ul><h4 id="线性判别分析法（LDA）"><a href="#线性判别分析法（LDA）" class="headerlink" title="线性判别分析法（LDA）"></a>线性判别分析法（LDA）</h4><p>至多能把C类数据降维到C-1维子空间</p><p>使用lda库的LDA类选择特征的代码如下：</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="false" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.lda <span class="keyword">import</span> LDA</span><br><span class="line"><span class="comment">#线性判别分析法，返回降维后的数据</span></span><br><span class="line"><span class="comment">#参数n_components为降维后的维数</span></span><br><span class="line">LDA(n_components=<span class="number">2</span>).fit_transform(X,Y) <span class="comment">#X=iris.data,Y= iris.target（鸢尾花）数据集</span></span><br></pre></td></tr></table></figure></div><h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p><img src="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/机器学习图库/2.JPG" alt="2"></p><h2 id="分类算法"><a href="#分类算法" class="headerlink" title="分类算法"></a>分类算法</h2><p>分类问题：目标值—类别</p><h3 id="sklearn转换器和估计器"><a href="#sklearn转换器和估计器" class="headerlink" title="sklearn转换器和估计器"></a>sklearn转换器和估计器</h3><h4 id="转换器"><a href="#转换器" class="headerlink" title="转换器"></a>转换器</h4><p>1.实例化 (实例化的是一个转换器类(Transformer))<br>2 调用fit_transform(对于文档建立分类词频矩阵，不能同时调用)<br>标准化：<br>(x - mean) / std<br>fit_transform()<br>fit() #计算 每一列的平均值、标准差<br>transform() # (x - mean) / std进行最终的转换</p><h4 id="估计器"><a href="#估计器" class="headerlink" title="估计器"></a>估计器</h4><h6 id="sklearn机器学习算法的实现"><a href="#sklearn机器学习算法的实现" class="headerlink" title="sklearn机器学习算法的实现"></a>sklearn机器学习算法的实现</h6><p>1、用于分类的估计器：</p><ul><li>sklearn.neighbors k-近邻算法</li><li>sklearn.naive_bayes 贝叶斯</li><li>sklearn.linear_model.LogisticRegression 逻辑回归</li><li>sklearn.tree 决策树与随机森林</li></ul><p>2、用于回归的估计器：</p><ul><li>sklearn.linear_model.LinearRegression 线性回归</li><li>sklearn.linear_model.Ridge 岭回归</li></ul><p>3、用于无监督学习的估计器</p><ul><li>sklearn.cluster.KMeans 聚类</li></ul><h6 id="估计器工作流程"><a href="#估计器工作流程" class="headerlink" title="估计器工作流程"></a>估计器工作流程</h6><p>1、实例化一个estimator</p><p>2、estimator.fit(x_train, y_train) 计算<br>—— 调用完毕，模型生成</p><p>3 模型评估：<br>1）直接比对真实值和预测值<br>y_predict = estimator.predict(x_test)<br>y_test y_predict<br>2）计算准确率<br>accuracy = estimator.score(x_test, y_test)</p><h3 id="K-近邻算法"><a href="#K-近邻算法" class="headerlink" title="K-近邻算法"></a>K-近邻算法</h3><h4 id="K-近邻算法（KNN）"><a href="#K-近邻算法（KNN）" class="headerlink" title="K-近邻算法（KNN）"></a>K-近邻算法（KNN）</h4><p>K-近邻算法(KNN)理论/原理：“物以类聚，人以群分”</p><p>相同/近似样本在样本空间中是比较接近的，所以可以使用和当前样本比较近的其他样本的目标属性值作为当前样本的预测值。</p><p>k-近邻算法的工作机制很简单：</p><p>给定测试样本，基于某种距离度量（一般使用欧几里德距离）找出训练集中与其最靠近的k个训练样本，然后基于这k个“邻居”的信息来进行预测。</p><p>如何确定谁是邻居？</p><p>计算距离：</p><p>距离公式：<br>欧氏距离：<script type="math/tex">d = \sqrt{(x1 - y1)^2 + (x2 - y2)^2 + (x3 - y3)^2 + ……}</script><br>曼哈顿距离—绝对值距离<br>明可夫斯基距离：欧氏距离和曼哈顿距离的推广 metric_params=None</p><p><img src="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/机器学习图库/123.jpg" alt="123"></p><p>图中<strong><span style="color:red">红线</span></strong>代表曼哈顿距离，<strong><span style="color:green">绿线</span></strong>代表欧氏距离，也就是直线距离，而<strong><span style="color:#00f">蓝线</span></strong>和<strong><span style="color:#ff0">黄线</span></strong>代表等价的曼哈顿距离。</p><h4 id="电影类型分析"><a href="#电影类型分析" class="headerlink" title="电影类型分析"></a>电影类型分析</h4><p>假设我们有现在几部电影</p><p><img src="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/机器学习图库/电影类型分析.png" alt="电影类型分析"></p><p>其中？ 号电影不知道类别，如何去预测？我们可以利用K近邻算法的思想</p><p><img src="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/机器学习图库/电影距离计算.png" alt="电影距离计算"></p><p>k = 1 ——&gt;最近距离18.7——&gt;电影为爱情片——&gt;预测？号电影为爱情片<br>k = 2 ——&gt;最近距离18.7和19.2——两部电影都是爱情片——预测？号电影为爱情片<br>……<br>k = 6——&gt; 六部电影爱情片和动作片一样多——&gt;无法确定<br>k = 7 ——&gt;若4部动作片，3部爱情片——&gt;预测？号电影为动作片,但实际电影为爱情片</p><p>如果取的最近的电影数量不一样？会是什么结果？<br>k 值取得过小，容易受到异常点的影响<br>k 值取得过大，样本不均衡的影响</p><h4 id="K-近邻算法API"><a href="#K-近邻算法API" class="headerlink" title="K-近邻算法API"></a>K-近邻算法API</h4><p>sklearn.neighbors.KNeighborsClassifier(n_neighbors=5,weights=’uniform’,algorithm=’auto’,<br>leaf_size=30,p=2,metric=’minkowski’,metric_params=None,n_jobs=None,**kwargs)</p><ul><li>邻居数k: n_neighbors:int,可选(默认= 5)</li><li>权重weights: weights = ‘uniform’ weights = ‘distance’</li><li><p>距离度量: p=1距离度量采用曼哈顿距离；p=2距离度量采用欧氏距离</p></li><li><p>algorithm：{‘auto’，‘ball_tree’，‘kd_tree’，‘brute’}，可选用于计算最近邻居的算法：‘ball_tree’将会使用 BallTree，‘kd_tree’将使用 KDTree。‘auto’将尝试根据传递给fit方法的值来决定最合适的算法。 (不同实现方式影响效率)</p></li></ul><h4 id="案例：鸢尾花种类预测"><a href="#案例：鸢尾花种类预测" class="headerlink" title="案例：鸢尾花种类预测"></a>案例：鸢尾花种类预测</h4><p>代码：</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="false" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#1)导入库</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"><span class="comment">#2)获取数据</span></span><br><span class="line">x,y = datasets.load_iris(<span class="literal">True</span>)</span><br><span class="line"><span class="comment">#3）划分数据集</span></span><br><span class="line">x_train,x_test,y_train,y_test = train_test_split(x,y,test_size  = <span class="number">0.2</span>)</span><br><span class="line"><span class="comment">#4）特征工程：标准化</span></span><br><span class="line">ss=StandardScaler()</span><br><span class="line">x_train=ss.fit_transform(x_train)</span><br><span class="line">x_test=ss.transform(x_test)</span><br><span class="line"><span class="comment">#5)KNN算法预估器(训练数据)</span></span><br><span class="line">estimator=KNeighborsClassifier(n_neighbors=<span class="number">3</span>) <span class="comment">#设置k=3</span></span><br><span class="line">estimator.fit(x_train,y_train)</span><br><span class="line"><span class="comment">#6)模型评估</span></span><br><span class="line"><span class="comment">#方法1：直接比对真实值和预测值</span></span><br><span class="line">y_predict=estimator.predict(x_test)</span><br><span class="line">print(<span class="string">'y_predict:\n'</span>,y_predict)</span><br><span class="line">print(<span class="string">'直接比对真实值和预测值：\n'</span>,y_test  y_predict)</span><br><span class="line"><span class="comment">#方法2：计算准确率</span></span><br><span class="line">score=estimator.score(x_test,y_test)</span><br><span class="line">print(<span class="string">'准确率为：\n'</span>,score)</span><br></pre></td></tr></table></figure></div><p>输出结果：</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="false" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">y_predict:</span><br><span class="line"> [<span class="number">0</span> <span class="number">2</span> <span class="number">0</span> <span class="number">2</span> <span class="number">2</span> <span class="number">0</span> <span class="number">2</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">2</span> <span class="number">0</span> <span class="number">2</span> <span class="number">2</span> <span class="number">0</span> <span class="number">0</span> <span class="number">2</span> <span class="number">1</span> <span class="number">0</span> <span class="number">2</span> <span class="number">2</span> <span class="number">1</span> <span class="number">2</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span>]</span><br><span class="line">直接比对真实值和预测值：</span><br><span class="line"> [ <span class="literal">True</span>  <span class="literal">True</span>  <span class="literal">True</span>  <span class="literal">True</span>  <span class="literal">True</span>  <span class="literal">True</span>  <span class="literal">True</span>  <span class="literal">True</span>  <span class="literal">True</span>  <span class="literal">True</span>  <span class="literal">True</span>  <span class="literal">True</span></span><br><span class="line">  <span class="literal">True</span>  <span class="literal">True</span>  <span class="literal">True</span>  <span class="literal">True</span>  <span class="literal">True</span>  <span class="literal">True</span>  <span class="literal">True</span>  <span class="literal">True</span>  <span class="literal">True</span>  <span class="literal">True</span>  <span class="literal">True</span>  <span class="literal">True</span></span><br><span class="line">  <span class="literal">True</span>  <span class="literal">True</span>  <span class="literal">True</span>  <span class="literal">True</span> <span class="literal">False</span>  <span class="literal">True</span>]</span><br><span class="line">准确率为：</span><br><span class="line"> <span class="number">0.9666666666666667</span></span><br></pre></td></tr></table></figure></div><h4 id="K-近邻总结"><a href="#K-近邻总结" class="headerlink" title="K-近邻总结"></a>K-近邻总结</h4><p>优点：简单，易于理解，易于实现，无需训练<br>缺点：<br>1）必须指定K值，K值选择不当则分类精度不能保证<br>2）懒惰算法，对测试样本分类时的计算量大，内存开销大<br>使用场景：小数据场景，几千～几万样本，具体场景具体业务去测试</p><h3 id="模型选择与调优"><a href="#模型选择与调优" class="headerlink" title="模型选择与调优"></a>模型选择与调优</h3><h4 id="什么是交叉验证-cross-validation"><a href="#什么是交叉验证-cross-validation" class="headerlink" title="什么是交叉验证(cross validation)"></a>什么是交叉验证(cross validation)</h4><p>交叉验证：将拿到的训练数据，分为训练和验证集。以下图为例：将数据分成4份，其中一份作为验证集。然后经过4次(组)的测试，每次都更换不同的验证集。即得到4组模型的结果，取平均值作为最终结果。又称4折交叉验证。</p><ul><li><p>训练集：训练集+验证集</p></li><li><p>测试集：测试集</p><p><img src="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/机器学习图库/交叉验证过程.png" alt="交叉验证过程"></p></li></ul><h4 id="超参数搜索-网格搜索-Grid-Search"><a href="#超参数搜索-网格搜索-Grid-Search" class="headerlink" title="超参数搜索-网格搜索(Grid Search)"></a>超参数搜索-网格搜索(Grid Search)</h4><p>通常情况下，<strong>有很多参数是需要手动指定的（如k-近邻算法中的K值），这种叫超参数</strong>。但是手动过程繁杂，所以需要对模型预设几种超参数组合。<strong>每组超参数都采用交叉验证来进行评估。最后选出最优参数组合建立模型。</strong></p><h6 id="模型选择与调优-1"><a href="#模型选择与调优-1" class="headerlink" title="模型选择与调优:"></a>模型选择与调优:</h6><p>sklearn.model_selection.GridSearchCV(estimator, param_grid=None,cv=None)</p><ul><li>对估计器的指定参数值进行详尽搜索</li><li>estimator：估计器对象</li><li>param_grid：估计器参数(dict){“n_neighbors”:[1,3,5]}</li><li>cv：指定几折交叉验证</li><li>fit：输入训练数据</li><li>score：准确率</li><li>结果分析：<br>bestscore:在交叉验证中验证的最好结果_<br>bestestimator：最好的参数模型<br>cvresults:每次交叉验证后的验证集准确率结果和训练集准确率结果</li></ul><h4 id="鸢尾花案例调优"><a href="#鸢尾花案例调优" class="headerlink" title="鸢尾花案例调优"></a>鸢尾花案例调优</h4><p>代码：</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="false" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"><span class="comment"># grid网格，search搜索，cv：cross_validation</span></span><br><span class="line"><span class="comment"># 搜索算法最合适的参数</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"><span class="comment">#1)获取数据</span></span><br><span class="line">x,y = datasets.load_iris(<span class="literal">True</span>)</span><br><span class="line"><span class="comment">#2）划分数据集</span></span><br><span class="line">x_train,x_test,y_train,y_test = train_test_split(x,y,test_size  = <span class="number">0.2</span>)</span><br><span class="line"><span class="comment">#3）特征工程：标准化</span></span><br><span class="line">ss=StandardScaler()</span><br><span class="line">x_train=ss.fit_transform(x_train)</span><br><span class="line">x_test=ss.transform(x_test)</span><br><span class="line"><span class="comment">#4)KNN算法预估器</span></span><br><span class="line">estimator=KNeighborsClassifier()</span><br><span class="line"><span class="comment">#网格搜索GridSearchCV进行最佳参数的查找</span></span><br><span class="line">params = &#123;<span class="string">'n_neighbors'</span>:[i <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">30</span>)],</span><br><span class="line">          <span class="string">'weights'</span>:[<span class="string">'distance'</span>,<span class="string">'uniform'</span>],</span><br><span class="line">          <span class="string">'p'</span>:[<span class="number">1</span>,<span class="number">2</span>]&#125;</span><br><span class="line"><span class="comment"># cross_val_score类似</span></span><br><span class="line">estimator = GridSearchCV(estimator,param_grid=params,scoring=<span class="string">'accuracy'</span>,cv = <span class="number">6</span>)</span><br><span class="line">estimator.fit(x_train,y_train)</span><br><span class="line"><span class="comment">#5)模型评估</span></span><br><span class="line"><span class="comment">#方法1：直接比对真实值和预测值</span></span><br><span class="line">y_predict=estimator.predict(x_test)</span><br><span class="line">print(<span class="string">'y_predict:\n'</span>,y_predict)</span><br><span class="line">print(<span class="string">'直接比对真实值和预测值：\n'</span>,y_test  y_predict)</span><br><span class="line"><span class="comment">#方法2：计算准确率</span></span><br><span class="line">score=estimator.score(x_test,y_test)</span><br><span class="line">print(<span class="string">'准确率为：\n'</span>,score)</span><br><span class="line"></span><br><span class="line"><span class="comment">#查看了GridSearchCV最佳的参数组合</span></span><br><span class="line"><span class="comment">#最佳参数：best_params_</span></span><br><span class="line">print(<span class="string">'最佳参数：\n'</span>,estimator.best_params_)</span><br><span class="line"><span class="comment">#最佳结果：best_score_</span></span><br><span class="line">print(<span class="string">'最佳结果：\n'</span>,estimator.best_score_)</span><br><span class="line"><span class="comment">#最佳估计器：best_estimator_</span></span><br><span class="line">print(<span class="string">'最佳估计器：\n'</span>,estimator.best_estimator_)</span><br><span class="line"><span class="comment">#交叉验证结果：cv_results_</span></span><br><span class="line"><span class="comment">#print('交叉验证结果：\n',estimator.cv_results_)</span></span><br></pre></td></tr></table></figure></div><p>输出结果：</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="false" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">y_predict:</span><br><span class="line"> [<span class="number">1</span> <span class="number">1</span> <span class="number">2</span> <span class="number">2</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">2</span> <span class="number">1</span> <span class="number">2</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">2</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">2</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span>]</span><br><span class="line">直接比对真实值和预测值：</span><br><span class="line"> [ <span class="literal">True</span>  <span class="literal">True</span>  <span class="literal">True</span>  <span class="literal">True</span>  <span class="literal">True</span>  <span class="literal">True</span>  <span class="literal">True</span>  <span class="literal">True</span> <span class="literal">False</span>  <span class="literal">True</span>  <span class="literal">True</span>  <span class="literal">True</span></span><br><span class="line">  <span class="literal">True</span>  <span class="literal">True</span>  <span class="literal">True</span>  <span class="literal">True</span> <span class="literal">False</span> <span class="literal">False</span>  <span class="literal">True</span>  <span class="literal">True</span>  <span class="literal">True</span>  <span class="literal">True</span>  <span class="literal">True</span>  <span class="literal">True</span></span><br><span class="line">  <span class="literal">True</span>  <span class="literal">True</span>  <span class="literal">True</span>  <span class="literal">True</span> <span class="literal">False</span>  <span class="literal">True</span>]</span><br><span class="line">准确率为：</span><br><span class="line"> <span class="number">0.8666666666666667</span></span><br><span class="line">最佳参数：</span><br><span class="line"> &#123;<span class="string">'n_neighbors'</span>: <span class="number">15</span>, <span class="string">'p'</span>: <span class="number">2</span>, <span class="string">'weights'</span>: <span class="string">'distance'</span>&#125;</span><br><span class="line">最佳结果：</span><br><span class="line"> <span class="number">0.9833333333333333</span></span><br><span class="line">最佳估计器：</span><br><span class="line"> KNeighborsClassifier(algorithm=<span class="string">'auto'</span>, leaf_size=<span class="number">30</span>, metric=<span class="string">'minkowski'</span>,</span><br><span class="line">                     metric_params=<span class="literal">None</span>, n_jobs=<span class="literal">None</span>, n_neighbors=<span class="number">15</span>, p=<span class="number">2</span>,</span><br><span class="line">                     weights=<span class="string">'distance'</span>)</span><br></pre></td></tr></table></figure></div><p>注：最佳结果—训练集再分为训练集+验证集，验证集的效果；准确率—整体测试集的效果。数据集不同</p><h3 id="朴素贝叶斯算法"><a href="#朴素贝叶斯算法" class="headerlink" title="朴素贝叶斯算法"></a>朴素贝叶斯算法</h3><p>朴素贝叶斯分类器是基于概率论的分类模型，其思想是先计算样本的先验概率，然后利用贝叶斯公式测算未知样本属于某个类别的后验概率，最终以最大后验概率对应的类别作为未知样本的预测类别。之所以叫”朴素”，是因为整个形式化过程只做最简单、最原始的假设。</p><h4 id="理论基础"><a href="#理论基础" class="headerlink" title="理论基础"></a>理论基础</h4><h5 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h5><p><img src="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/机器学习图库/1234.JPG" alt="1234"></p><p>问题：</p><p>1、女神喜欢的概率？ $P(喜欢)=\frac{4}{7}$</p><p>2、职业是程序员并且体型匀称的概率？ $P(程序员，匀称)=\frac{1}{7}$ (联合概率)</p><p>3、在女神喜欢的条件下，职业是程序员的概率？ $P(程序员|喜欢)=\frac{2}{4}=\frac{1}{2}$ （条件概率）</p><p>4、在女神喜欢的条件下，职业是产品，体重是超重的概率？</p><p>​ $P(程序员，超重|喜欢)=\frac{1}{4}$ （联合概率、条件概率）</p><p>相互独立：$P(AB)=P(A)P(B)$&lt;=&gt;事件A与事件B相互独立</p><p>$P(程序员，匀称)=\frac{1}{7}$ $P(程序员)=\frac{3}{7}$ $P(匀称)=\frac{4}{7}$ $P(程序员，匀称)≠P(程序员)P(匀称)$ 不独立</p><p><strong><span style="color:red">贝叶斯公式：</span></strong>$P(A|B)=\frac{P(B|A)P(A)}{P(B)}$</p><p>$P(喜欢|产品经理,超重)=\frac{P(产品经理,超重|喜欢)P(喜欢)}{P(产品经理,超重)}=0$</p><p>朴素? 假设:特征与特征之间是相互独立的</p><p><strong><span style="color:red">朴素贝叶斯算法</span></strong>=朴素+贝叶斯</p><p>$P(喜欢|产品经理,超重)=\frac{P(产品经理,超重|喜欢)P(喜欢)}{P(产品经理,超重)}=\frac{P(产品经理|喜欢)P(超重|喜欢)P(喜欢)}{P(产品经理)P(超重)}=\frac{7}{12}$</p><p><img src="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/机器学习图库/8d16d796670bb4901c7a4c13ca3aa1fa.jpg" alt="img"></p><h4 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h4><p>朴素贝叶斯算法应用场景：文本分类</p><p>公式：$P(C|W)=\frac{P(W|C)P(C)}{P(W)}$</p><p>注：$W$为给定文档的特征值（频数统计，预测文档提供），$C$为文档类别</p><p><strong>公式如果应用在文章分类的场景当中，我们可以这样看：</strong></p><script type="math/tex;mode=display">P(C|F1,F2,…)=\frac{P(F1,F2,…|C)P(C)}{P(F1,F2,…)}</script><p><strong>其中$C$可以是不同类别</strong></p><p>公式分为三个部分：</p><ul><li><p>$P(C)$：每个文档类别的概率(某文档类别数／总文档数量)</p></li><li><p>$P(W|C)$：给定类别下特征（被预测文档中出现的词）的概率</p><p>​ 计算方法：$P(F1│C)=Ni/N $（训练文档中去计算）</p><p>​ $Ni$为该$F1$词在$C$类别所有文档中出现的次数<br>​ $N$为所属类别$C$下的文档所有词出现的次数和</p></li><li><p>$P(F1,F2,…) $预测文档中每个词的概率</p></li></ul><h4 id="常用贝叶斯分类器"><a href="#常用贝叶斯分类器" class="headerlink" title="常用贝叶斯分类器"></a>常用贝叶斯分类器</h4><h5 id="1-高斯贝叶斯分类器"><a href="#1-高斯贝叶斯分类器" class="headerlink" title="1.高斯贝叶斯分类器"></a>1.高斯贝叶斯分类器</h5><p>适用条件：自变量X均为连续的数值型<br>假设条件：自变量X服从高斯正态分布<br>自变量X的条件概率：$P(x_j∣C_i)=\frac{1}{\sqrt{2\pi}\sigma_{ji}}exp(-\frac{(x_j-\mu_{ji})^2}{2\sigma_{ji}^2})$</p><p>其中$x_j$为第$j$个自变量的取值，$μ_{ji}$为训练集中自变量$x_j$属于类别 $C_i$的均值，$σ_{ji}$为训练集中自变量$x_j$属于类别 $C_i$的标准差。</p><h5 id="2-多项式贝叶斯分类器"><a href="#2-多项式贝叶斯分类器" class="headerlink" title="2.多项式贝叶斯分类器"></a>2.多项式贝叶斯分类器</h5><p>适用条件：自变量X均为离散型变量<br>假设条件：自变量X的条件概率服从多项式分布<br>自变量X的条件概率：$P(x_j=x_{jk}∣C_i)=\frac{N_{ik}+\alpha}{N_i+n\alpha}$</p><p>其中$x_{jk}$为自变量$x_j$的第$k$个取值，$N_{ik}$表示因变量为类别$C_i$时自变量$x_j$取值$x_{jk}$的样本个数，$N_i$为类别$C_i$的样本个数，$α$为平滑系数（防止条件概率等于0，通常取1），n为训练文档中统计出的<strong><span style="color:red">特征词</span></strong>个数。</p><h5 id="3-伯努利贝叶斯分类器"><a href="#3-伯努利贝叶斯分类器" class="headerlink" title="3.伯努利贝叶斯分类器"></a>3.伯努利贝叶斯分类器</h5><p>适用条件：自变量X均为0-1二元变量<br>假设条件：自变量X的条件概率服从伯努利分布<br>自变量X的条件概率</p><p>$P(x_j∣C_i)=px_j+(1−p)(1−x_j)$</p><p>其中$x_j$为第$j$个自变量，其取值为0或1；$p$表示类别为$C_i$时自变量取1的概率，可以用经验频率代替</p><p>$p=P(x_j=1∣C_i)=\frac{N_{i1}+α}{N_i+nα}$</p><p>$N_{i1}$表示在类别$C_i$时自变量$x_j$取1的样本量。</p><h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><h5 id="1-高斯贝叶斯分类器-1"><a href="#1-高斯贝叶斯分类器-1" class="headerlink" title="1.高斯贝叶斯分类器"></a>1.高斯贝叶斯分类器</h5><p>代码：</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="false" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#高斯贝叶斯分类器进行癌症预测</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="comment"># 1、读取数据</span></span><br><span class="line">column_name = [<span class="string">'Sample code number'</span>, <span class="string">'Clump Thickness'</span>, <span class="string">'Uniformity of Cell Size'</span>, <span class="string">'Uniformity of Cell Shape'</span>,</span><br><span class="line">                   <span class="string">'Marginal Adhesion'</span>, <span class="string">'Single Epithelial Cell Size'</span>, <span class="string">'Bare Nuclei'</span>, <span class="string">'Bland Chromatin'</span>,</span><br><span class="line">                   <span class="string">'Normal Nucleoli'</span>, <span class="string">'Mitoses'</span>, <span class="string">'Class'</span>]</span><br><span class="line">data = pd.read_csv(<span class="string">"https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data"</span>,</span><br><span class="line">                       names=column_name)</span><br><span class="line"><span class="comment"># 2、数据处理—处理缺失值</span></span><br><span class="line">data = data.replace(to_replace=<span class="string">'?'</span>, value=np.nan)   <span class="comment">#1)替换np.nan</span></span><br><span class="line">data = data.dropna()    <span class="comment">#2)删除缺失值</span></span><br><span class="line">print(data.isnull().any()) <span class="comment">#确认不存在缺失值</span></span><br><span class="line">print(<span class="string">'-----------------------------------'</span>)</span><br><span class="line"><span class="comment"># 取出特征值</span></span><br><span class="line">x = data[column_name[<span class="number">1</span>:<span class="number">10</span>]]  <span class="comment">#x=data.iloc[:,1:-1]</span></span><br><span class="line">y = data[column_name[<span class="number">10</span>]]   <span class="comment">#y=data['Class']</span></span><br><span class="line"><span class="comment">#设置正例和负例</span></span><br><span class="line">y = y.map(&#123;<span class="number">2</span>:<span class="number">0</span>,<span class="number">4</span>:<span class="number">1</span>&#125;)</span><br><span class="line">print(y.value_counts())</span><br><span class="line">print(<span class="string">'-----------------------------------'</span>)</span><br><span class="line"><span class="comment">#3、分割数据集</span></span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=<span class="number">0.3</span>)</span><br><span class="line"><span class="comment">#4、特征工程—标准化</span></span><br><span class="line">std = StandardScaler()</span><br><span class="line">x_train = std.fit_transform(x_train)</span><br><span class="line">x_test = std.transform(x_test)</span><br><span class="line"><span class="comment">#5、estimator估计器流程</span></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> naive_bayes</span><br><span class="line"><span class="comment">#调用高斯朴素贝叶斯分类器的“类”</span></span><br><span class="line">gnb = naive_bayes.GaussianNB()</span><br><span class="line"><span class="comment">#模型拟合</span></span><br><span class="line">gnb.fit(x_train,y_train)</span><br><span class="line"><span class="comment">#6、进行预测（模型评估）</span></span><br><span class="line"><span class="comment">#模型在测试数据集上的预测</span></span><br><span class="line">gnb_pred = gnb.predict(x_test)</span><br><span class="line"><span class="comment">#各类别的预测数量</span></span><br><span class="line">print(pd.Series(gnb_pred).value_counts())</span><br><span class="line">print(<span class="string">'-----------------------------------'</span>)</span><br><span class="line"><span class="comment">#导入第三方包</span></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="comment">#构建混淆矩阵</span></span><br><span class="line">cm = pd.crosstab(gnb_pred,y_test)</span><br><span class="line"><span class="comment">#绘制混淆矩阵图</span></span><br><span class="line">sns.heatmap(cm,annot = <span class="literal">True</span> , cmap = <span class="string">'GnBu'</span> , fmt = <span class="string">'d'</span>)</span><br><span class="line"><span class="comment">#去除x轴和y轴的标签</span></span><br><span class="line">plt.xlabel(<span class="string">'Real'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Predict'</span>)</span><br><span class="line"><span class="comment">#显示图形</span></span><br><span class="line">plt.show()</span><br><span class="line">print(<span class="string">'模型的准确率：\n'</span>,metrics.accuracy_score(y_test,gnb_pred))</span><br><span class="line">print(<span class="string">'模型的评估报告：\n'</span>,metrics.classification_report(y_test,gnb_pred))</span><br><span class="line"></span><br><span class="line"><span class="comment">#计算正例的预测概率，用于生成ROC曲线的数据</span></span><br><span class="line">y_score = gnb.predict_proba(x_test)[:,<span class="number">1</span>]</span><br><span class="line">fpr,tpr,threshold = metrics.roc_curve(y_test,y_score)</span><br><span class="line"><span class="comment">#计算AUC的值</span></span><br><span class="line">roc_auc = metrics.auc(fpr,tpr)</span><br><span class="line"><span class="comment">#绘制面积图</span></span><br><span class="line">plt.stackplot(fpr,tpr,color = <span class="string">'steelblue'</span>,alpha = <span class="number">0.5</span>,edgecolor = <span class="string">'black'</span>)</span><br><span class="line"><span class="comment">#添加边际线</span></span><br><span class="line">plt.plot(fpr,tpr,color= <span class="string">'black'</span>,lw =<span class="number">1</span>)</span><br><span class="line"><span class="comment">#添加对角线</span></span><br><span class="line">plt.plot([<span class="number">0</span>,<span class="number">1</span>],[<span class="number">0</span>,<span class="number">1</span>],color = <span class="string">'red'</span>,linestyle = <span class="string">'--'</span>)</span><br><span class="line"><span class="comment">#添加文本信息</span></span><br><span class="line">plt.text(<span class="number">0.5</span>,<span class="number">0.3</span>,<span class="string">'ROC curve(area = %0.2f)'</span>% roc_auc)</span><br><span class="line"><span class="comment">#添加x轴与y轴标签</span></span><br><span class="line">plt.xlabel(<span class="string">'1-Specificity'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Sensitivity'</span>)</span><br><span class="line"><span class="comment">#显示图形</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></div><p>输出结果：</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="false" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">Sample code number             <span class="literal">False</span></span><br><span class="line">Clump Thickness                <span class="literal">False</span></span><br><span class="line">Uniformity of Cell Size        <span class="literal">False</span></span><br><span class="line">Uniformity of Cell Shape       <span class="literal">False</span></span><br><span class="line">Marginal Adhesion              <span class="literal">False</span></span><br><span class="line">Single Epithelial Cell Size    <span class="literal">False</span></span><br><span class="line">Bare Nuclei                    <span class="literal">False</span></span><br><span class="line">Bland Chromatin                <span class="literal">False</span></span><br><span class="line">Normal Nucleoli                <span class="literal">False</span></span><br><span class="line">Mitoses                        <span class="literal">False</span></span><br><span class="line">Class                          <span class="literal">False</span></span><br><span class="line">dtype: bool</span><br><span class="line">-----------------------------------</span><br><span class="line"><span class="number">0</span>    <span class="number">444</span></span><br><span class="line"><span class="number">1</span>    <span class="number">239</span></span><br><span class="line">Name: Class, dtype: int64</span><br><span class="line">-----------------------------------</span><br><span class="line"><span class="number">0</span>    <span class="number">123</span></span><br><span class="line"><span class="number">1</span>     <span class="number">82</span></span><br><span class="line">dtype: int64</span><br><span class="line">-----------------------------------</span><br><span class="line">模型的准确率：</span><br><span class="line"> <span class="number">0.9707317073170731</span></span><br><span class="line">模型的评估报告：</span><br><span class="line">               precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">           <span class="number">0</span>       <span class="number">0.99</span>      <span class="number">0.96</span>      <span class="number">0.98</span>       <span class="number">127</span></span><br><span class="line">           <span class="number">1</span>       <span class="number">0.94</span>      <span class="number">0.99</span>      <span class="number">0.96</span>        <span class="number">78</span></span><br><span class="line"></span><br><span class="line">    accuracy                           <span class="number">0.97</span>       <span class="number">205</span></span><br><span class="line">   macro avg       <span class="number">0.97</span>      <span class="number">0.97</span>      <span class="number">0.97</span>       <span class="number">205</span></span><br><span class="line">weighted avg       <span class="number">0.97</span>      <span class="number">0.97</span>      <span class="number">0.97</span>       <span class="number">205</span></span><br></pre></td></tr></table></figure></div><p><img src="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/机器学习图库/12.png" alt="12"></p><p><img src="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/机器学习图库/13.png" alt="13"></p><h5 id="2-多项式贝叶斯分类器-1"><a href="#2-多项式贝叶斯分类器-1" class="headerlink" title="2.多项式贝叶斯分类器"></a>2.多项式贝叶斯分类器</h5><p>代码：</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="false" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> model_selection,naive_bayes,metrics</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">data=pd.read_csv(<span class="string">r'mushrooms.csv'</span>)</span><br><span class="line">print(data.head())</span><br><span class="line"><span class="comment">#使用factorize函数将字符型数据做因子化处理，将其转换为整数型数据</span></span><br><span class="line"><span class="comment">#factorize函数返回的是两个元素的元组，第一个元素为转换成的数值，第二个元素为数值对应的字符水平</span></span><br><span class="line">columns=data.columns[<span class="number">1</span>:]</span><br><span class="line"><span class="keyword">for</span> column <span class="keyword">in</span> columns:</span><br><span class="line">    data[column]=pd.factorize(data[column])[<span class="number">0</span>]</span><br><span class="line"><span class="comment">#拆分为训练集和测试集</span></span><br><span class="line">x_train,x_test,y_train,y_test=model_selection.train_test_split(data[columns],data.type,test_size=<span class="number">0.25</span>,random_state=<span class="number">1234</span>)</span><br><span class="line"><span class="comment">#调用多项式朴素贝叶斯</span></span><br><span class="line">mnb=naive_bayes.MultinomialNB()</span><br><span class="line">mnb.fit(x_train,y_train)</span><br><span class="line">mnb_pred=mnb.predict(x_test)</span><br><span class="line"><span class="comment">#显示预测结果，各类别的预测数量</span></span><br><span class="line"><span class="comment">#模型检验</span></span><br><span class="line">print(<span class="string">'模型的准确率为：'</span>,metrics.accuracy_score(y_test,mnb_pred))</span><br><span class="line">print(<span class="string">'模型的评估报告：\n'</span>,metrics.classification_report(y_test,mnb_pred))</span><br><span class="line"><span class="comment">#绘制ROC曲线</span></span><br><span class="line">y_score=mnb.predict_proba(x_test)[:,<span class="number">1</span>]</span><br><span class="line">fpr,tpr,threshold=metrics.roc_curve(y_test.map(&#123;<span class="string">'e'</span>:<span class="number">0</span>,<span class="string">'p'</span>:<span class="number">1</span>&#125;),y_score)</span><br><span class="line">roc_auc=metrics.auc(fpr,tpr)</span><br><span class="line">plt.stackplot(fpr,tpr,color=<span class="string">'steelblue'</span>,alpha=<span class="number">0.5</span>,edgecolor=<span class="string">'black'</span>)</span><br><span class="line">plt.plot(fpr,tpr,color=<span class="string">'black'</span>,lw=<span class="number">1</span>)</span><br><span class="line">plt.plot([<span class="number">0</span>,<span class="number">1</span>],[<span class="number">0</span>,<span class="number">1</span>],color=<span class="string">'red'</span>,linestyle=<span class="string">'--'</span>)</span><br><span class="line">plt.text(<span class="number">0.5</span>,<span class="number">0.3</span>,<span class="string">'ROC Curve (area=%0.2f)'</span> % roc_auc)</span><br><span class="line">plt.xlabel(<span class="string">'l-Specificity'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Sensitivity'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></div><p>输出结果：</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="false" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">  type cap-shape cap-surface  ... spore-<span class="keyword">print</span>-color population habitat</span><br><span class="line"><span class="number">0</span>    p         x           s  ...                 k          s       u</span><br><span class="line"><span class="number">1</span>    e         x           s  ...                 n          n       g</span><br><span class="line"><span class="number">2</span>    e         b           s  ...                 n          n       m</span><br><span class="line"><span class="number">3</span>    p         x           y  ...                 k          s       u</span><br><span class="line"><span class="number">4</span>    e         x           s  ...                 n          a       g</span><br><span class="line"></span><br><span class="line">[<span class="number">5</span> rows x <span class="number">23</span> columns]</span><br><span class="line">模型的准确率为： <span class="number">0.8877400295420975</span></span><br><span class="line">模型的评估报告：</span><br><span class="line">               precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">           e       <span class="number">0.85</span>      <span class="number">0.95</span>      <span class="number">0.89</span>      <span class="number">1017</span></span><br><span class="line">           p       <span class="number">0.94</span>      <span class="number">0.83</span>      <span class="number">0.88</span>      <span class="number">1014</span></span><br><span class="line"></span><br><span class="line">    accuracy                           <span class="number">0.89</span>      <span class="number">2031</span></span><br><span class="line">   macro avg       <span class="number">0.89</span>      <span class="number">0.89</span>      <span class="number">0.89</span>      <span class="number">2031</span></span><br><span class="line">weighted avg       <span class="number">0.89</span>      <span class="number">0.89</span>      <span class="number">0.89</span>      <span class="number">2031</span></span><br></pre></td></tr></table></figure></div><p><img src="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/机器学习图库/Figure_1-1583391690020.png" alt="Figure_1"></p><h5 id="3-伯努利贝叶斯分类器-1"><a href="#3-伯努利贝叶斯分类器-1" class="headerlink" title="3.伯努利贝叶斯分类器"></a>3.伯努利贝叶斯分类器</h5><p>未完待续</p><h4 id="案例：新闻分类"><a href="#案例：新闻分类" class="headerlink" title="案例：新闻分类"></a>案例：新闻分类</h4><p>代码：</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="false" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#朴素贝叶斯对新闻数据集进行预测</span></span><br><span class="line"><span class="comment">#1）导入库</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> fetch_20newsgroups</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfVectorizer</span><br><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> MultinomialNB</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="comment">#2）获取新闻的数据，20个类别</span></span><br><span class="line">news = fetch_20newsgroups(subset=<span class="string">'all'</span>)</span><br><span class="line"><span class="comment">#3）进行数据集分割</span></span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(news.data, news.target, test_size=<span class="number">0.3</span>)</span><br><span class="line"><span class="comment">#4）对于文本数据，进行特征抽取</span></span><br><span class="line">tf = TfidfVectorizer()</span><br><span class="line">x_train = tf.fit_transform(x_train)</span><br><span class="line"><span class="comment">#这里打印出来的列表是：训练集当中的所有不同词的组成的一个列表</span></span><br><span class="line">print(tf.get_feature_names())</span><br><span class="line"><span class="comment"># print(x_train.toarray())</span></span><br><span class="line">x_test = tf.transform(x_test) <span class="comment"># 不需要fit_transform</span></span><br><span class="line"><span class="comment">#5）estimator估计器流程</span></span><br><span class="line">mnb = MultinomialNB(alpha=<span class="number">1.0</span>) <span class="comment">#默认alpha=1.0</span></span><br><span class="line">mnb.fit(x_train, y_train)</span><br><span class="line"><span class="comment">#6）进行预测（模型评估）</span></span><br><span class="line">y_predict = mnb.predict(x_test)</span><br><span class="line">print(<span class="string">'预测每篇文章的类别：\n'</span>, y_predict[:<span class="number">100</span>])</span><br><span class="line">print(<span class="string">'真实类别为：\n'</span>, y_test[:<span class="number">100</span>])</span><br><span class="line">print(<span class="string">'模型预测的准确率为：\n'</span>, mnb.score(x_test, y_test))</span><br><span class="line"><span class="comment">#print('模型预测的准确率为：\n',metrics.accuracy_score(y_test,y_predict))</span></span><br><span class="line"><span class="comment">#混淆矩阵</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line">print(<span class="string">'模型的评估报告：\n'</span>,classification_report(y_test,y_predict))</span><br></pre></td></tr></table></figure></div><p>输出结果：</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="false" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">'00'</span>, <span class="string">'000'</span>, <span class="string">'0000'</span>, <span class="string">'00000'</span>, <span class="string">'000000'</span>, <span class="string">'00000000'</span>, <span class="string">'0000000004'</span>, <span class="string">'0000000005'</span>, <span class="string">'00000000b'</span>, <span class="string">'00000001'</span>, <span class="string">'00000001b'</span>, <span class="string">'00000010'</span>,  ···································································   <span class="string">'zzrk'</span>, <span class="string">'zzs'</span>, <span class="string">'zzt'</span>, <span class="string">'zzvsi'</span>, <span class="string">'zzx'</span>, <span class="string">'zzy_3w'</span>, <span class="string">'zzzzzz'</span>, <span class="string">'zzzzzzt'</span>, <span class="string">'³ation'</span>, <span class="string">'íålittin'</span>, <span class="string">'ñaustin'</span>, <span class="string">'ýé'</span>, <span class="string">'ÿhooked'</span>]</span><br><span class="line">预测每篇文章的类别：</span><br><span class="line"> [<span class="number">16</span>  <span class="number">7</span>  <span class="number">2</span>  <span class="number">4</span> <span class="number">10</span>  <span class="number">3</span> <span class="number">18</span>  <span class="number">7</span>  <span class="number">8</span> <span class="number">12</span>  <span class="number">0</span> <span class="number">13</span> <span class="number">11</span> <span class="number">11</span>  <span class="number">0</span> <span class="number">15</span> <span class="number">18</span> <span class="number">10</span> <span class="number">12</span>  <span class="number">8</span> <span class="number">12</span>  <span class="number">2</span> <span class="number">15</span>  <span class="number">7</span></span><br><span class="line">  <span class="number">9</span> <span class="number">12</span> <span class="number">15</span> <span class="number">16</span>  <span class="number">9</span>  <span class="number">0</span>  <span class="number">3</span> <span class="number">15</span>  <span class="number">8</span>  <span class="number">3</span> <span class="number">14</span> <span class="number">17</span>  <span class="number">1</span>  <span class="number">0</span> <span class="number">15</span> <span class="number">16</span>  <span class="number">9</span>  <span class="number">2</span>  <span class="number">3</span>  <span class="number">6</span>  <span class="number">5</span>  <span class="number">4</span>  <span class="number">6</span> <span class="number">14</span></span><br><span class="line">  <span class="number">8</span>  <span class="number">9</span> <span class="number">13</span>  <span class="number">9</span> <span class="number">11</span> <span class="number">12</span> <span class="number">10</span> <span class="number">10</span>  <span class="number">3</span> <span class="number">11</span> <span class="number">11</span>  <span class="number">0</span>  <span class="number">6</span> <span class="number">12</span> <span class="number">15</span>  <span class="number">3</span> <span class="number">15</span>  <span class="number">6</span>  <span class="number">5</span>  <span class="number">1</span>  <span class="number">9</span> <span class="number">14</span> <span class="number">10</span>  <span class="number">3</span></span><br><span class="line">  <span class="number">7</span> <span class="number">11</span>  <span class="number">0</span>  <span class="number">3</span>  <span class="number">7</span> <span class="number">16</span> <span class="number">13</span>  <span class="number">9</span>  <span class="number">5</span> <span class="number">15</span>  <span class="number">1</span> <span class="number">13</span> <span class="number">15</span> <span class="number">16</span>  <span class="number">7</span>  <span class="number">4</span>  <span class="number">1</span> <span class="number">16</span> <span class="number">16</span> <span class="number">18</span> <span class="number">14</span> <span class="number">15</span>  <span class="number">7</span> <span class="number">16</span></span><br><span class="line"> <span class="number">15</span> <span class="number">14</span>  <span class="number">0</span> <span class="number">14</span>]</span><br><span class="line">真实类别为：</span><br><span class="line"> [<span class="number">19</span>  <span class="number">7</span>  <span class="number">2</span>  <span class="number">4</span> <span class="number">10</span>  <span class="number">3</span> <span class="number">18</span>  <span class="number">3</span>  <span class="number">8</span> <span class="number">12</span>  <span class="number">0</span> <span class="number">13</span> <span class="number">11</span> <span class="number">11</span>  <span class="number">0</span>  <span class="number">0</span> <span class="number">18</span> <span class="number">10</span> <span class="number">12</span> <span class="number">15</span> <span class="number">12</span>  <span class="number">2</span> <span class="number">15</span>  <span class="number">7</span></span><br><span class="line">  <span class="number">9</span> <span class="number">12</span> <span class="number">15</span> <span class="number">16</span>  <span class="number">9</span>  <span class="number">0</span>  <span class="number">3</span> <span class="number">15</span>  <span class="number">8</span>  <span class="number">3</span> <span class="number">14</span> <span class="number">17</span>  <span class="number">1</span>  <span class="number">0</span> <span class="number">15</span> <span class="number">16</span>  <span class="number">9</span>  <span class="number">2</span>  <span class="number">3</span>  <span class="number">8</span>  <span class="number">5</span>  <span class="number">4</span>  <span class="number">6</span> <span class="number">14</span></span><br><span class="line">  <span class="number">8</span>  <span class="number">9</span> <span class="number">13</span>  <span class="number">9</span> <span class="number">11</span> <span class="number">12</span> <span class="number">10</span> <span class="number">10</span>  <span class="number">3</span> <span class="number">11</span> <span class="number">11</span>  <span class="number">0</span>  <span class="number">6</span> <span class="number">12</span>  <span class="number">0</span>  <span class="number">3</span>  <span class="number">0</span>  <span class="number">6</span>  <span class="number">5</span>  <span class="number">1</span>  <span class="number">9</span> <span class="number">14</span> <span class="number">12</span>  <span class="number">6</span></span><br><span class="line">  <span class="number">8</span> <span class="number">12</span>  <span class="number">0</span>  <span class="number">3</span>  <span class="number">7</span> <span class="number">18</span> <span class="number">13</span>  <span class="number">9</span>  <span class="number">5</span> <span class="number">15</span>  <span class="number">1</span> <span class="number">13</span> <span class="number">15</span> <span class="number">16</span>  <span class="number">7</span>  <span class="number">4</span>  <span class="number">1</span> <span class="number">16</span> <span class="number">18</span> <span class="number">18</span> <span class="number">14</span> <span class="number">15</span>  <span class="number">7</span> <span class="number">13</span></span><br><span class="line"> <span class="number">15</span>  <span class="number">1</span>  <span class="number">0</span> <span class="number">14</span>]</span><br><span class="line">模型预测的准确率为：</span><br><span class="line"> <span class="number">0.8593915811814644</span></span><br><span class="line">    模型的评估报告：</span><br><span class="line">               precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">           <span class="number">0</span>       <span class="number">0.89</span>      <span class="number">0.76</span>      <span class="number">0.82</span>       <span class="number">235</span></span><br><span class="line">           <span class="number">1</span>       <span class="number">0.93</span>      <span class="number">0.72</span>      <span class="number">0.81</span>       <span class="number">320</span></span><br><span class="line"> ···································································</span><br><span class="line">          <span class="number">18</span>       <span class="number">0.99</span>      <span class="number">0.63</span>      <span class="number">0.77</span>       <span class="number">235</span></span><br><span class="line">          <span class="number">19</span>       <span class="number">1.00</span>      <span class="number">0.21</span>      <span class="number">0.35</span>       <span class="number">177</span></span><br><span class="line"></span><br><span class="line">    accuracy                           <span class="number">0.86</span>      <span class="number">5654</span></span><br><span class="line">   macro avg       <span class="number">0.88</span>      <span class="number">0.84</span>      <span class="number">0.84</span>      <span class="number">5654</span></span><br><span class="line">weighted avg       <span class="number">0.88</span>      <span class="number">0.86</span>      <span class="number">0.85</span>      <span class="number">5654</span></span><br></pre></td></tr></table></figure></div><h4 id="朴素贝叶斯总结"><a href="#朴素贝叶斯总结" class="headerlink" title="朴素贝叶斯总结"></a>朴素贝叶斯总结</h4><ul><li>优点：<ul><li>朴素贝叶斯模型发源于古典数学理论，有稳定的分类效率。</li><li>对缺失数据不太敏感，算法也比较简单，常用于文本分类。</li><li>分类准确度高，速度快</li></ul></li><li>缺点：<ul><li>由于使用了样本属性独立性的假设，所以如果特征属性有关联时其效果不好</li></ul></li></ul><h3 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h3><h4 id="认识决策树"><a href="#认识决策树" class="headerlink" title="认识决策树"></a>认识决策树</h4><p>​ 如何高效的进行决策？<br>​ 特征的先后顺序</p><h4 id="决策树分类原理详解"><a href="#决策树分类原理详解" class="headerlink" title="决策树分类原理详解"></a>决策树分类原理详解</h4><p>​ 已知 四个特征值 预测 是否贷款给某个人<br>​ 先看房子，再工作 -&gt; 是否贷款 只看了两个特征<br>​ 年龄，信贷情况，工作 看了三个特征</p><h5 id="信息论基础"><a href="#信息论基础" class="headerlink" title="信息论基础"></a>信息论基础</h5><p>1）<strong>信息</strong>：消除随机不确定性的东西<br>小明：“我今年18岁” - 信息<br>小华： ”小明明年19岁” - 不是信息(当小明告诉我他今年18岁，这就消除了对小明年龄的不确定性)<br>2）<strong>信息的衡量标准</strong> - <strong><span style="color:red">熵</span></strong><br>熵：表示随机变量不确定性的度量 熵单位为比特（bit）</p><p>（解释：说白了就是物体内部的混乱程度，比如杂货市场里面什么都有那肯定混乱，专卖店里面只卖一个牌子那就稳定多了）<br>公式：</p><script type="math/tex;mode=display">H（D）=-\sum_{i=1}^{n} p(x i) * \log _{2} p(x i)</script><p>熵：不确定性越大，得到的熵值也就越大</p><p>3）<strong>决策树的划分依据之一</strong>———<strong><span style="color:red">信息增益</span></strong></p><p>特征A对训练数据集D的信息增益$g(D,A)$,定义为集合D的熵$H(D)$与特征A给定条件下D的信息条件熵$H(D|A)$之差，即公式为：<script type="math/tex">g(D,A)=H(D)-H(D|A)</script></p><p>公式的详细解释：</p><p>熵的计算：<script type="math/tex">H(D)=-\sum_{k=1}^{K} \frac{\left|C_{k}\right|}{|D|} \log_{2} \frac{\left|C_{k}\right|}{|D|}</script></p><p>条件熵的计算公式：<script type="math/tex">H(D | A)=\sum_{i=1}^{n} \frac{\left|D_{i}\right|}{|D|} H\left(D_{i}\right)=-\sum_{i=1}^{n} \frac{\left|D_{i}\right|}{|D|} \sum_{k=1}^{K} \frac{\left|D_{i k}\right|}{\left|D_{i}\right|} \log_{2} \frac{\left|D_{i k}\right|}{\left|D_{i}\right|}</script></p><p>注：$C_{k}$表示属于某个类别的样本数</p><p>注：信息增益表示得知特征X的信息而信息的不确定性减少的程度使得类Y的信息熵减少的程度</p><p><strong>例子</strong></p><p><img src="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/机器学习图库/银行贷款数据.png" alt="银行贷款数据"></p><p>根据某人年龄、工作、房子和信贷情况，判断是否贷款？</p><p>在历史数据中（15次贷款）有6次没贷款，9次贷款，所以此时的熵应为：</p><script type="math/tex;mode=display">H(D)=-(\frac{6}{15}*log_{2}\frac{6}{15}+\frac{9}{15}*log_{2}\frac{9}{15})=0.971</script><p>$H(青年)=-(\frac{2}{5}<em>log_{2}\frac{2}{5}+\frac{3}{5}</em>log_{2}\frac{3}{5})=0.971$</p><p>$H(中年)=-(\frac{2}{5}<em>log_{2}\frac{2}{5}+\frac{3}{5}</em>log_{2}\frac{3}{5})=0.971$</p><p>$H(老年)=-(\frac{1}{5}<em>log_{2}\frac{1}{5}+\frac{4}{5}</em>log_{2}\frac{4}{5})=0.722$</p><p>$H(D|年龄)=\frac{5}{15}H(青年)+\frac{5}{15}H(中年)+\frac{5}{15}H(老年)=0.888$</p><p>$g(D,年龄)=H(D)-H(D|年龄)=0.083$ 注：别人计算为0.313</p><p>我们以A1、A2、A3、A4代表年龄、有工作、有自己的房子和贷款情况。最终计算的结果g(D, A1) = 0.313, g(D, A2) = 0.324, g(D, A3) = 0.420,g(D, A4) = 0.363。所以我们选择A3 作为划分的第一个特征。这样我们就可以一棵树慢慢建立</p><h4 id="决策树的三种算法实现"><a href="#决策树的三种算法实现" class="headerlink" title="决策树的三种算法实现"></a>决策树的三种算法实现</h4><p>当然决策树的原理不止信息增益这一种，还有其他方法。但是原理都类似，我们就不去举例计算。</p><ul><li><p>ID3</p><p>信息增益 最大的准则</p></li><li><p>C4.5</p><p>信息增益比 最大的准则</p></li><li><p>CART<br>分类树: 基尼系数 最小的准则 在sklearn中可以选择划分的默认原则<br>优势：划分更加细致（从后面例子的树显示来理解）</p></li></ul><h4 id="决策树算法API"><a href="#决策树算法API" class="headerlink" title="决策树算法API"></a>决策树算法API</h4><p>class sklearn.tree.DecisionTreeClassifier(criterion=’gini’, max_depth=None,random_state=None)<br>决策树分类器</p><ul><li><p>criterion:默认是’gini’系数，也可以选择信息增益的熵’entropy’</p></li><li><p>max_depth:树的深度大小</p></li><li><p>random_state:随机数种子</p></li><li>其中会有些超参数：max_depth:树的深度大小</li></ul><p>其它超参数我们会结合随机森林讲解</p><h4 id="案例1：鸢尾花种类预测"><a href="#案例1：鸢尾花种类预测" class="headerlink" title="案例1：鸢尾花种类预测"></a>案例1：鸢尾花种类预测</h4><p>代码：</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="false" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#决策树对鸢尾花进行分类</span></span><br><span class="line"><span class="comment">#1)导入库</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"><span class="comment">#2)获取数据</span></span><br><span class="line">x,y = datasets.load_iris(<span class="literal">True</span>)</span><br><span class="line"><span class="comment">#3）划分数据集</span></span><br><span class="line">x_train,x_test,y_train,y_test = train_test_split(x,y,test_size  = <span class="number">0.2</span>)</span><br><span class="line"><span class="comment">#决策树不需要特征工程：标准化</span></span><br><span class="line"><span class="comment">#4)决策树算法预估器(训练数据)</span></span><br><span class="line">estimator=DecisionTreeClassifier(criterion=<span class="string">'entropy'</span>)</span><br><span class="line">estimator.fit(x_train,y_train)</span><br><span class="line"><span class="comment">#5)模型评估</span></span><br><span class="line"><span class="comment">#方法1：直接比对真实值和预测值</span></span><br><span class="line">y_predict=estimator.predict(x_test)</span><br><span class="line">print(<span class="string">'y_predict:\n'</span>,y_predict)</span><br><span class="line">print(<span class="string">'直接比对真实值和预测值：\n'</span>,y_test  y_predict)</span><br><span class="line"><span class="comment">#方法2：计算准确率</span></span><br><span class="line">score=estimator.score(x_test,y_test)</span><br><span class="line">print(<span class="string">'准确率为：\n'</span>,score)</span><br></pre></td></tr></table></figure></div><p>输出结果：</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="false" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">y_predict:</span><br><span class="line"> [<span class="number">1</span> <span class="number">2</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">2</span> <span class="number">0</span> <span class="number">1</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">2</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">0</span> <span class="number">2</span> <span class="number">1</span> <span class="number">2</span>]</span><br><span class="line">直接比对真实值和预测值：</span><br><span class="line"> [ <span class="literal">True</span>  <span class="literal">True</span>  <span class="literal">True</span>  <span class="literal">True</span>  <span class="literal">True</span>  <span class="literal">True</span>  <span class="literal">True</span> <span class="literal">False</span>  <span class="literal">True</span>  <span class="literal">True</span> <span class="literal">False</span>  <span class="literal">True</span></span><br><span class="line">  <span class="literal">True</span>  <span class="literal">True</span>  <span class="literal">True</span>  <span class="literal">True</span>  <span class="literal">True</span>  <span class="literal">True</span>  <span class="literal">True</span>  <span class="literal">True</span> <span class="literal">False</span>  <span class="literal">True</span>  <span class="literal">True</span>  <span class="literal">True</span></span><br><span class="line">  <span class="literal">True</span>  <span class="literal">True</span>  <span class="literal">True</span>  <span class="literal">True</span>  <span class="literal">True</span>  <span class="literal">True</span>]</span><br><span class="line">准确率为：</span><br><span class="line"> <span class="number">0.9</span></span><br></pre></td></tr></table></figure></div><p>注：比对kNN算法准确率低；由于鸢尾花数据少，而kNN算法使用场景为小数据场景</p><h4 id="决策树可视化"><a href="#决策树可视化" class="headerlink" title="决策树可视化"></a>决策树可视化</h4><p>1、sklearn.tree.export_graphviz() 该函数能够导出DOT格式</p><p>tree.export_graphviz(estimator,out_file=’tree.dot’,feature_names=[‘’,’’])</p><p>2、工具:(能够将dot文件转换为pdf、png)</p><p>安装graphviz</p><p>ubuntu:sudo apt-get install graphviz Mac:brew install graphviz</p><p>3、运行命令</p><p>然后我们运行这个命令</p><p>dot -Tpng tree.dot -o tree.png</p><p>以上文鸢尾花数据为例：</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="false" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> export_graphviz</span><br><span class="line">export_graphviz(estimator,out_file=<span class="string">'iris_tree.dot'</span>,feature_names=iris.feature_names)</span><br></pre></td></tr></table></figure></div><p>导出文档iris_tree.dot，打开文档全选复制，粘贴到<a href="http://www.webgraphviz.com/网页版上可以实现可视化。无需安装软件。" target="_blank" rel="noopener external nofollow noreferrer">http://www.webgraphviz.com/网页版上可以实现可视化。无需安装软件。</a></p><h4 id="案例：泰坦尼克号乘客生存预测"><a href="#案例：泰坦尼克号乘客生存预测" class="headerlink" title="案例：泰坦尼克号乘客生存预测"></a>案例：泰坦尼克号乘客生存预测</h4><p>泰坦尼克号数据<br>在泰坦尼克号和titanic2数据帧描述泰坦尼克号上的个别乘客的生存状态。这里使用的数据集是由各种研究人员开始的。其中包括许多研究人员创建的旅客名单，由Michael A. Findlay编辑。我们提取的数据集中的特征是票的类别，存活，乘坐班，年龄，登陆，home.dest，房间，票，船和性别。</p><p>代码：</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="false" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#决策树进行乘客生存预测</span></span><br><span class="line"><span class="comment">#1、导入库</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction <span class="keyword">import</span> DictVectorizer</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier,export_graphviz</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> tree</span><br><span class="line"><span class="comment">#2、获取数据</span></span><br><span class="line">titan = pd.read_csv(<span class="string">"http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic.txt"</span>)</span><br><span class="line"><span class="comment">#3、数据的处理</span></span><br><span class="line">x = titan[[<span class="string">'pclass'</span>, <span class="string">'age'</span>, <span class="string">'sex'</span>]]</span><br><span class="line">y = titan[<span class="string">'survived'</span>]</span><br><span class="line"><span class="comment"># ①缺失值处理，将特征当中有类别的这些特征进行字典特征抽取</span></span><br><span class="line">x[<span class="string">'age'</span>].fillna(x[<span class="string">'age'</span>].mean(), inplace=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># ②对于x转换成字典数据</span></span><br><span class="line">x=x.to_dict(orient=<span class="string">"records"</span>) <span class="comment"># [&#123;"pclass": "1st", "age": 29.00, "sex": "female"&#125;, &#123;&#125;]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#4、划分数据集</span></span><br><span class="line">x_train,x_test,y_train,y_test = train_test_split(x,y,test_size  = <span class="number">0.2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#5、字典特征抽取</span></span><br><span class="line">transfer = DictVectorizer()</span><br><span class="line">x_train = transfer.fit_transform(x_train)</span><br><span class="line">x_test=transfer.transform(x_test) <span class="comment">#不需要调用fit_transform</span></span><br><span class="line">print(transfer.get_feature_names())</span><br><span class="line">print(<span class="string">'-----------------------------------------------------'</span>)</span><br><span class="line"><span class="comment">#print(x)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#决策树不需要特征工程：标准化</span></span><br><span class="line"><span class="comment">#6、决策树算法预估器(训练数据)</span></span><br><span class="line">estimator=DecisionTreeClassifier(criterion=<span class="string">'entropy'</span>)</span><br><span class="line">estimator.fit(x_train,y_train)</span><br><span class="line"><span class="comment">#7、模型评估</span></span><br><span class="line"><span class="comment">#方法1：直接比对真实值和预测值</span></span><br><span class="line">y_predict=estimator.predict(x_test)</span><br><span class="line">print(<span class="string">'y_predict:\n'</span>,y_predict)</span><br><span class="line">print(<span class="string">'-----------------------------------------------------'</span>)</span><br><span class="line">print(<span class="string">'直接比对真实值和预测值：\n'</span>,y_test  y_predict)</span><br><span class="line">print(<span class="string">'-----------------------------------------------------'</span>)</span><br><span class="line"><span class="comment">#方法2：计算准确率</span></span><br><span class="line">score=estimator.score(x_test,y_test)</span><br><span class="line">print(<span class="string">'准确率为：\n'</span>,score)</span><br><span class="line"><span class="comment">#8、可视化导出titan_tree.dot文档</span></span><br><span class="line">export_graphviz(estimator,out_file=<span class="string">'titan_tree.dot'</span>,feature_names=transfer.get_feature_names())</span><br></pre></td></tr></table></figure></div><p>输出结果：</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="false" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">'age'</span>, <span class="string">'pclass=1st'</span>, <span class="string">'pclass=2nd'</span>, <span class="string">'pclass=3rd'</span>, <span class="string">'sex=female'</span>, <span class="string">'sex=male'</span>]</span><br><span class="line">-----------------------------------------------------</span><br><span class="line">y_predict:</span><br><span class="line"> [<span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span></span><br><span class="line"> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span></span><br><span class="line"> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span></span><br><span class="line"> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span></span><br><span class="line"> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span></span><br><span class="line"> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span></span><br><span class="line"> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span></span><br><span class="line"> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span>]</span><br><span class="line">-----------------------------------------------------</span><br><span class="line">直接比对真实值和预测值：</span><br><span class="line"> <span class="number">199</span>      <span class="literal">True</span></span><br><span class="line"><span class="number">1308</span>     <span class="literal">True</span></span><br><span class="line"><span class="number">814</span>      <span class="literal">True</span></span><br><span class="line"><span class="number">393</span>     <span class="literal">False</span></span><br><span class="line"><span class="number">560</span>      <span class="literal">True</span></span><br><span class="line">        ...  </span><br><span class="line"><span class="number">955</span>      <span class="literal">True</span></span><br><span class="line"><span class="number">147</span>      <span class="literal">True</span></span><br><span class="line"><span class="number">30</span>       <span class="literal">True</span></span><br><span class="line"><span class="number">923</span>     <span class="literal">False</span></span><br><span class="line"><span class="number">404</span>      <span class="literal">True</span></span><br><span class="line">Name: survived, Length: <span class="number">263</span>, dtype: bool</span><br><span class="line">-----------------------------------------------------</span><br><span class="line">准确率为：</span><br><span class="line"> <span class="number">0.779467680608365</span></span><br></pre></td></tr></table></figure></div><h4 id="决策树总结"><a href="#决策树总结" class="headerlink" title="决策树总结"></a>决策树总结</h4><p>优点：可视化 - 可解释能力强<br>缺点：容易产生过拟合</p><p>改进：</p><ul><li>减枝cart算法(决策树API当中已经实现，随机森林参数调优有相关介绍)</li><li><strong>随机森林</strong></li></ul><h3 id="集成学习方法之随机森林"><a href="#集成学习方法之随机森林" class="headerlink" title="集成学习方法之随机森林"></a>集成学习方法之随机森林</h3><h4 id="什么是集成学习方法"><a href="#什么是集成学习方法" class="headerlink" title="什么是集成学习方法"></a>什么是集成学习方法</h4><p>集成学习通过建立几个模型组合的来解决单一预测问题。它的工作原理是<strong>生成多个分类器/模型</strong>，各自独立地学习和作出预测。<strong>这些预测最后结合成组合预测，因此优于任何一个单分类的做出预测。</strong></p><h4 id="什么是随机森林"><a href="#什么是随机森林" class="headerlink" title="什么是随机森林"></a>什么是随机森林</h4><p>在机器学习中，<strong>随机森林是一个包含多个决策树的分类器</strong>，并且其输出的类别是由个别树输出的类别的众数而定。</p><h4 id="随机森林原理过程"><a href="#随机森林原理过程" class="headerlink" title="随机森林原理过程"></a>随机森林原理过程</h4><p>训练集：特征值 +目标值 N个样本 M个特征</p><h6 id="两个随机"><a href="#两个随机" class="headerlink" title="两个随机"></a>两个随机</h6><p>训练集随机 - N个样本中随机有放回的抽样N个<br>bootstrap(随机有放回抽样)<br>[1, 2, 3, 4, 5]经过随机有放回抽样得到新的树的训练 [2, 2, 3, 1, 5]</p><p>特征随机 - 从M个特征中随机抽取m个特征<br>​ 要求：M &gt;&gt; m 效果：降维</p><h4 id="随机森林算法API"><a href="#随机森林算法API" class="headerlink" title="随机森林算法API"></a>随机森林算法API</h4><p>class sklearn.ensemble.RandomForestClassifier(n_estimators=10, criterion=’gini’, max_depth=None, bootstrap=True, random_state=None, min_samples_split=2)</p><p><strong>随机森林分类器</strong></p><ul><li><p>n_estimators：integer，optional（default = 10）森林里的树木数量120,200,300,500,800,1200</p></li><li><p>criterian：string，可选（default =“gini”）分割特征的测量方法</p></li><li><p>max_depth：integer或None，可选（默认=无）树的最大深度 5,8,15,25,30</p></li><li><p>max_features=”auto”,每个决策树的最大特征数量</p><p>​ If “auto”, then max_features=sqrt(n_features). 对M求平方根<br>​ If “sqrt”, then max_features=sqrt(n_features) (same as “auto”).<br>​ If “log2”, then max_features=log2(n_features).<br>​ If None, then max_features=n_features.</p></li><li><p>bootstrap：boolean，optional（default = True） 是否在构建树时使用放回抽样</p></li><li>min_samples_split:节点划分最少样本数</li><li>min_samples_leaf:叶子节点的最小样本数</li></ul><p><strong>超参数</strong>：n_estimator, max_depth, min_samples_split,min_samples_leaf</p><h4 id="案例：泰坦尼克号乘客生存预测-1"><a href="#案例：泰坦尼克号乘客生存预测-1" class="headerlink" title="案例：泰坦尼克号乘客生存预测"></a>案例：泰坦尼克号乘客生存预测</h4><p>代码：</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="false" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#复制上文中决策树代码比对决策树与随机森林效果</span></span><br><span class="line"><span class="comment">#随机森林进行乘客生存预测</span></span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line">estimator= RandomForestClassifier()</span><br><span class="line"><span class="comment"># grid网格，search搜索，cv：cross_validation</span></span><br><span class="line"><span class="comment"># 搜索算法最合适的参数</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"><span class="comment">#网格搜索GridSearchCV进行最佳参数的查找</span></span><br><span class="line">params =&#123;<span class="string">"n_estimators"</span>: [<span class="number">120</span>,<span class="number">200</span>,<span class="number">300</span>,<span class="number">500</span>,<span class="number">800</span>,<span class="number">1200</span>], <span class="string">"max_depth"</span>: [<span class="number">5</span>, <span class="number">8</span>, <span class="number">15</span>, <span class="number">25</span>, <span class="number">30</span>]&#125;</span><br><span class="line"><span class="comment"># cross_val_score类似</span></span><br><span class="line">estimator = GridSearchCV(estimator,param_grid=params,cv = <span class="number">6</span>)</span><br><span class="line">estimator.fit(x_train, y_train)</span><br><span class="line"><span class="comment">#模型评估</span></span><br><span class="line"><span class="comment">#方法1：直接比对真实值和预测值</span></span><br><span class="line">y_predict=estimator.predict(x_test)</span><br><span class="line">print(<span class="string">'---------------以下为随机森林预测效果-----------------'</span>)</span><br><span class="line">print(<span class="string">'y_predict:\n'</span>,y_predict)</span><br><span class="line">print(<span class="string">'随机森林预测的直接比对真实值和预测值：\n'</span>,y_test  y_predict)</span><br><span class="line">print(<span class="string">'-----------------------------------------------------'</span>)</span><br><span class="line"><span class="comment">#方法2：计算准确率</span></span><br><span class="line">score=estimator.score(x_test,y_test)</span><br><span class="line">print(<span class="string">'随机森林预测的准确率为：\n'</span>,score)</span><br><span class="line">print(<span class="string">'-----------------------------------------------------'</span>)</span><br><span class="line"><span class="comment">#查看了GridSearchCV最佳的参数组合</span></span><br><span class="line"><span class="comment">#最佳参数：best_params_</span></span><br><span class="line">print(<span class="string">'最佳参数：\n'</span>,estimator.best_params_)</span><br><span class="line"><span class="comment">#最佳结果：best_score_</span></span><br><span class="line">print(<span class="string">'最佳结果：\n'</span>,estimator.best_score_)</span><br><span class="line"><span class="comment">#最佳估计器：best_estimator_</span></span><br><span class="line">print(<span class="string">'最佳估计器：\n'</span>,estimator.best_estimator_)</span><br><span class="line"><span class="comment">#交叉验证结果：cv_results_</span></span><br><span class="line"><span class="comment">#print('交叉验证结果：\n',estimator.cv_results_)</span></span><br></pre></td></tr></table></figure></div><p>输出结果：注：随机森林输出结果等待时间长</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="false" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#决策树输出结果：</span></span><br><span class="line">[<span class="string">'age'</span>, <span class="string">'pclass=1st'</span>, <span class="string">'pclass=2nd'</span>, <span class="string">'pclass=3rd'</span>, <span class="string">'sex=female'</span>, <span class="string">'sex=male'</span>]</span><br><span class="line">-----------------------------------------------------</span><br><span class="line">y_predict:</span><br><span class="line"> [<span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span></span><br><span class="line"> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span></span><br><span class="line"> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span></span><br><span class="line"> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span></span><br><span class="line"> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span></span><br><span class="line"> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span></span><br><span class="line"> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span></span><br><span class="line"> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line">-----------------------------------------------------</span><br><span class="line">直接比对真实值和预测值：</span><br><span class="line"> <span class="number">379</span>     <span class="literal">False</span></span><br><span class="line"><span class="number">1099</span>     <span class="literal">True</span></span><br><span class="line"><span class="number">970</span>      <span class="literal">True</span></span><br><span class="line"><span class="number">482</span>      <span class="literal">True</span></span><br><span class="line"><span class="number">573</span>      <span class="literal">True</span></span><br><span class="line">        ...  </span><br><span class="line"><span class="number">80</span>       <span class="literal">True</span></span><br><span class="line"><span class="number">295</span>      <span class="literal">True</span></span><br><span class="line"><span class="number">4</span>        <span class="literal">True</span></span><br><span class="line"><span class="number">1173</span>     <span class="literal">True</span></span><br><span class="line"><span class="number">684</span>      <span class="literal">True</span></span><br><span class="line">Name: survived, Length: <span class="number">263</span>, dtype: bool</span><br><span class="line">-----------------------------------------------------</span><br><span class="line">准确率为：</span><br><span class="line"> <span class="number">0.7490494296577946</span></span><br><span class="line">    </span><br><span class="line"><span class="comment">#随机森林输出结果：</span></span><br><span class="line">---------------以下为随机森林预测效果-----------------</span><br><span class="line">y_predict:</span><br><span class="line"> [<span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span></span><br><span class="line"> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span></span><br><span class="line"> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span></span><br><span class="line"> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span></span><br><span class="line"> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span></span><br><span class="line"> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span></span><br><span class="line"> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span></span><br><span class="line"> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line">随机森林预测的直接比对真实值和预测值：</span><br><span class="line"> <span class="number">379</span>     <span class="literal">False</span></span><br><span class="line"><span class="number">1099</span>     <span class="literal">True</span></span><br><span class="line"><span class="number">970</span>      <span class="literal">True</span></span><br><span class="line"><span class="number">482</span>      <span class="literal">True</span></span><br><span class="line"><span class="number">573</span>      <span class="literal">True</span></span><br><span class="line">        ...  </span><br><span class="line"><span class="number">80</span>       <span class="literal">True</span></span><br><span class="line"><span class="number">295</span>      <span class="literal">True</span></span><br><span class="line"><span class="number">4</span>        <span class="literal">True</span></span><br><span class="line"><span class="number">1173</span>     <span class="literal">True</span></span><br><span class="line"><span class="number">684</span>      <span class="literal">True</span></span><br><span class="line">Name: survived, Length: <span class="number">263</span>, dtype: bool</span><br><span class="line">-----------------------------------------------------</span><br><span class="line">随机森林预测的准确率为：</span><br><span class="line"> <span class="number">0.7908745247148289</span></span><br><span class="line">-----------------------------------------------------</span><br><span class="line">最佳参数：</span><br><span class="line"> &#123;<span class="string">'max_depth'</span>: <span class="number">5</span>, <span class="string">'n_estimators'</span>: <span class="number">300</span>&#125;</span><br><span class="line">最佳结果：</span><br><span class="line"> <span class="number">0.8295238095238096</span></span><br><span class="line">最佳估计器：</span><br><span class="line"> RandomForestClassifier(bootstrap=<span class="literal">True</span>, class_weight=<span class="literal">None</span>, criterion=<span class="string">'gini'</span>,</span><br><span class="line">                       max_depth=<span class="number">5</span>, max_features=<span class="string">'auto'</span>, max_leaf_nodes=<span class="literal">None</span>,</span><br><span class="line">                       min_impurity_decrease=<span class="number">0.0</span>, min_impurity_split=<span class="literal">None</span>,</span><br><span class="line">                       min_samples_leaf=<span class="number">1</span>, min_samples_split=<span class="number">2</span>,</span><br><span class="line">                       min_weight_fraction_leaf=<span class="number">0.0</span>, n_estimators=<span class="number">300</span>,</span><br><span class="line">                       n_jobs=<span class="literal">None</span>, oob_score=<span class="literal">False</span>, random_state=<span class="literal">None</span>,</span><br><span class="line">                       verbose=<span class="number">0</span>, warm_start=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure></div><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><ul><li>在当前所有算法中，具有极好的准确率</li><li>能够有效地运行在大数据集上，处理具有高维特征的输入样本，而且不需要降维</li><li>能够评估各个特征在分类问题上的重要性</li></ul><h2 id="回归算法"><a href="#回归算法" class="headerlink" title="回归算法"></a>回归算法</h2><p>回归问题：目标值 - 连续型的数据</p><h3 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h3><h4 id="什么是线性回归"><a href="#什么是线性回归" class="headerlink" title="什么是线性回归"></a>什么是线性回归</h4><p>线性回归(Linear regression)是利用<strong>回归方程(函数)</strong>对一个或<strong>多个自变量(<span style="color:red">特征值</span>)和因变量(<span style="color:red">目标值</span>)之间</strong>关系进行建模的一种分析方式。</p><p>特点：只有一个自变量的情况称为单变量回归（如：$y=kx+b$），大于一个自变量情况的叫做多元回归。</p><p>通用公式：$h_w(x)=w_1x_1+w_2x_2+w_3x_3…+b=w^Tx+b$</p><p>其中$w$权重，$b$偏置</p><p>$w$和$x$可以理解为矩阵：$\mathbf{w}=\left(\begin{array}{c}b \ w_{1} \ w_{2}\end{array}\right)$,$\mathbf{x}=\left(\begin{array}{c}1 \ x_{1} \ x_{2}\end{array}\right)$</p><p>线性回归当中的关系有两种，一种是线性关系，另一种是非线性关系。</p><p><strong>例子：</strong></p><h6 id="线性关系"><a href="#线性关系" class="headerlink" title="线性关系"></a>线性关系</h6><p>注：如果在单特征与目标值的关系呈直线关系，或者两个特征与目标值呈现平面的关系</p><p>数据：工资和年龄（2个特征）</p><p>目标：预测银行会贷款给我多少钱（标签）</p><p>考虑：工资和年龄都会影响最终银行贷款的结果那么它们各自有多大的影响呢？（参数）</p><p><img src="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/机器学习图库/捕获.JPG" alt="捕获"></p><p>$X1,X2$就是我们的两个特征（年龄，工资）$ Y$是银行最终会借给我们多少钱</p><p>找到最合适的一条线（想象一个高维）来最好的拟合我们的数据点</p><p><img src="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/机器学习图库/image-20200302161335608.png" alt="image-20200302161335608"></p><p>假设$w_1$是年龄的参数，$w_2$是工资的参数</p><p>拟合的平面：$h_w(x)=w_1x_1+w_2x_2$</p><p>整合：$h_w(x)=w^Tx$</p><h6 id="非线性关系"><a href="#非线性关系" class="headerlink" title="非线性关系"></a>非线性关系</h6><p><img src="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/机器学习图库/非线性关系.png" alt="非线性关系"></p><p>如果是非线性关系，那么回归方程可以理解为：$w_1x_1+w_2x_2^2+w_3x_3^2$</p><h4 id="线性回归原理"><a href="#线性回归原理" class="headerlink" title="线性回归原理"></a>线性回归原理</h4><h6 id="1、误差"><a href="#1、误差" class="headerlink" title="1、误差"></a><strong>1、误差</strong></h6><p>真实值和预测值之间肯定是要存在差异的（用$ε$来表示该误差）</p><p>对于每个样本：$<br>y^{(i)}=w^{T} x^{(i)}+\varepsilon^{(i)}<br>$</p><p>误差$\varepsilon^{(i)}$是独立同分布，并且服从均值为0方差为$θ^2$的高斯分布(正态分布)</p><blockquote><p><strong>独立</strong>：张三和李四一起来贷款，他俩没关系</p><p><strong>同分布</strong>：他俩都来得是我们假定的这家银行</p><p><strong>高斯分布</strong>：银行可能会多给，也可能会少给，但是绝大多数情况下这个浮动不会太大，极小情况下浮动会比较大，符合正常情况</p></blockquote><p>预测值与误差：$y^{(i)}=w^{T} x^{(i)}+\varepsilon^{(i)}$ $(1)$</p><p>由于误差服从高斯分布：$p(\varepsilon^{(i)})=\frac{1}{\sqrt{2 \pi} \sigma} \exp \left(-\frac{(\varepsilon^{(i)})^{2}}{2 \sigma^{2}}\right)$ $(2)$</p><p>将$(1)$式代入$(2)$式：$<br>p(y^{(i)}|x^{(i)};w)=\frac{1}{\sqrt{2 \pi} \sigma} \exp \left(-\frac{(y^{(i)}-w^Tx^{(i)})^{2}}{2 \sigma^{2}}\right)<br>$</p><p>似然函数：$L(w)=\prod_{i=1}^{m} p\left(y^{(i)} | x^{(i)} ; w\right)=\prod_{i=1}^{m} \frac{1}{\sqrt{2 \pi} \sigma} \exp \left(-\frac{\left(y^{(i)}-w^{T} x^{(i)}\right)^{2}}{2 \sigma^{2}}\right)$</p><p>解释：什么样的参数跟我们的数据组合后恰好是真实值</p><p>对数似然：$logL(w)=log\prod_{i=1}^{m} \frac{1}{\sqrt{2 \pi} \sigma} \exp \left(-\frac{\left(y^{(i)}-w^{T} x^{(i)}\right)^{2}}{2 \sigma^{2}}\right)$</p><p>解释：乘法难解，加法就容易了，对数里面乘法可以转换成加法</p><p>展开化简：$\sum_{i=1}^{m} \log \frac{1}{\sqrt{2 \pi} \sigma} \exp \left(-\frac{\left(y^{(i)}-w^{T} x^{(i)}\right)^{2}}{2 \sigma^{2}}\right)$<br>$=m \log \frac{1}{\sqrt{2 \pi} \sigma}-\frac{1}{\sigma^{2}} \cdot \frac{1}{2} \sum_{i=1}^{m}\left(y^{(i)}-w^{T} x^{(i)}\right)^{2}$</p><p>目标：让似然函数（对数变换后也一样）越大越好</p><h6 id="2、损失函数（Loss-Function）"><a href="#2、损失函数（Loss-Function）" class="headerlink" title="2、损失函数（Loss Function）"></a><strong><span style="color:red">2、损失函数</span>（Loss Function）</strong></h6><p>$J(w)=\frac{1}{2} \sum_{i=1}^{m}\left(y^{(i)}-w^{T} x^{(i)}\right)^{2}=\frac{1}{2} \sum_{i=1}^{m}\left(h_w(x^{(i)})-y^{(i)}\right)^{2}$</p><p>这里的这个损失函数就是著名的<strong>最小二乘损失函数</strong></p><h6 id="3、优化算法"><a href="#3、优化算法" class="headerlink" title="3、优化算法"></a>3、优化算法</h6><p><strong>如何去求模型当中的W，使得损失最小？（目的是找到最小损失对应的W值）</strong></p><p>线性回归经常使用的两种优化算法:</p><p>1)正规方程——天才（直接求解$w$）</p><p>目标函数：$J(w)=\frac{1}{2} \sum_{i=1}^{m}\left(h_w(x^{(i)})-y^{(i)}\right)^{2}=\frac{1}{2}\left(Xw-y\right)^T\left(Xw-y\right)$</p><p>求偏导：</p><script type="math/tex;mode=display">\begin{aligned}
&\nabla_{w} J(w)=\nabla_{w}\left(\frac{1}{2}(X w-y)^{r}(X w-y)\right)=\nabla_{w}\left(\frac{1}{2}\left(w^{T} X^{T}-y^{T}\right)(X w-y)\right)\\
&=\nabla_{w}\left(\frac{1}{2}\left(w^{T} X^{T} X w-w^{T} X^{T} y-y^{T} X w+y^{T} y\right)\right)\\
&=\frac{1}{2}\left(2 X^{T} X w-X^{T} y-\left(y^{T} X\right)^{\bar{T}}\right)=X^{T} X w-X^{T} y
\end{aligned}</script><p>令偏导等于0得：$w=\left(X^TX\right)^{-1}X^Ty$</p><p><img src="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/机器学习图库/损失行数求解1.png" alt="损失行数求解1"></p><p>理解：X为特征值矩阵，y为目标值矩阵。直接求到最好的结果</p><p>缺点：当特征过多过复杂时，求解速度太慢并且得不到结果</p><p>2）<strong>梯度下降(Gradient Descent)</strong>——勤奋努力的普通人（一步一步的求$w$）</p><p><img src="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/机器学习图库/梯度下降公式.png" alt="梯度下降公式"></p><p>理解：α为学习速率，需要手动指定（超参数），α旁边的整体表示方向</p><p>沿着这个函数下降的方向找，最后就能找到山谷的最低点，然后更新W值</p><p>使用：面对训练数据规模十分庞大的任务 ，能够找到较好的结果</p><p><strong>我们通过两个图更好理解梯度下降的过程:</strong></p><p><img src="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/机器学习图库/单变量的梯度下降.png" alt="单变量的梯度下降"></p><p><img src="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/机器学习图库/多变量的梯度下降.png" alt="多变量的梯度下降"></p><h6 id="4、优化动态图演示"><a href="#4、优化动态图演示" class="headerlink" title="4、优化动态图演示"></a>4、优化动态图演示</h6><p><img src="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/机器学习图库/线性回归优化动态图.gif" alt="线性回归优化动态图"></p><h4 id="线性回归算法API"><a href="#线性回归算法API" class="headerlink" title="线性回归算法API"></a>线性回归算法API</h4><p><strong>方法一</strong></p><p>sklearn.linear_model.LinearRegression(fit_intercept=True)</p><ul><li><p>通过正规方程优化</p></li><li><p>fit_intercept：是否计算偏置</p></li><li><p>LinearRegression.coef_：回归系数_</p></li><li><p>LinearRegression.intercept_：偏置_</p></li></ul><p><strong>方法二</strong></p><p>sklearn.linear_model.SGDRegressor(loss=”squared_loss”, fit_intercept=True, learning_rate =’invscaling’, eta0=0.01)</p><ul><li>SGDRegressor类实现了随机梯度下降学习，它支持不同的loss函数和正则化惩罚项来拟合线性回归模型。</li><li>loss:损失类型 loss=”squared_loss”: 普通最小二乘法</li><li>fit_intercept：是否计算偏置</li><li>learning_rate : string, optional</li></ul><p>​ 学习率填充</p><p>​ ‘constant’: eta = eta0</p><p>​ ‘optimal’: eta = 1.0 / (alpha * (t + t0)) [default]</p><p>​ ‘invscaling’: eta = eta0 / pow(t, power_t) power_t=0.25:存在父类当中</p><p>对于一个常数值的学习率来说，可以使用learning_rate=’constant’ ，并使用eta0来指定学习率。</p><ul><li><p>SGDRegressor.coef_：回归系数_</p></li><li><p>_SGDRegressor.intercept_：偏置</p></li></ul><blockquote><p>注：sklearn提供给我们两种实现的API， 可以根据选择使用</p></blockquote><h4 id="波士顿房价预测"><a href="#波士顿房价预测" class="headerlink" title="波士顿房价预测"></a>波士顿房价预测</h4><p>代码：（补充完善<a href="https://blog.csdn.net/weixin_41890393/article/details/83589860）" target="_blank" rel="noopener external nofollow noreferrer">https://blog.csdn.net/weixin_41890393/article/details/83589860）</a></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="false" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#正规方程的优化方法对波士顿房价进行预测</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="comment">#1)获取数据</span></span><br><span class="line">x,y= datasets.load_boston(<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#平滑处理预测值y</span></span><br><span class="line"><span class="comment">#平滑处理y值，x不处理。（x代表特征，y代表预测值）</span></span><br><span class="line"><span class="comment">#y=np.log(y)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#2）划分数据集</span></span><br><span class="line">x_train,x_test,y_train,y_test = train_test_split(x,y,random_state=<span class="number">22</span>)</span><br><span class="line"><span class="comment">#3)特征工程：标准化 不适合用标准化数据 一般用平滑处理</span></span><br><span class="line"><span class="comment">#transfer=StandardScaler()</span></span><br><span class="line"><span class="comment">#x_train=transfer.fit_transform(x_train)</span></span><br><span class="line"><span class="comment">#x_test=transfer.transform(x_test)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#4)正规方程的优化算法预估器</span></span><br><span class="line">estimator=LinearRegression()</span><br><span class="line">estimator.fit(x_train,y_train)</span><br><span class="line"><span class="comment">#5)得出模型</span></span><br><span class="line">print(<span class="string">'正规方程权重系数为:\n'</span>,estimator.coef_)</span><br><span class="line">print(<span class="string">'正规方程偏置为:\n'</span>,estimator.intercept_)</span><br><span class="line"><span class="comment">#6）模型评估</span></span><br><span class="line">y_predict=estimator.predict(x_test)</span><br><span class="line">print(<span class="string">'预测房价:\n'</span>,y_predict)</span><br><span class="line">print(<span class="string">'正规方程—均方误差MSE为:'</span>, metrics.mean_squared_error(y_test, y_predict))</span><br><span class="line"><span class="comment">#7）交叉验证</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_predict</span><br><span class="line">predicted = cross_val_predict(estimator,x,y,cv=<span class="number">10</span>)</span><br><span class="line">print(<span class="string">"MSE:"</span>, metrics.mean_squared_error(y, predicted))</span><br><span class="line"><span class="comment">#6）可视化</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="comment"># scatter</span></span><br><span class="line">plt.scatter(y, predicted)</span><br><span class="line"><span class="comment"># plot</span></span><br><span class="line">plt.plot([y.min(), y.max()], [y.min(), y.max()], <span class="string">'k--'</span>, lw=<span class="number">4</span>)</span><br><span class="line">plt.xlabel(<span class="string">"Measured"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"Predicted"</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment">#展示结果</span></span><br><span class="line"><span class="comment">#创建画布</span></span><br><span class="line">plt.figure(figsize=(<span class="number">20</span>,<span class="number">8</span>),dpi=<span class="number">80</span>)</span><br><span class="line"><span class="comment">#绘图</span></span><br><span class="line">plt.rcParams[<span class="string">'font.sans-serif'</span>]=<span class="string">'SimHei'</span></span><br><span class="line">plt.rcParams[<span class="string">'axes.unicode_minus'</span>]=<span class="literal">False</span></span><br><span class="line">x=range(len(y_predict))</span><br><span class="line">y1=y_predict</span><br><span class="line">y2=y_test</span><br><span class="line"><span class="comment">#折线图</span></span><br><span class="line">plt.plot(x,y1,linestyle=<span class="string">'-'</span>)</span><br><span class="line">plt.plot(x,y2,linestyle=<span class="string">'-.'</span>)</span><br><span class="line"><span class="comment">#增加图例</span></span><br><span class="line">plt.legend([<span class="string">'房价预测值'</span>,<span class="string">'房价真实值'</span>])</span><br><span class="line">plt.title(<span class="string">'波士顿房价走势图'</span>)</span><br><span class="line"><span class="comment">#展示</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></div><p>输出结果：（注：特征有几个权重就有几个）</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="false" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">正规方程权重系数为:</span><br><span class="line"> [<span class="number">-0.64817766</span>  <span class="number">1.14673408</span> <span class="number">-0.05949444</span>  <span class="number">0.74216553</span> <span class="number">-1.95515269</span>  <span class="number">2.70902585</span></span><br><span class="line"> <span class="number">-0.07737374</span> <span class="number">-3.29889391</span>  <span class="number">2.50267196</span> <span class="number">-1.85679269</span> <span class="number">-1.75044624</span>  <span class="number">0.87341624</span></span><br><span class="line"> <span class="number">-3.91336869</span>]</span><br><span class="line">正规方程偏置为:</span><br><span class="line"> <span class="number">22.62137203166228</span></span><br><span class="line">预测房价:</span><br><span class="line"> [<span class="number">28.22944896</span> <span class="number">31.5122308</span>  <span class="number">21.11612841</span> <span class="number">32.6663189</span>  <span class="number">20.0023467</span>  <span class="number">19.07315705</span></span><br><span class="line">...........................................................</span><br><span class="line"> <span class="number">28.58237108</span>]</span><br><span class="line">正规方程—均方误差MSE为: <span class="number">20.6275137630954</span></span><br><span class="line">MSE: <span class="number">34.53965953999329</span></span><br></pre></td></tr></table></figure></div><p><img src="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/机器学习图库/Figure_1-1583217802407.png" alt="Figure_1"></p><p><img src="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/机器学习图库/Figure_19.png" alt="Figure_19"></p><p>代码：</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="false" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#梯度下降的优化方法对波士顿房价进行预测</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> SGDRegressor</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="comment">#1)获取数据</span></span><br><span class="line">x,y= datasets.load_boston(<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#2）划分数据集</span></span><br><span class="line">x_train,x_test,y_train,y_test = train_test_split(x,y,random_state=<span class="number">22</span>)</span><br><span class="line"><span class="comment">#3)特征工程：标准化 </span></span><br><span class="line">transfer=StandardScaler()</span><br><span class="line">x_train=transfer.fit_transform(x_train)</span><br><span class="line">x_test=transfer.transform(x_test)</span><br><span class="line"></span><br><span class="line"><span class="comment">#4)梯度下降的优化算法预估器</span></span><br><span class="line">estimator=SGDRegressor(learning_rate=<span class="string">'constant'</span>, eta0=<span class="number">0.01</span>,max_iter=<span class="number">10000</span>)<span class="comment">#调参数</span></span><br><span class="line">estimator.fit(x_train,y_train)</span><br><span class="line"><span class="comment">#5)得出模型</span></span><br><span class="line">print(<span class="string">'梯度下降权重系数为:\n'</span>,estimator.coef_)</span><br><span class="line">print(<span class="string">'梯度下降截距为:\n'</span>,estimator.intercept_)</span><br><span class="line"><span class="comment">#6）模型评估</span></span><br><span class="line">y_predict=estimator.predict(x_test)</span><br><span class="line">print(<span class="string">'预测房价:\n'</span>,y_predict)</span><br><span class="line">print(<span class="string">'梯度下降—均方误差MSE为:'</span>, metrics.mean_squared_error(y_test, y_predict))</span><br></pre></td></tr></table></figure></div><p>输出结果：</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="false" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">梯度下降权重系数为:</span><br><span class="line"> [<span class="number">-0.529811</span>    <span class="number">0.93849958</span> <span class="number">-0.43482307</span>  <span class="number">0.77305338</span> <span class="number">-1.71206925</span>  <span class="number">2.82393382</span></span><br><span class="line"> <span class="number">-0.16256326</span> <span class="number">-3.09703859</span>  <span class="number">1.63812767</span> <span class="number">-0.93714655</span> <span class="number">-1.72483573</span>  <span class="number">0.88640923</span></span><br><span class="line"> <span class="number">-3.90806571</span>]</span><br><span class="line">梯度下降截距为:</span><br><span class="line"> [<span class="number">22.61725005</span>]</span><br><span class="line">预测房价:</span><br><span class="line"> [<span class="number">28.29992805</span> <span class="number">31.66102185</span> <span class="number">21.46408381</span> <span class="number">32.63060514</span> <span class="number">20.23508805</span> <span class="number">18.98298548</span></span><br><span class="line">...........................................................</span><br><span class="line"> <span class="number">28.33964857</span>]</span><br><span class="line">梯度下降—均方误差MSE为: <span class="number">21.004247881383602</span></span><br></pre></td></tr></table></figure></div><h4 id="正规方程和梯度下降对比"><a href="#正规方程和梯度下降对比" class="headerlink" title="正规方程和梯度下降对比"></a>正规方程和梯度下降对比</h4><p><img src="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/机器学习图库/正规方程和梯度下降对比.png" alt="正规方程和梯度下降对比"></p><p><strong>文字对比</strong></p><div class="table-container"><table><thead><tr><th style="text-align:center">梯度下降</th><th style="text-align:center"><strong>正规方程</strong></th></tr></thead><tbody><tr><td style="text-align:center">需要选择学习率</td><td style="text-align:center">不需要</td></tr><tr><td style="text-align:center">需要迭代求解</td><td style="text-align:center">一次运算得出</td></tr><tr><td style="text-align:center">特征数量较大可以使用</td><td style="text-align:center">需要计算方程，时间复杂度高O(n3)</td></tr></tbody></table></div><p>选择：</p><ul><li>小规模数据：<ul><li><strong>LinearRegression(不能解决拟合问题)</strong></li><li>岭回归</li></ul></li><li>大规模数据：SGDRegressor</li></ul><h4 id="拓展-关于优化方法BGD、SGD、MBGD、SAG"><a href="#拓展-关于优化方法BGD、SGD、MBGD、SAG" class="headerlink" title="拓展-关于优化方法BGD、SGD、MBGD、SAG"></a>拓展-关于优化方法BGD、SGD、MBGD、SAG</h4><p>目标函数：$J(w)=\frac{1}{2m} \sum_{i=1}^{m}\left(y^{(i)}-h_w(x^{(i)})\right)^{2}$</p><p>① 批量梯度下降<strong>（batch gradient descent）</strong>：（容易得到最优解，但是由于每次考虑所有样本，速度很慢）</p><p>$\frac{\partial J(w)}{\partial w_{j}}=-\frac{1}{m} \sum_{i=1}^{m}\left(y^{(i)}-h_{w}\left(x^{(i)}\right)\right) x_{j}^{(i)}$</p><p>$w_{j}^{\prime}=w_{j}+\frac{α}{m} \sum_{i=1}^{m}\left(y^{(i)}-h_{w}\left(x^{(i)}\right)\right) x_{j}^{(i)}$</p><p>批量梯度下降的算法执行过程如下图：</p><p><img src="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/机器学习图库/031938497051596.png" alt="031938497051596"></p><p>② <span style="color:red">随机梯度下降</span><strong>（Stochastic Gradient Descent, SGD）</strong>：（每次找一个样本，迭代速度快，但不一定每次都朝着收敛的方向，而是震荡的方式趋向极小点）</p><p>$w_{j}^{\prime}=w_{j}+α\left(y^{(i)}-h_{w}\left(x^{(i)}\right)\right) x_{j}^{(i)}$</p><ul><li>SGD的优点是：<ul><li>高效</li><li>容易实现</li></ul></li><li>SGD的缺点是：<ul><li>SGD需要许多超参数：比如正则项参数、迭代数。</li><li>SGD对于特征标准化是敏感的。</li></ul></li></ul><p>③ 小批量梯度下降法<strong>（Mini-batch gradient descent）</strong>：（每次更新选择一小部分数据来算，实用！</p><p>$w_{j}^{\prime}=w_{j}+α\frac{1}{10} \sum_{k=i}^{i+9}\left(y^{(k)}-h_{w}\left(x^{(k)}\right)\right) x_{j}^{(k)}$</p><p>④随机平均梯度法(Stochasitc Average Gradient)：（由于收敛的速度太慢，有人提出SAG等基于梯度下降的算法）</p><blockquote><p>Scikit-learn：SGDRegressor、岭回归、逻辑回归等当中都会有SAG优化</p></blockquote><h3 id="欠拟合与过拟合"><a href="#欠拟合与过拟合" class="headerlink" title="欠拟合与过拟合"></a>欠拟合与过拟合</h3><p><img src="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/机器学习图库/20190720210756791.png" alt="在这里插入图片描述"></p><ul><li>欠拟合（模型过于简单）</li></ul><p><img src="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/机器学习图库/20190720205912608.png" alt="在这里插入图片描述"></p><ul><li>过拟合（模型过于复杂）</li></ul><p><img src="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/机器学习图库/20190720210220248.png" alt="在这里插入图片描述"></p><p><strong>解决办法</strong></p><p>欠拟合解决办法</p><ul><li><p>增加新的特征，可以考虑加入进特征组合、高次特征，来增大假设空间</p></li><li><p>采用非线性模型，比如核SVM 、决策树、DNN等模型</p></li><li><p>Boosting，Boosting 往往会有较小的 Bias，比如 Gradient Boosting 等</p></li><li><p>如果已正则化，尝试减少正则化程度$λ$</p></li></ul><p>过拟合解决办法</p><ul><li><p>交叉检验，通过交叉检验得到较优的模型参数</p></li><li><p>特征选择，减少特征数或使用较少的特征组合，对于按区间离散化的特征，增大划分的区间</p></li><li><p>正则化，常用的有 L1、L2 正则</p></li><li><p>如果已正则化，尝试增大正则化程度λ</p></li><li><p>增加训练数据可以有限的避免过拟合</p></li><li><p>Bagging，将多个弱学习器Bagging 一下效果会好很多，比如随机森林等</p><p><img src="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/机器学习图库/20190720212516689.png" alt="在这里插入图片描述"></p><p><strong>正则化类别</strong></p></li></ul><ol><li>L1正则化<br>作用：可以使其中一些W的值直接为0，删除这个特征的影响。<br>LOSSO回归</li><li>L2正则化<br>作用：可以使得其中一些W的值都很小，都接近于0，削弱某个特征的影响。<br>优点：越小的参数说明模型越简单，越简单的模型则越不容易产生过拟合现象。<br>Ridge回归（岭回归）。<br>加入L2正则化后的损失函数：</li></ol><p>$J(w)=\frac{1}{2m} \sum_{i=1}^{m}\left(y^{(i)}-h_w(x^{(i)})\right)^{2}+λ\sum_{j=1}^{n}w_j^2$</p><p>其中m为样本数，n为特征数</p><h3 id="岭回归"><a href="#岭回归" class="headerlink" title="岭回归"></a>岭回归</h3><p>岭回归，其实也是一种线性回归。只不过在算法建立回归方程时候，加上正则化（L2）的限制，从而达到解决过拟合的效果。</p><h4 id="参数推导"><a href="#参数推导" class="headerlink" title="参数推导"></a>参数推导</h4><p>线性回归模型的目标函数:$J(β)=\sum\left(y-Xβ\right)^{2}$</p><p>为了保证回归系数$β$可求，岭回归模型在目标函数上加了一个L2范数的惩罚项</p><p>$J(β)=\sum\left(y-Xβ\right)^{2}+λ||β||_2^2=\sum\left(y-Xβ\right)^{2}+\sumλβ^2$</p><p>其中$λ$为非负数，$λ$越大，则为了使$J(β)$最小，回归系数$β$就越小。<br><strong>推导过程：</strong></p><p>$\begin{array}{c}<br>J(\beta)=(y-X \beta)^{T}(y-X \beta)+\lambda \beta^{T} \beta \\<br>=y^{T} y-y^{T} X \beta-\beta^{T} X^{T} y+\beta^{T} X^{T} X \beta+\lambda \beta^{T} \beta \\<br>令 \frac{\partial J(\beta)}{\partial \beta}=0 \\<br>\Rightarrow 0-X^{T} y-X^{T} y+2 X^{T} X \beta+2 \lambda \beta=0 \\<br>\Rightarrow \beta=\left(X^{T} X+\lambda I\right)^{-1} X^{T} y<br>\end{array}$</p><p>L2范数惩罚项的加入使得$(X^{T}X+λI)$满秩，保证了可逆，但是也由于惩罚项的加$λ$，使得回归系数$β$的估计不再是无偏估计。所以岭回归是以放弃无偏性、降低精度为代价解决病态矩阵问题的回归方法。<br>单位矩阵$I$的对角线上全是1，像一条山岭一样，这也是岭回归名称的由来。</p><h4 id="λ-的选择"><a href="#λ-的选择" class="headerlink" title="$λ$的选择"></a>$λ$的选择</h4><p>模型的方差：回归系数的方差<br>模型的偏差：预测值和真实值的差异<br>随着模型复杂度的提升，在训练集上的效果就越好，即模型的偏差就越小；但是同时模型的方差就越大。对于岭回归的$λ$而言，随着$λ$的增大，$(X^{T}X+λI)$就越大，$(X^{T}X+λI)^{-1}$就越小，模型的方差就越小；而$λ$越大使得$β$的估计值更加偏离真实值，模型的偏差就越大。所以岭回归的关键是找到一个合理的$λ$值来平衡模型的方差和偏差。<br>根据凸优化，可以将岭回归模型的目标函数$J(β)$最小化问题等价于$\left\{\begin{array}{l}\operatorname{argmin}\left\{\Sigma\left(y-X \beta^{2}\right)\right\} \ \Sigma \beta^{2} \leq t\end{array}\right.$</p><p>其中$t$为一个常数。以最简单的二维为例，即$β(β_1,β_2)$其几何图形是:</p><p><img src="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/机器学习图库/20181103202221367.jpg" alt="20181103202221367"></p><p><img src="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/机器学习图库/20181103202237148.jpg" alt="20181103202237148"></p><p>抛物面代表的是$\sum(y-X\beta)^2$的部分，圆柱体代表的是$β_1^{1}+β_2^{2}≤t$的部分。最小二乘解是抛物面的中心，岭回归解是抛物面与圆柱体的交点。岭回归的惩罚项$∑λβ^2$是关于回归系数$β$的二次函数，对目标函数求偏导时会保留$β$，抛物面与圆柱体很难相交于轴上使某个变量的回归系数为0，因此岭回归不能实现变量的剔除。</p><p><strong>（1）岭迹法确定$λ$值</strong><br>由$β=(X^TX+λI)^{−1}X^Ty$ 可知$β$是$λ$的函数，当$λ∈[0,∞)$时，在平面直角坐标系中的$β−λ$曲线称为岭迹曲线。当$β$趋于稳定的点就是所要寻找的$λ$值。<br>代码：</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="false" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Ridge</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">data=pd.read_csv(<span class="string">'diabetes.csv'</span>)</span><br><span class="line">x=data.iloc[:,<span class="number">1</span>:<span class="number">-1</span>]</span><br><span class="line">y=data[<span class="string">'Outcome'</span>]</span><br><span class="line"><span class="comment">#拆分为训练集和测试集</span></span><br><span class="line">x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=<span class="number">0.2</span>,random_state=<span class="number">1234</span>)</span><br><span class="line"><span class="comment">#构造不同的lambda值</span></span><br><span class="line">Lambdas=np.logspace(<span class="number">-5</span>,<span class="number">2</span>,<span class="number">200</span>)</span><br><span class="line"><span class="comment">#存放偏回归系数</span></span><br><span class="line">ridge_cofficients=[]</span><br><span class="line"><span class="keyword">for</span> Lambda <span class="keyword">in</span> Lambdas:</span><br><span class="line">    ridge=Ridge(alpha=Lambda,normalize=<span class="literal">True</span>)</span><br><span class="line">    ridge.fit(x_train,y_train)</span><br><span class="line">    ridge_cofficients.append(ridge.coef_)</span><br><span class="line"></span><br><span class="line"><span class="comment">#绘制岭迹曲线</span></span><br><span class="line">plt.rcParams[<span class="string">'font.sans-serif'</span>]=[<span class="string">'Microsoft YaHei'</span>]</span><br><span class="line">plt.rcParams[<span class="string">'axes.unicode_minus'</span>]=<span class="literal">False</span></span><br><span class="line">plt.style.use(<span class="string">'ggplot'</span>)</span><br><span class="line">plt.plot(Lambdas,ridge_cofficients)</span><br><span class="line"><span class="comment">#x轴做对数处理</span></span><br><span class="line">plt.xscale(<span class="string">'log'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Log(Lambda)'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Cofficients'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></div><p>输出结果：</p><p><img src="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/机器学习图库/Figure_1-1583378255882.png" alt="Figure_1"></p><ul><li>正则化力度λ越大，权重系数w越小</li><li>正则化力度λ越小，权重系数w越大</li></ul><p>书上说在0.01附近大多数回归系数就趋于稳定，这哪看得出？所以定性的方法一般不太靠谱，还是用定量的方法吧！<br><strong>（2）交叉验证法确定$λ$值</strong></p><p>交叉验证法的思想是，将数据集拆分为$k$个数据组(每组样本量大体相当)，从$k$组中挑选$k-1$组用于模型的训练，剩下的1组用于模型的测试，则会有$k-1$个训练集和测试集配对，每一种训练集和测试集下都会有对应的一个模型及模型评分（如均方误差），进而可以得到一个平均评分。对于$λ$值则选择平均评分最优的$λ$值。RidgeCV(alphas=(0.1, 1.0, 10.0), fit_intercept=True, normalize=False, scoring=None, cv=None, gcv_mode=None, store_cv_values=False)•</p><ul><li>lambdas：用于指定多个$λ$值的元组或数组对象，默认包含0.1,1,10三种值。</li><li><p>fit_intercept：bool类型，是否需要拟合截距项，默认为True。</p></li><li><p>normalize：bool类型，建模时是否对数据集做标准化处理，默认为False。</p></li><li>scoring：指定用于模型评估的度量方法。</li><li>cv：指定交叉验证的重数。gcv_mode：指定广义交叉验证的方法。</li><li>store_cv_values：bool类型，是否保存每个λ\lambdaλ下交叉验证的评估信息，默认为False，只有cv为None时有效。</li></ul><p>代码：</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="false" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> RidgeCV</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">data=pd.read_csv(<span class="string">'diabetes.csv'</span>)</span><br><span class="line">x=data.iloc[:,<span class="number">1</span>:<span class="number">-1</span>]</span><br><span class="line">y=data[<span class="string">'Outcome'</span>]</span><br><span class="line"><span class="comment">#拆分为训练集和测试集</span></span><br><span class="line">x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=<span class="number">0.2</span>,random_state=<span class="number">1234</span>)</span><br><span class="line"><span class="comment">#构造不同的lambda值</span></span><br><span class="line">Lambdas=np.logspace(<span class="number">-5</span>,<span class="number">2</span>,<span class="number">200</span>)</span><br><span class="line"><span class="comment">#设置交叉验证的参数，使用均方误差评估</span></span><br><span class="line">ridge_cv=RidgeCV(alphas=Lambdas,normalize=<span class="literal">True</span>,scoring=<span class="string">'neg_mean_squared_error'</span>,cv=<span class="number">10</span>)</span><br><span class="line">ridge_cv.fit(x_train,y_train)</span><br><span class="line">print(ridge_cv.alpha_)</span><br></pre></td></tr></table></figure></div><p>输出结果：</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="false" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">0.038720387818125535</span></span><br></pre></td></tr></table></figure></div><h4 id="代码实现-1"><a href="#代码实现-1" class="headerlink" title="代码实现"></a>代码实现</h4><p>Ridge(alpha=1.0, fit_intercept=True, normalize=False, copy_X=True, max_iter=None, tol=0.001, solver=‘auto’, random_state=None)</p><ul><li><p>具有l2正则化的线性回归</p></li><li><p>alpha：正则化力度，也叫 λ，默认为1。λ取值：0.1~1 ~10</p></li><li><p>fit_intercept：bool类型，是否需要拟合截距项，默认为True。</p></li><li><p>normalize：bool类型，建模时是否对数据集做标准化处理，默认为False。</p><p>normalize=False:可以在fit之前调用preprocessing.StandardScaler标准化数据</p></li><li><p>copy_X：bool类型，是否复制自变量X的数值，默认为True。</p></li><li><p>max_iter：指定模型的最大迭代次数。</p></li><li><p>solver：指定模型求解最优化问题的算法，默认为’auto’。</p><p>sag:如果数据集、特征都比较大，选择该随机梯度下降优化</p></li><li><p>random_state：指定随机生成器的种子。</p></li><li><p>Ridge.coef_:回归权重_</p></li><li><p>Ridge.intercept_:回归偏置</p><blockquote><p>All last four solvers support both dense and sparse data. However,only ‘sag’ supports sparse input when ‘fit_intercept’is True.</p></blockquote><p>Ridge方法相当于SGDRegressor(penalty=’l2’, loss=”squared_loss”),只不过SGDRegressor实现了一个普通的随机梯度下降学习，推荐使用Ridge(实现了SAG)</p><ul><li><p>sklearn.linear_model.RidgeCV(_BaseRidgeCV, RegressorMixin)_</p></li><li><p>具有l2正则化的线性回归，可以进行交叉验证</p></li><li><p>_coef_:回归系数</p></li></ul><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="false" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">_BaseRidgeCV</span><span class="params">(LinearModel)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, alphas=<span class="params">(<span class="number">0.1</span>, <span class="number">1.0</span>, <span class="number">10.0</span>)</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 fit_intercept=True, normalize=False, scoring=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                 cv=None, gcv_mode=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                 store_cv_values=False)</span>:</span></span><br></pre></td></tr></table></figure></div></li></ul><p>代码：</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="false" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Ridge,RidgeCV</span><br><span class="line">data=pd.read_csv(<span class="string">'diabetes.csv'</span>)</span><br><span class="line">x=data.iloc[:,<span class="number">1</span>:<span class="number">-1</span>]</span><br><span class="line">y=data[<span class="string">'Outcome'</span>]</span><br><span class="line"><span class="comment">#拆分为训练集和测试集</span></span><br><span class="line">x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=<span class="number">0.2</span>,random_state=<span class="number">1234</span>)</span><br><span class="line"><span class="comment">#构造不同的lambda值</span></span><br><span class="line">Lambdas=np.logspace(<span class="number">-5</span>,<span class="number">2</span>,<span class="number">200</span>)</span><br><span class="line"><span class="comment">#设置交叉验证的参数，使用均方误差评估</span></span><br><span class="line">ridge_cv=RidgeCV(alphas=Lambdas,normalize=<span class="literal">True</span>,scoring=<span class="string">'neg_mean_squared_error'</span>,cv=<span class="number">10</span>)</span><br><span class="line">ridge_cv.fit(x_train,y_train)</span><br><span class="line">print(ridge_cv.alpha_)</span><br><span class="line"><span class="comment">#基于最佳lambda值建模</span></span><br><span class="line">ridge=Ridge(alpha=ridge_cv.alpha_,normalize=<span class="literal">True</span>)</span><br><span class="line">ridge.fit(x_train,y_train)</span><br><span class="line"><span class="comment">#打印回归系数</span></span><br><span class="line">print(pd.Series(index=[<span class="string">'Intercept'</span>]+x_train.columns.tolist(),</span><br><span class="line">                data=[ridge.intercept_]+ridge.coef_.tolist()))</span><br><span class="line"><span class="comment">#模型评估</span></span><br><span class="line">ridge_pred=ridge.predict(x_test)</span><br><span class="line"><span class="comment">#均方误差</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line">MSE=mean_squared_error(y_test,ridge_pred)</span><br><span class="line">print(MSE)</span><br></pre></td></tr></table></figure></div><p>输出结果：</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="false" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">0.038720387818125535</span></span><br><span class="line">Intercept                  <span class="number">-0.829372</span></span><br><span class="line">Glucose                     <span class="number">0.005658</span></span><br><span class="line">BloodPressure              <span class="number">-0.002019</span></span><br><span class="line">SkinThickness              <span class="number">-0.000805</span></span><br><span class="line">Insulin                    <span class="number">-0.000016</span></span><br><span class="line">BMI                         <span class="number">0.012776</span></span><br><span class="line">DiabetesPedigreeFunction    <span class="number">0.158436</span></span><br><span class="line">Age                         <span class="number">0.004989</span></span><br><span class="line">dtype: float64</span><br><span class="line"><span class="number">0.16870817670347735</span></span><br></pre></td></tr></table></figure></div><h3 id="LASSO回归"><a href="#LASSO回归" class="headerlink" title="LASSO回归"></a>LASSO回归</h3><h4 id="参数推导-1"><a href="#参数推导-1" class="headerlink" title="参数推导"></a>参数推导</h4><p>岭回归无法剔除变量，而LASSO回归模型，将惩罚项由L2范数变为L1范数，可以将一些不重要的回归系数缩减为0，达到剔除变量的目的。</p><p>$J(β)=\sum\left(y-Xβ\right)^{2}+λ||β||_1=\sum\left(y-Xβ\right)^{2}+\sumλ|β|=ESS（β）+λl_1(β)$</p><h4 id="λ-的选择-1"><a href="#λ-的选择-1" class="headerlink" title="$λ$的选择"></a>$λ$的选择</h4><p>直接使用交叉验证法</p><p>LassoCV(eps=0.001, n_alphas=100, alphas=None, fit_intercept=True, normalize=False, precompute=‘auto’, max_iter=1000, tol=0.0001, copy_X=True, cv=None, verbose=False, n_jobs=1, positive=False, random_state=None, selection=‘cyclic’)</p><ul><li>eps：指代λ\lambdaλ最小值与最大值的商，默认为0.001。</li><li>n_alphas：指定λ\lambdaλ的个数，默认为100个。</li><li>alphas：指定具体的λ\lambdaλ列表用于模型的运算。</li><li>fit_intercept：bool类型，是否需要拟合截距项，默认为True。</li><li>normalize：bool类型，建模时是否对数据集做标准化处理，默认为False。</li><li>precompute：bool类型，是否在建模前计算Gram矩阵提升运算速度，默认为False。</li><li>max_iter：指定模型的最大迭代次数。</li><li>tol：指定模型收敛的阈值，默认为0.0001。</li><li>copy_X：bool类型，是否复制自变量X的数值，默认为True。</li><li>cv：指定交叉验证的重数。</li><li>verbose：bool类型，是否返回模型运行的详细信息，默认为False。</li><li>n_jobs：指定使用的CPU数量，默认为1，如果为-1表示所有CPU用于交叉验证的运算。</li><li>positive：bool类型，是否将回归系数强制为正数，默认为False。</li><li>random_state：指定随机生成器的种子。</li><li>selection：指定每次迭代选择的回归系数，如果为’random’，表示每次迭代中将随机更新回归系数；如果为’cyclic’，则每次迭代时回归系数的更新都基于上一次运算。</li></ul><p>代码：</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="false" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LassoCV</span><br><span class="line">data=pd.read_csv(<span class="string">'diabetes.csv'</span>)</span><br><span class="line">x=data.iloc[:,<span class="number">1</span>:<span class="number">-1</span>]</span><br><span class="line">y=data[<span class="string">'Outcome'</span>]</span><br><span class="line"><span class="comment">#拆分为训练集和测试集</span></span><br><span class="line">x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=<span class="number">0.2</span>,random_state=<span class="number">1234</span>)</span><br><span class="line"><span class="comment">#构造不同的lambda值</span></span><br><span class="line">Lambdas=np.logspace(<span class="number">-5</span>,<span class="number">2</span>,<span class="number">200</span>)</span><br><span class="line"><span class="comment">#设置交叉验证的参数，使用均方误差评估</span></span><br><span class="line">lasso_cv=LassoCV(alphas=Lambdas,normalize=<span class="literal">True</span>,cv=<span class="number">10</span>,max_iter=<span class="number">10000</span>)</span><br><span class="line">lasso_cv.fit(x_train,y_train)</span><br><span class="line">print(lasso_cv.alpha_)</span><br></pre></td></tr></table></figure></div><p>输出结果：</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="false" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">0.00011357333583431052</span></span><br></pre></td></tr></table></figure></div><h4 id="代码实现-2"><a href="#代码实现-2" class="headerlink" title="代码实现"></a>代码实现</h4><p>Lasso(alpha=1.0, fit_intercept=True, normalize=False, precompute=False, copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, positive=False, random_state=None, selection=‘cyclic’)</p><ul><li>alphas：指定λ\lambdaλ值，默认为1。</li><li>fit_intercept：bool类型，是否需要拟合截距项，默认为True。</li><li>normalize：bool类型，建模时是否对数据集做标准化处理，默认为False。</li><li>precompute：bool类型，是否在建模前计算Gram矩阵提升运算速度，默认为False。</li><li>copy_X：bool类型，是否复制自变量X的数值，默认为True。</li><li>max_iter：指定模型的最大迭代次数。</li><li>tol：指定模型收敛的阈值，默认为0.0001。</li><li>warm_start：bool类型，是否将前一次训练结果用作后一次的训练，默认为False。</li><li>positive：bool类型，是否将回归系数强制为正数，默认为False。</li><li>random_state：指定随机生成器的种子。</li><li>selection：指定每次迭代选择的回归系数，如果为’random’，表示每次迭代中将随机更新回归系数；如果为’cyclic’，则每次迭代时回归系数的更新都基于上一次运算。</li></ul><p>代码：</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="false" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Lasso,LassoCV</span><br><span class="line">data=pd.read_csv(<span class="string">'diabetes.csv'</span>)</span><br><span class="line">x=data.iloc[:,<span class="number">1</span>:<span class="number">-1</span>]</span><br><span class="line">y=data[<span class="string">'Outcome'</span>]</span><br><span class="line"><span class="comment">#拆分为训练集和测试集</span></span><br><span class="line">x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=<span class="number">0.2</span>,random_state=<span class="number">1234</span>)</span><br><span class="line"><span class="comment">#构造不同的lambda值</span></span><br><span class="line">Lambdas=np.logspace(<span class="number">-5</span>,<span class="number">2</span>,<span class="number">200</span>)</span><br><span class="line"><span class="comment">#设置交叉验证的参数，使用均方误差评估</span></span><br><span class="line">lasso_cv=LassoCV(alphas=Lambdas,normalize=<span class="literal">True</span>,cv=<span class="number">10</span>,max_iter=<span class="number">10000</span>)</span><br><span class="line">lasso_cv.fit(x_train,y_train)</span><br><span class="line"><span class="comment">#基于最佳lambda值建模</span></span><br><span class="line">lasso=Lasso(alpha=lasso_cv.alpha_,normalize=<span class="literal">True</span>,max_iter=<span class="number">10000</span>)</span><br><span class="line">lasso.fit(x_train,y_train)</span><br><span class="line"><span class="comment">#打印回归系数</span></span><br><span class="line">print(pd.Series(index=[<span class="string">'Intercept'</span>]+x_train.columns.tolist(),</span><br><span class="line">                data=[lasso.intercept_]+lasso.coef_.tolist()))</span><br><span class="line"><span class="comment">#模型评估</span></span><br><span class="line">lasso_pred=lasso.predict(x_test)</span><br><span class="line"><span class="comment">#均方误差</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line">MSE=mean_squared_error(y_test,lasso_pred)</span><br><span class="line">print(MSE)</span><br></pre></td></tr></table></figure></div><p>输出结果：</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="false" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Intercept                  <span class="number">-0.842415</span></span><br><span class="line">Glucose                     <span class="number">0.005801</span></span><br><span class="line">BloodPressure              <span class="number">-0.001983</span></span><br><span class="line">SkinThickness              <span class="number">-0.000667</span></span><br><span class="line">Insulin                    <span class="number">-0.000011</span></span><br><span class="line">BMI                         <span class="number">0.012670</span></span><br><span class="line">DiabetesPedigreeFunction    <span class="number">0.153236</span></span><br><span class="line">Age                         <span class="number">0.004865</span></span><br><span class="line">dtype: float64</span><br><span class="line"><span class="number">0.16876008361250405</span></span><br></pre></td></tr></table></figure></div><p>相对于岭回归而言，可以看到LASSO回归剔除了两个变量，降低了模型的复杂度，同时减少了均方误差，提高了模型的拟合效果。</p><h3 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h3><p>逻辑回归（Logistic Regression）是机器学习中的一种分类模型，逻辑回归是一种分类算法，虽然名字中带有回归，但是它与回归之间有一定的联系。由于算法的简单和高效，在实际中应用非常广泛。</p><h4 id="逻辑回归的应用场景"><a href="#逻辑回归的应用场景" class="headerlink" title="逻辑回归的应用场景"></a>逻辑回归的应用场景</h4><ul><li>广告点击率</li><li>是否为垃圾邮件</li><li>是否患病</li><li>金融诈骗</li><li>虚假账号</li></ul><p>目的：分类还是回归？经典的二分类算法！</p><p>逻辑回归的决策边界：可以是非线性的</p><h4 id="逻辑回归的原理"><a href="#逻辑回归的原理" class="headerlink" title="逻辑回归的原理"></a>逻辑回归的原理</h4><p><strong>输入</strong></p><p>$h_w(x)=w_1x_1+w_2x_2+w_3x_3…+b=w^Tx+b$</p><p><strong>Sigmoid函数</strong></p><p>公式：$g(z)=\frac{1}{1+e^{-z}}$</p><p>自变量取值为任意实数，值域[0,1]</p><p><img src="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/机器学习图库/image-20200304105302691.png" alt="image-20200304105302691"></p><blockquote><p>解释：将任意的输入映射到了[0,1]区间，我们在线性回归中可以得到一个预测值，再将该值映射到Sigmoid 函数中这样就完成了由值到概率的转换，也就是分类任务</p></blockquote><p>预测函数：</p><script type="math/tex;mode=display">h_w(x)=g(w^Tx)=\frac{1}{1+e^{-w^Tx}}</script><p>分类任务：</p><p>$P(y=1|x;w)=h_w(x)$ ; $P(y=0|x;w)=1-h_w(x)$</p><p>整合得： $P(y|x;w)=(h_w(x))^y(1-h_w(x))^{1-y}$</p><blockquote><p>解释：对于二分类任务（0，1），整合后y取0只保留$(1-h_w(x))^{1-y}$，y取1只保留$(h_w(x))^y$</p></blockquote><p><img src="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/机器学习图库/逻辑回归运算过程.png" alt="逻辑回归运算过程"></p><h4 id="损失以及优化"><a href="#损失以及优化" class="headerlink" title="损失以及优化"></a>损失以及优化</h4><h5 id="1、损失"><a href="#1、损失" class="headerlink" title="1、损失"></a>1、损失</h5><p>逻辑回归的损失，称之为<strong>对数似然损失</strong>，公式如下：</p><ul><li>分开类别：</li></ul><script type="math/tex;mode=display">\operatorname{cost}\left(h_{\theta}(x), y\right)=\left\{\begin{array}{ll}
-\log \left(h_{\theta}(x)\right) & \text { if } y=1 \\
-\log \left(1-h_{\theta}(x)\right) & \text { if } y=0
\end{array}\right.</script><p><img src="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/机器学习图库/单个损失解释.png" alt="单个损失解释"></p><p>综合完整损失函数：$$<br>\operatorname{cost}\left(h_{\theta}(x), y\right)=\sum_{i=1}^{m}-y_{i} \log \left(h_{\theta}(x)\right)-\left(1-y_{i}\right) \log \left(1-h_{\theta}(x)\right)</p><p>$$</p><p><img src="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/机器学习图库/损失计算过程.png" alt="损失计算过程"></p><h5 id="2、优化"><a href="#2、优化" class="headerlink" title="2、优化"></a>2、优化</h5><p>同样使用梯度下降优化算法，去减少损失函数的值。这样去更新逻辑回归前面对应算法的权重参数，<strong>提升原本属于1类别的概率，降低原本是0类别的概率。</strong></p><h4 id="逻辑回归算法API"><a href="#逻辑回归算法API" class="headerlink" title="逻辑回归算法API"></a>逻辑回归算法API</h4><p>sklearn.linear_model.LogisticRegression(solver=’liblinear’, penalty=‘l2’, C = 1.0)</p><ul><li><p>solver:优化求解方式</p><p>默认开源的liblinear库实现，内部使用了坐标轴下降法来迭代优化损失函数</p></li><li><p>sag：根据数据集自动选择，随机平均梯度下降</p></li><li><p>penalty：正则化的种类</p></li><li><p>C：正则化力度</p></li></ul><blockquote><p><strong>默认将类别数量少的当做正例</strong></p></blockquote><p>LogisticRegression方法相当于 SGDClassifier(loss=”log”, penalty=” “),SGDClassifier实现了一个普通的随机梯度下降学习，也支持平均随机梯度下降法（ASGD），可以通过设置average=True。而使用LogisticRegression(实现了SAG)良／恶性乳腺癌肿瘤预测</p><h4 id="案例：癌症分类预测"><a href="#案例：癌症分类预测" class="headerlink" title="案例：癌症分类预测"></a>案例：癌症分类预测</h4><p>良/恶性乳腺癌肿瘤预测</p><p><strong>数据描述:</strong></p><p>（1）699条样本，共11列数据，第一列用语检索的id，后9列分别是与肿瘤</p><p>相关的医学特征，最后一列表示肿瘤类型的数值。</p><p>（2）包含16个缺失值，用”?”标出。</p><p>原始数据的下载地址：<a href="https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/" target="_blank" rel="noopener external nofollow noreferrer">https://archive.ics.uci.edu/ml/machine-learning-databases/</a></p><p><strong>分析:</strong></p><ul><li>缺失值处理</li><li>标准化处理</li><li>逻辑回归预测</li></ul><p>代码：</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="false" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#逻辑回归进行癌症预测</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="comment"># 1、读取数据</span></span><br><span class="line">column_name = [<span class="string">'Sample code number'</span>, <span class="string">'Clump Thickness'</span>, <span class="string">'Uniformity of Cell Size'</span>, <span class="string">'Uniformity of Cell Shape'</span>,</span><br><span class="line">                   <span class="string">'Marginal Adhesion'</span>, <span class="string">'Single Epithelial Cell Size'</span>, <span class="string">'Bare Nuclei'</span>, <span class="string">'Bland Chromatin'</span>,</span><br><span class="line">                   <span class="string">'Normal Nucleoli'</span>, <span class="string">'Mitoses'</span>, <span class="string">'Class'</span>]</span><br><span class="line">data = pd.read_csv(<span class="string">"https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data"</span>,</span><br><span class="line">                       names=column_name)</span><br><span class="line"><span class="comment"># 2、数据处理—处理缺失值</span></span><br><span class="line">data = data.replace(to_replace=<span class="string">'?'</span>, value=np.nan)   <span class="comment">#1)替换np.nan</span></span><br><span class="line">data = data.dropna()    <span class="comment">#2)删除缺失值</span></span><br><span class="line">print(data.isnull().any()) <span class="comment">#确认不存在缺失值</span></span><br><span class="line"><span class="comment"># 取出特征值</span></span><br><span class="line">x = data[column_name[<span class="number">1</span>:<span class="number">10</span>]]  <span class="comment">#x=data.iloc[:,1:-1]</span></span><br><span class="line">y = data[column_name[<span class="number">10</span>]]   <span class="comment">#y=data['Class']</span></span><br><span class="line"><span class="comment">#3、分割数据集</span></span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=<span class="number">0.3</span>)</span><br><span class="line"><span class="comment">#4、特征工程—标准化</span></span><br><span class="line">std = StandardScaler()</span><br><span class="line">x_train = std.fit_transform(x_train)</span><br><span class="line">x_test = std.transform(x_test)</span><br><span class="line"><span class="comment"># 5、使用逻辑回归</span></span><br><span class="line">lr = LogisticRegression()</span><br><span class="line">lr.fit(x_train, y_train)</span><br><span class="line"><span class="comment">#逻辑回归的模型参数：回归系数和偏置</span></span><br><span class="line">print(<span class="string">"权重：\n"</span>, lr.coef_)</span><br><span class="line">print(<span class="string">"偏置：\n"</span>, lr.intercept_)</span><br><span class="line"><span class="comment">#6、模型评估</span></span><br><span class="line"><span class="comment">#方法1：直接比对真实值和预测值</span></span><br><span class="line">y_predict=lr.predict(x_test)</span><br><span class="line">print(<span class="string">'y_predict:\n'</span>,y_predict)</span><br><span class="line">print(<span class="string">'直接比对真实值和预测值：\n'</span>,y_test  y_predict)</span><br><span class="line"><span class="comment">#方法2：计算准确率</span></span><br><span class="line">score=lr.score(x_test,y_test)</span><br><span class="line">print(<span class="string">'准确率为：\n'</span>,score)</span><br></pre></td></tr></table></figure></div><p>输出结果：</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="false" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">Sample code number             <span class="literal">False</span></span><br><span class="line">Clump Thickness                <span class="literal">False</span></span><br><span class="line">Uniformity of Cell Size        <span class="literal">False</span></span><br><span class="line">Uniformity of Cell Shape       <span class="literal">False</span></span><br><span class="line">Marginal Adhesion              <span class="literal">False</span></span><br><span class="line">Single Epithelial Cell Size    <span class="literal">False</span></span><br><span class="line">Bare Nuclei                    <span class="literal">False</span></span><br><span class="line">Bland Chromatin                <span class="literal">False</span></span><br><span class="line">Normal Nucleoli                <span class="literal">False</span></span><br><span class="line">Mitoses                        <span class="literal">False</span></span><br><span class="line">Class                          <span class="literal">False</span></span><br><span class="line">dtype: bool</span><br><span class="line">权重：</span><br><span class="line"> [[<span class="number">1.4449604</span>  <span class="number">0.10902357</span> <span class="number">0.64529009</span> <span class="number">1.02979746</span> <span class="number">0.2544256</span>  <span class="number">1.55064687</span></span><br><span class="line">  <span class="number">0.92516667</span> <span class="number">0.62683691</span> <span class="number">0.54739363</span>]]</span><br><span class="line">偏置：</span><br><span class="line"> [<span class="number">-1.08426503</span>]</span><br><span class="line">y_predict: <span class="comment">#注：2——良性  4——恶性</span></span><br><span class="line"> [<span class="number">4</span> <span class="number">4</span> <span class="number">2</span> <span class="number">4</span> <span class="number">2</span> <span class="number">2</span> <span class="number">4</span> <span class="number">2</span> <span class="number">4</span> <span class="number">4</span> <span class="number">2</span> <span class="number">2</span> <span class="number">4</span> <span class="number">2</span> <span class="number">4</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">4</span> <span class="number">4</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">4</span> <span class="number">2</span> <span class="number">2</span> <span class="number">4</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">4</span> <span class="number">2</span></span><br><span class="line"> <span class="number">2</span> <span class="number">2</span> <span class="number">4</span> <span class="number">2</span> <span class="number">4</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">4</span> <span class="number">4</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">4</span> <span class="number">4</span> <span class="number">4</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">4</span> <span class="number">4</span> <span class="number">2</span> <span class="number">4</span> <span class="number">2</span> <span class="number">2</span> <span class="number">4</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">4</span> <span class="number">2</span> <span class="number">4</span> <span class="number">2</span> <span class="number">2</span> <span class="number">4</span> <span class="number">4</span></span><br><span class="line"> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">4</span> <span class="number">4</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">4</span> <span class="number">2</span> <span class="number">4</span> <span class="number">4</span> <span class="number">4</span> <span class="number">4</span> <span class="number">2</span> <span class="number">4</span> <span class="number">2</span> <span class="number">2</span> <span class="number">4</span> <span class="number">2</span> <span class="number">2</span> <span class="number">4</span> <span class="number">2</span> <span class="number">4</span> <span class="number">4</span> <span class="number">2</span> <span class="number">2</span> <span class="number">4</span> <span class="number">2</span> <span class="number">2</span> <span class="number">4</span> <span class="number">2</span> <span class="number">2</span> <span class="number">4</span></span><br><span class="line"> <span class="number">2</span> <span class="number">2</span> <span class="number">4</span> <span class="number">2</span> <span class="number">2</span> <span class="number">4</span> <span class="number">2</span> <span class="number">4</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">4</span> <span class="number">4</span> <span class="number">4</span> <span class="number">2</span> <span class="number">4</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">4</span> <span class="number">2</span> <span class="number">4</span> <span class="number">2</span> <span class="number">4</span> <span class="number">2</span> <span class="number">4</span> <span class="number">4</span> <span class="number">2</span> <span class="number">4</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">4</span> <span class="number">4</span> <span class="number">2</span></span><br><span class="line"> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">4</span> <span class="number">2</span> <span class="number">2</span> <span class="number">4</span> <span class="number">4</span> <span class="number">4</span> <span class="number">4</span> <span class="number">2</span> <span class="number">2</span> <span class="number">4</span> <span class="number">4</span> <span class="number">2</span> <span class="number">4</span> <span class="number">4</span> <span class="number">2</span> <span class="number">4</span> <span class="number">2</span> <span class="number">4</span> <span class="number">4</span> <span class="number">4</span> <span class="number">2</span> <span class="number">2</span> <span class="number">4</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">4</span> <span class="number">2</span> <span class="number">4</span> <span class="number">2</span> <span class="number">4</span> <span class="number">2</span></span><br><span class="line"> <span class="number">2</span> <span class="number">4</span> <span class="number">4</span> <span class="number">2</span> <span class="number">4</span> <span class="number">2</span> <span class="number">4</span> <span class="number">2</span> <span class="number">4</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">4</span> <span class="number">4</span> <span class="number">4</span> <span class="number">4</span> <span class="number">4</span> <span class="number">2</span> <span class="number">4</span>]</span><br><span class="line">直接比对真实值和预测值：</span><br><span class="line"> <span class="number">221</span>    <span class="literal">True</span></span><br><span class="line"><span class="number">266</span>    <span class="literal">True</span></span><br><span class="line"><span class="number">4</span>      <span class="literal">True</span></span><br><span class="line"><span class="number">183</span>    <span class="literal">True</span></span><br><span class="line"><span class="number">341</span>    <span class="literal">True</span></span><br><span class="line">       ... </span><br><span class="line"><span class="number">55</span>     <span class="literal">True</span></span><br><span class="line"><span class="number">132</span>    <span class="literal">True</span></span><br><span class="line"><span class="number">15</span>     <span class="literal">True</span></span><br><span class="line"><span class="number">597</span>    <span class="literal">True</span></span><br><span class="line"><span class="number">479</span>    <span class="literal">True</span></span><br><span class="line">Name: Class, Length: <span class="number">205</span>, dtype: bool</span><br><span class="line">准确率为：</span><br><span class="line"> <span class="number">0.9707317073170731</span></span><br></pre></td></tr></table></figure></div><p>在很多分类场景当中我们不一定只关注预测的准确率！！！！！</p><p>比如以这个癌症举例子！！！我们并不关注预测的准确率，而是关注在所有的样本当中，癌症患者有没有被全部预测（检测）出来。</p><h3 id="分类模型的评估"><a href="#分类模型的评估" class="headerlink" title="分类模型的评估"></a>分类模型的评估</h3><p>准确率、精确率、召回率、f1_score，混淆矩阵，ks，ks曲线，ROC曲线，psi等。</p><h4 id="准确率、精确率、召回率、f1-score"><a href="#准确率、精确率、召回率、f1-score" class="headerlink" title="准确率、精确率、召回率、f1_score"></a>准确率、精确率、召回率、f1_score</h4><p><strong>准确率</strong>（Accuracy）的定义是：对于给定的测试集，分类模型正确分类的样本数与总样本数之比；</p><p>代码示例：</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="false" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#1、准确率</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line">y_pred = [<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>,<span class="number">9</span>,<span class="number">9</span>,<span class="number">8</span>,<span class="number">5</span>,<span class="number">8</span>]</span><br><span class="line">y_true = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>,<span class="number">2</span>,<span class="number">6</span>,<span class="number">3</span>,<span class="number">5</span>,<span class="number">9</span>]</span><br><span class="line"></span><br><span class="line">accuracy_score(y_true, y_pred)</span><br><span class="line">Out[<span class="number">127</span>]: <span class="number">0.33333333333333331</span></span><br><span class="line">    </span><br><span class="line">accuracy_score(y_true, y_pred, normalize=<span class="literal">False</span>)  <span class="comment"># 类似海明距离，每个类别求准确后，再求微平均</span></span><br><span class="line">Out[<span class="number">128</span>]: <span class="number">3</span></span><br></pre></td></tr></table></figure></div><p><strong>精确率</strong>(Precision)：预测结果为正例样本中真实为正例的比例（了解）</p><p><img src="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/机器学习图库/精确率.png" alt="精确率"></p><p><strong>召回率</strong>(Recall)：真实为正例的样本中预测结果为正例的比例（查的全，对正样本的区分能力）</p><p>召回率：查得全不全 应用：质量检测 次品</p><p><img src="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/机器学习图库/召回率.png" alt="召回率"></p><p>那么怎么更好理解这个两个概念</p><p><img src="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/机器学习图库/精确率与召回率理解.png" alt="精确率与召回率理解"></p><p><strong>F1_score</strong>：反映了模型的稳健型</p><p>在理想情况下，我们希望模型的精确率越高越好，同时召回率也越高越高，但是，现实情况往往事与愿违，在现实情况下，精确率和召回率像是坐在跷跷板上一样，往往出现一个值升高，另一个值降低，那么，有没有一个指标来综合考虑精确率和召回率了，这个指标就是F值。F值的计算公式为：</p><p>$F=\frac{(a^2+1)<em>P</em>R}{a^2*(P+R)}$ 式中：$P：Precision$，$R：Recall$，$a$：权重因子。</p><p>当$a=1$时，$F$值便是$F1$值，代表精确率和召回率的权重是一样的，是最常用的一种评价指标。</p><p>$F 1=\frac{2 T P}{2 T P+F N+F P}=\frac{2 \cdot \text { Precision } \cdot \text { Recall}}{\text { Precision }+\text { Recall}}$</p><h4 id="混淆矩阵"><a href="#混淆矩阵" class="headerlink" title="混淆矩阵"></a>混淆矩阵</h4><p>在分类任务下，预测结果(Predicted Condition)与正确标记(True Condition)之间存在四种不同的组合，构成混淆矩阵(适用于多分类)</p><p>混淆矩阵一级指标（最底层的）：</p><p>真实值是positive，模型认为是positive的数量（True Positive=TP）；</p><p>真实值是positive，模型认为是negative的数量（False Negative=FN）：这就是统计学上的第一类错误（Type I Error）；</p><p>真实值是negative，模型认为是positive的数量（False Positive=FP）：这就是统计学上的第二类错误（Type II Error）；</p><p>真实值是negative，模型认为是negative的数量（True Negative=TN）</p><p><img src="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/机器学习图库/20180531113257203.png" alt="img"></p><p>示例及实现代码</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="false" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 假如有一个模型在测试集上得到的预测结果为：</span></span><br><span class="line">y_true=[<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">3</span>]<span class="comment"># 实际的类别  </span></span><br><span class="line">y_pred=[<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">3</span>,<span class="number">3</span>]<span class="comment"># 模型预测的类别  </span></span><br><span class="line"><span class="comment">#使用sklearn模块计算混淆矩阵</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix</span><br><span class="line">confusion_mat=confusion_matrix(y_true,y_pred)</span><br><span class="line">print(confusion_mat) <span class="comment">#看看混淆矩阵长啥样 </span></span><br><span class="line"> </span><br><span class="line">[[<span class="number">2</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">0</span> <span class="number">2</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">2</span>]]</span><br></pre></td></tr></table></figure></div><p><img src="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/机器学习图库/20190611144159116.png" alt="img"></p><p>混淆矩阵可视化</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="false" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_confusion_matrix</span><span class="params">(confusion_mat)</span>:</span>  <span class="comment">#将混淆矩阵画图并显示出来''' </span></span><br><span class="line">	plt.title(<span class="string">'Confusion matrix'</span>)</span><br><span class="line">	plt.imshow(confusion_mat,interpolation=<span class="string">'nearest'</span>,cmap=plt.cm.gray)</span><br><span class="line">	plt.colorbar()</span><br><span class="line">	tick_marks=np.arange(confusion_mat.shape[<span class="number">0</span>])</span><br><span class="line">	plt.xticks(tick_marks,tick_marks)</span><br><span class="line">	plt.yticks(tick_marks,tick_marks)</span><br><span class="line">	plt.ylabel(<span class="string">'True label'</span>)</span><br><span class="line">	plt.xlabel(<span class="string">'Predicted label'</span>)</span><br><span class="line">	plt.show()</span><br><span class="line">plot_confusion_matrix(confusion_mat)</span><br></pre></td></tr></table></figure></div><p><img src="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/机器学习图库/Figure_111.png" alt="Figure_111"></p><p><strong>分类评估报告API</strong></p><p>sklearn.metrics.classification_report(y_true, y_pred, labels=[], target_names=None )</p><ul><li>y_true：真实目标值</li><li>y_pred：估计器预测目标值</li><li>labels:指定类别对应的数字</li><li>target_names：目标类别名称</li><li>return：每个类别精确率与召回率</li></ul><p>上文逻辑回归进行癌症预测代码+下文代码：</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="false" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line">report=classification_report(y_test, lr.predict(x_test), labels=[<span class="number">2</span>, <span class="number">4</span>], target_names=[<span class="string">'良性'</span>, <span class="string">'恶性'</span>])</span><br><span class="line">print(<span class="string">"精确率和召回率为："</span>,report)</span><br></pre></td></tr></table></figure></div><p>输出结果：</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="false" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">精确率和召回率为：               precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">                       良性       <span class="number">0.98</span>      <span class="number">0.98</span>      <span class="number">0.98</span>       <span class="number">132</span></span><br><span class="line">                       恶性       <span class="number">0.96</span>      <span class="number">0.97</span>      <span class="number">0.97</span>        <span class="number">73</span></span><br><span class="line"></span><br><span class="line">    accuracy                           <span class="number">0.98</span>       <span class="number">205</span></span><br><span class="line">   macro avg       <span class="number">0.97</span>      <span class="number">0.97</span>      <span class="number">0.97</span>       <span class="number">205</span></span><br><span class="line">weighted avg       <span class="number">0.98</span>      <span class="number">0.98</span>      <span class="number">0.98</span>       <span class="number">205</span></span><br></pre></td></tr></table></figure></div><p>二级指标</p><p>混淆矩阵里面统计的是个数，有时候面对大量的数据，光凭算个数，很难衡量模型的优劣。因此混淆矩阵在基本的统计结果上又延伸了如下4个指标，我称他们是二级指标（通过最底层指标加减乘除得到的）：</p><p>准确率（Accuracy）—— 针对整个模型</p><p>精确率（Precision）</p><p>灵敏度（Sensitivity）：就是召回率（Recall）</p><p>特异度（Specificity）</p><p><img src="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/机器学习图库/20180531115939413.png" alt="img"></p><p><strong>例：</strong></p><p>假设这样一个情况，如果99个样本癌症，1个样本非癌症，不管怎样我全都预测正例(默认癌症为正例),准确率就为99%但是这样效果并不好，这就是样本不均衡下的评估问题</p><p>解：准确率：99% 召回率：99/99=100% 精确率：99%</p><p>F1—score：2*99%/199%=99.497%</p><p>AUC：0.5 —不好的模型 AUC下文介绍</p><ul><li><p>TPR=100%</p></li><li><p>FPR=1/1=100%</p></li></ul><p>问题：如何衡量样本不均衡下的评估？</p><h4 id="ROC曲线和AUC计算"><a href="#ROC曲线和AUC计算" class="headerlink" title="ROC曲线和AUC计算"></a>ROC曲线和AUC计算</h4><p>计算ROC值</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="false" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_auc_score</span><br><span class="line">y_true = np.array([<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">y_scores = np.array([<span class="number">0.1</span>, <span class="number">0.4</span>, <span class="number">0.35</span>, <span class="number">0.8</span>])</span><br><span class="line">print(roc_auc_score(y_true, y_scores))</span><br><span class="line"></span><br><span class="line"><span class="number">0.75</span></span><br></pre></td></tr></table></figure></div><ul><li>TPR = TP / (TP + FN)<ul><li>所有真实类别为1的样本中，预测类别为1的比例</li></ul></li><li>FPR = FP / (FP + FN)<ul><li>所有真实类别为0的样本中，预测类别为1的比例</li></ul></li></ul><p><strong>ROC曲线</strong></p><ul><li>ROC曲线的横轴就是FPR，纵轴就是TPR，当二者相等时，表示的意义则是：对于不论真实类别是1还是0的样本，分类器预测为1的概率是相等的，此时AUC为0.5</li></ul><p><img src="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/机器学习图库/ROC.png" alt="ROC"></p><p><strong>AUC指标</strong></p><p>AUC(AUC值是一个概率值)的概率意义是随机取一对正负样本，正样本得分大于负样本的概率<br>AUC的最小值为0.5，最大值为1，取值越高越好<br>AUC=1，完美分类器，采用这个预测模型时，不管设定什么阈值都能得出完美预测。绝大多数预测的场合，不存在完美分类器。<br>0.5&lt;AUC&lt;1，优于随机猜测。这个分类器（模型）妥善设定阈值的话，能有预测价值。</p><blockquote><p>AUC就是ROC 曲线下的面积，通常情况下数值介于0.5-1之间，可以评价分类器的好坏，数值越大说明越好。</p></blockquote><p><strong>AUC计算API</strong></p><p>from sklearn.metrics import roc_auc_score</p><p>sklearn.metrics.roc_auc_score(y_true, y_score)</p><ul><li>计算ROC曲线面积，即AUC值</li><li>y_true:每个样本的真实类别，必须为0(反例),1(正例)标记</li><li>y_score:每个样本预测的概率值</li></ul><p>代码（接上文代码）：</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="false" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">print(y_test.head())</span><br><span class="line"><span class="comment">#y_true:每个样本的真实类别，必须为0（反例），1（正例）标记</span></span><br><span class="line"><span class="comment">#将y_test 转换成0和1</span></span><br><span class="line">y_test = np.where(y_test &gt; <span class="number">2.5</span>, <span class="number">1</span>, <span class="number">0</span>) <span class="comment">#y_test数值大于2.5设置为1，不大于设置为0</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_auc_score</span><br><span class="line">print(<span class="string">"AUC指标："</span>, roc_auc_score(y_test, lr.predict(x_test)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># ROC曲线</span></span><br><span class="line"><span class="comment"># y_score为模型预测正例的概率</span></span><br><span class="line">y_score = lr.predict_proba(x_test)[:, <span class="number">1</span>]</span><br><span class="line"><span class="comment"># 计算不同阈值下，fpr和tpr的组合之，fpr表示1-Specificity，tpr表示Sensitivity</span></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line">fpr, tpr, threshold = metrics.roc_curve(y_test, y_score)</span><br><span class="line"><span class="comment"># 计算AUC</span></span><br><span class="line">roc_auc = metrics.auc(fpr, tpr)</span><br><span class="line"><span class="comment"># 绘制面积图</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.stackplot(fpr, tpr, color=<span class="string">'steelblue'</span>, alpha=<span class="number">0.5</span>, edgecolor=<span class="string">'black'</span>)</span><br><span class="line"><span class="comment"># 添加ROC曲线的轮廓</span></span><br><span class="line">plt.plot(fpr, tpr, color=<span class="string">'black'</span>, lw=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 添加对角线作为参考线</span></span><br><span class="line">plt.plot([<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>], color=<span class="string">'red'</span>, linestyle=<span class="string">'--'</span>)</span><br><span class="line">plt.text(<span class="number">0.5</span>, <span class="number">0.3</span>, <span class="string">'ROC curve (area=%0.2f)'</span> % roc_auc)</span><br><span class="line">plt.xlabel(<span class="string">'1-Specificity'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Sensitivity'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></div><p>输出结果：</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="false" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">53</span>     <span class="number">4</span></span><br><span class="line"><span class="number">503</span>    <span class="number">2</span></span><br><span class="line"><span class="number">434</span>    <span class="number">2</span></span><br><span class="line"><span class="number">495</span>    <span class="number">2</span></span><br><span class="line"><span class="number">595</span>    <span class="number">2</span></span><br><span class="line">Name: Class, dtype: int64</span><br><span class="line">AUC指标： <span class="number">0.9556899934597778</span></span><br></pre></td></tr></table></figure></div><p><img src="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/机器学习图库/Figure_11.png" alt="Figure_11"></p><p><strong>总结</strong></p><ul><li>AUC只能用来评价二分类</li><li>AUC非常适合评价样本不平衡中的分类器性能</li></ul><h4 id="LIft和gain"><a href="#LIft和gain" class="headerlink" title="LIft和gain"></a>LIft和gain</h4><p>Lift图衡量的是，与不利用模型相比，模型的预测能力“变好”了多少，lift(提升指数)越大，模型的运行效果越好。<br>Gain图是描述整体精准度的指标。<br>计算公式如下：</p><p>$Lift=\frac{\frac{TP}{TP+FP}}{\frac{P}{P+N}}$</p><p>$Gain=\frac{TP}{TP+FP}$</p><p>作图步骤：<br>（1） 根据学习器的预测结果（注意，是正例的概率值，非0/1变量）对样本进行排序（从大到小）——-这就是截断点依次选取的顺序；<br>（2） 按顺序选取截断点，并计算Lift和Gain —-也可以只选取n个截断点，分别在1/n，2/n，3/n等位置</p><p><img src="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/机器学习图库/20180309191310524.png" alt="20180309191310524"></p><p><img src="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/机器学习图库/20180309191506788.png" alt="20180309191506788"></p><h3 id="回归模型的评估"><a href="#回归模型的评估" class="headerlink" title="回归模型的评估"></a>回归模型的评估</h3><p>主要有以下方法：</p><div class="table-container"><table><thead><tr><th style="text-align:center"><strong>指标</strong></th><th><strong>描述</strong></th><th style="text-align:left"><strong>metrics方法</strong></th></tr></thead><tbody><tr><td style="text-align:center">Mean Absolute Error (MAE)</td><td>平均绝对误差</td><td style="text-align:left">from sklearn.metrics import mean_absolute_error</td></tr><tr><td style="text-align:center">Mean Square Error(MSE)</td><td>平均方差</td><td style="text-align:left">from sklearn.metrics import mean_squared_error</td></tr><tr><td style="text-align:center">R-Squared</td><td>R平方值</td><td style="text-align:left">from sklearn.metrics import r2_score</td></tr></tbody></table></div><p>代码：</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="false" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#sklearn的调用</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_absolute_error</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error </span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> r2_score</span><br><span class="line"> </span><br><span class="line">mean_absolute_error(y_test,y_predict)</span><br><span class="line">mean_squared_error(y_test,y_predict)</span><br><span class="line">r2_score(y_test,y_predict)</span><br></pre></td></tr></table></figure></div><p>（一）<strong>平均绝对误差（Mean Absolute Error，MAE）</strong></p><p>平均绝对误差就是指预测值与真实值之间平均相差多大 ：</p><p>$MAE=\frac{1}{m}\sum_{i=1}^{m}|f_i-y_i|$</p><p>平均绝对误差能更好地反映预测值误差的实际情况.</p><p>（二）<strong>均方误差（Mean Squared Error，MSE）</strong></p><p>观测值与真值偏差的平方和与观测次数的比值：</p><p>$MSE=\frac{1}{m}\sum_{i=1}^{m}(f_i-y_i)^2$</p><p>这也是线性回归中最常用的损失函数，线性回归过程中尽量让该损失函数最小。那么模型之间的对比也可以用它来比较。MSE可以评价数据的变化程度，MSE的值越小，说明预测模型描述实验数据具有更好的精确度。</p><p>（三）<strong>R-square(决定系数)</strong></p><p>$R^2=1-\frac{\sum(Y_actual-Y_predict)^2}{\sum(Y_actual-Y_mean)^2}$</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="false" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#以线性回归为例</span></span><br><span class="line">lr = LinearRegression()</span><br><span class="line">lr.fit(x_train,y_train)</span><br><span class="line">y_ = lr.predict(X_test)</span><br><span class="line"><span class="comment"># R2 决定系数</span></span><br><span class="line">lr.score(X_test,y_test)   <span class="comment">#与用from sklearn.metrics import r2_score相同</span></span><br></pre></td></tr></table></figure></div><p>数学理解： 分母理解为原始数据的离散程度，分子为预测数据和原始数据的误差，二者相除可以消除原始数据离散程度的影响</p><p>其实“决定系数”是通过数据的变化来表征一个拟合的好坏。</p><p>理论上取值范围$ （-∞，1] $, 正常取值范围为[0 1] ———实际操作中通常会选择拟合较好的曲线计算R²，因此很少出现$-∞$</p><p>越接近1，表明方程的变量对y的解释能力越强，这个模型对数据拟合的也较好</p><p>越接近0，表明模型拟合的越差</p><p>经验值：&gt;0.4， 拟合效果好</p><p>缺点：数据集的样本越大，R²越大，因此，不同数据集的模型结果比较会有一定的误差</p><p>（四）<strong>Adjusted R-Square (校正决定系数）</strong></p><p>$R^2_adjusted=1-\frac{(1-R^2)(n-1)}{n-p-1}$</p><p>$n$为样本数量，$p$为特征数量</p><ul><li>消除了样本数量和特征数量的影响</li></ul><p>（五）<strong>交叉验证（Cross-Validation）</strong></p><p>交叉验证，有的时候也称作循环估计（Rotation Estimation），是一种统计学上将数据样本切割成较小子集的实用方法，该理论是由Seymour Geisser提出的。在给定的建模样本中，拿出大部分样本进行建模型，留小部分样本用刚建立的模型进行预报，并求这小部分样本的预报误差，记录它们的平方加和。这个过程一直进行，直到所有的样本都被预报了一次而且仅被预报一次。把每个样本的预报误差平方加和，称为PRESS(predicted Error Sum of Squares)。<br>交叉验证的基本思想是把在某种意义下将原始数据(dataset)进行分组,一部分做为训练集(train set)，另一部分做为验证集(validation set or test set)。首先用训练集对分类器进行训练，再利用验证集来测试训练得到的模型(model)，以此来做为评价分类器的性能指标。<br>无论分类还是回归模型，都可以利用交叉验证，进行模型评估，示例代码：</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="false" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.cross_validation <span class="keyword">import</span> cross_val_score</span><br><span class="line">print(cross_val_score(knn, X_train, y_train, cv=<span class="number">4</span>))</span><br><span class="line">print(cross_cal_score(lr, X, y, cv=<span class="number">2</span>))</span><br></pre></td></tr></table></figure></div><h3 id="模型保存和加载"><a href="#模型保存和加载" class="headerlink" title="模型保存和加载"></a>模型保存和加载</h3><p>当训练或者计算好一个模型之后，那么如果别人需要我们提供结果预测，就需要保存模型（主要是保存算法的参数）</p><h4 id="sklearn模型的保存和加载API"><a href="#sklearn模型的保存和加载API" class="headerlink" title="sklearn模型的保存和加载API"></a>sklearn模型的保存和加载API</h4><p>from sklearn.externals import joblib</p><ul><li><p>保存：joblib.dump(rf, ‘test.pkl’)</p></li><li><p>加载：estimator = joblib.load(‘test.pkl’)</p></li></ul><h4 id="线性回归的模型保存加载案例"><a href="#线性回归的模型保存加载案例" class="headerlink" title="线性回归的模型保存加载案例"></a>线性回归的模型保存加载案例</h4><p><strong>保存</strong></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="false" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用线性模型进行预测</span></span><br><span class="line"><span class="comment">#使用正规方程求解</span></span><br><span class="line">lr = LinearRegression()</span><br><span class="line"><span class="comment">#此时在干什么？</span></span><br><span class="line">lr.fit(x_train, y_train)</span><br><span class="line"><span class="comment">#保存训练完结束的模型</span></span><br><span class="line">joblib.dump(lr, <span class="string">"test.pkl"</span>)</span><br></pre></td></tr></table></figure></div><p><strong>加载</strong></p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="false" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用线性模型进行预测</span></span><br><span class="line"><span class="comment">#使用正规方程求解</span></span><br><span class="line"><span class="comment">#lr = LinearRegression()</span></span><br><span class="line"><span class="comment">#此时在干什么？</span></span><br><span class="line"><span class="comment">#lr.fit(x_train, y_train)</span></span><br><span class="line"><span class="comment">#保存训练完结束的模型</span></span><br><span class="line"><span class="comment">#joblib.dump(lr, "test.pkl")</span></span><br><span class="line"><span class="comment">#当上步模型已经保存好了就不需要训练fit和保存模型了</span></span><br><span class="line"><span class="comment">#通过已有的模型去预测房价</span></span><br><span class="line">model = joblib.load(<span class="string">"test.pkl"</span>)</span><br><span class="line">print(<span class="string">"从文件加载进来的模型预测房价的结果："</span>, std_y.inverse_transform(model.predict(x_test)))</span><br></pre></td></tr></table></figure></div><h2 id="无监督学习-K-means算法"><a href="#无监督学习-K-means算法" class="headerlink" title="无监督学习-K-means算法"></a>无监督学习-K-means算法</h2><h3 id="什么是无监督学习"><a href="#什么是无监督学习" class="headerlink" title="什么是无监督学习"></a>什么是无监督学习</h3><p>没有目标值—无监督学习</p><p><strong>例：</strong></p><ul><li><p>一家广告平台需要根据相似的人口学特征和购买习惯将美国人口分成不同的小组，以便广告客户可以通过有关联的广告接触到他们的目标客户。</p></li><li><p>Airbnb 需要将自己的房屋清单分组成不同的社区，以便用户能更轻松地查阅这些清单。</p></li><li>一个数据科学团队需要降低一个大型数据集的维度的数量，以便简化建模和降低文件大小。</li></ul><p>我们可以怎样最有用地对其进行归纳和分组？我们可以怎样以一种压缩格式有效地表征数据？这都是无监督学习的目标，之所以称之为无监督，是因为这是从无标签的数据开始学习的。</p><h3 id="无监督学习包含算法"><a href="#无监督学习包含算法" class="headerlink" title="无监督学习包含算法"></a>无监督学习包含算法</h3><ul><li>聚类<ul><li>K-means(K均值聚类)</li></ul></li><li>降维<ul><li>PCA</li></ul></li></ul><h3 id="K-means原理"><a href="#K-means原理" class="headerlink" title="K-means原理"></a>K-means原理</h3><p>我们先来看一下一个K-means的聚类效果图</p><p><img src="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/机器学习图库/K-means如何聚类效果.png" alt="K-means如何聚类效果"></p><h4 id="K-means聚类步骤"><a href="#K-means聚类步骤" class="headerlink" title="K-means聚类步骤"></a>K-means聚类步骤</h4><p>1、随机设置K个特征空间内的点作为初始的聚类中心</p><p>2、对于其他每个点计算到K个中心的距离，未知的点选择最近的一个聚类中心点作为标记类别</p><p>3、接着对着标记的聚类中心之后，重新计算出每个聚类的新中心点（平均值）</p><p>4、如果计算得出的新中心点与原中心点一样，那么结束，否则重新进行第二步过程</p><p>K—超参数 根据以下确定： 1）看需求；2）调节超参数</p><p>我们以一张图来解释效果<br><img src="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/机器学习图库/K-means过程分析.png" alt="K-means过程分析"></p><h3 id="K-means算法API"><a href="#K-means算法API" class="headerlink" title="K-means算法API"></a>K-means算法API</h3><p>sklearn.cluster.KMeans(n_clusters=8,init=‘k-means++’)</p><ul><li><p>k-means聚类</p></li><li><p>n_clusters:开始的聚类中心数量</p></li><li><p>init:初始化方法，默认为’k-means ++’</p></li><li><p>labels_:默认标记的类型，可以和真实值比较（不是值比较）</p></li></ul><h3 id="案例：k-means对Instacart-Market用户聚类"><a href="#案例：k-means对Instacart-Market用户聚类" class="headerlink" title="案例：k-means对Instacart Market用户聚类"></a>案例：k-means对Instacart Market用户聚类</h3><p><strong>分析</strong></p><ul><li>1、PCA降维数据</li><li>2、k-means聚类</li><li>3、聚类结果显示</li></ul><p>代码：</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="false" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="comment"># 1、获取数据</span></span><br><span class="line">order_products = pd.read_csv(<span class="string">"./instacart/order_products__prior.csv"</span>)</span><br><span class="line">products = pd.read_csv(<span class="string">"./instacart/products.csv"</span>)</span><br><span class="line">orders = pd.read_csv(<span class="string">"./instacart/orders.csv"</span>)</span><br><span class="line">aisles = pd.read_csv(<span class="string">"./instacart/aisles.csv"</span>)</span><br><span class="line"><span class="comment"># 2、合并表</span></span><br><span class="line"><span class="comment"># order_products__prior.csv：订单与商品信息</span></span><br><span class="line"><span class="comment"># 字段：order_id, product_id, add_to_cart_order, reordered</span></span><br><span class="line"><span class="comment"># products.csv：商品信息</span></span><br><span class="line"><span class="comment"># 字段：product_id, product_name, aisle_id, department_id</span></span><br><span class="line"><span class="comment"># orders.csv：用户的订单信息</span></span><br><span class="line"><span class="comment"># 字段：order_id,user_id,eval_set,order_number,….</span></span><br><span class="line"><span class="comment"># aisles.csv：商品所属具体物品类别</span></span><br><span class="line"><span class="comment"># 字段： aisle_id, aisle</span></span><br><span class="line"><span class="comment"># 合并aisles和products aisle和product_id</span></span><br><span class="line">tab1 = pd.merge(aisles, products, on=[<span class="string">"aisle_id"</span>, <span class="string">"aisle_id"</span>])</span><br><span class="line">tab2 = pd.merge(tab1, order_products, on=[<span class="string">"product_id"</span>, <span class="string">"product_id"</span>])</span><br><span class="line">tab3 = pd.merge(tab2, orders, on=[<span class="string">"order_id"</span>, <span class="string">"order_id"</span>])</span><br><span class="line"><span class="comment">#print(tab3.head())</span></span><br><span class="line"><span class="comment"># 3、找到user_id和aisle之间的关系</span></span><br><span class="line">table = pd.crosstab(tab3[<span class="string">"user_id"</span>], tab3[<span class="string">"aisle"</span>])</span><br><span class="line">data = table[:<span class="number">10000</span>]  <span class="comment">#缩小数据</span></span><br><span class="line"><span class="comment"># 4、PCA降维</span></span><br><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line"><span class="comment"># 1）实例化一个转换器类</span></span><br><span class="line">transfer = PCA(n_components=<span class="number">0.95</span>)</span><br><span class="line"><span class="comment"># 2）调用fit_transform</span></span><br><span class="line">data_new = transfer.fit_transform(data)</span><br><span class="line">print(data_new.shape)</span><br><span class="line"><span class="comment">#5、预估器流程</span></span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line">estimator=KMeans(n_clusters=<span class="number">3</span>)</span><br><span class="line"><span class="comment">#KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,</span></span><br><span class="line"><span class="comment">#       n_clusters=3, n_init=10, n_jobs=None, precompute_distances='auto',</span></span><br><span class="line"><span class="comment">#       random_state=None, tol=0.0001, verbose=0)</span></span><br><span class="line">estimator.fit(data_new)</span><br><span class="line">y_predict=estimator.predict(data_new)</span><br><span class="line">print(y_predict[:<span class="number">300</span>])</span><br></pre></td></tr></table></figure></div><p>输出结果：</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="false" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">(<span class="number">10000</span>, <span class="number">42</span>)</span><br><span class="line">[<span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span></span><br><span class="line"> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span></span><br><span class="line"> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span></span><br><span class="line"> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">2</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span></span><br><span class="line"> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span></span><br><span class="line"> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">2</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">2</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span></span><br><span class="line"> <span class="number">2</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">2</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span></span><br><span class="line"> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">2</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">2</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span></span><br><span class="line"> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span>]</span><br></pre></td></tr></table></figure></div><p>问题：如何去评估聚类的效果呢？</p><h3 id="Kmeans性能评估指标"><a href="#Kmeans性能评估指标" class="headerlink" title="Kmeans性能评估指标"></a>Kmeans性能评估指标</h3><h4 id="轮廓系数"><a href="#轮廓系数" class="headerlink" title="轮廓系数"></a>轮廓系数</h4><p>$SC_i=\frac{b_i-a_i}{max(b_i,a_i)}$</p><blockquote><p>注：对于每个点 $i$为已聚类数据中的样本 ，$b_i$ 为 $i$ 到其它族群的所有样本的距离最小值，$a_i $为 $i$ 到本身簇的距离平均值。最终计算出所有的样本点的轮廓系数平均值</p></blockquote><h4 id="轮廓系数值分析"><a href="#轮廓系数值分析" class="headerlink" title="轮廓系数值分析"></a>轮廓系数值分析</h4><p><img src="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/机器学习图库/轮廓系数分析.png" alt="img"></p><p>分析过程（我们以一个蓝1点为例）</p><p>1、计算出蓝1离本身族群所有点的距离的平均值a_i</p><p>2、蓝1到其它两个族群的距离计算出平均值红平均，绿平均，取最小的那个距离作为b_i</p><p>根据公式：极端值考虑：如果b_i &gt;&gt;a_i: 那么公式结果趋近于1；如果a_i&gt;&gt;&gt;b_i: 那么公式结果趋近于-1</p><h4 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h4><p>如果$b_i&gt;&gt;a_i$:趋近于1效果越好，$ b_i&lt;&lt;a_i$:趋近于-1，效果不好。轮廓系数的值是介于 [-1,1] ，越趋近于1代表内聚度和分离度都相对较优。</p><h4 id="轮廓系数API"><a href="#轮廓系数API" class="headerlink" title="轮廓系数API"></a>轮廓系数API</h4><p>sklearn.metrics.silhouette_score(X, labels)</p><ul><li>计算所有样本的平均轮廓系数</li><li>X：特征值</li><li>labels：被聚类标记的目标值</li></ul><h4 id="用户聚类结果评估"><a href="#用户聚类结果评估" class="headerlink" title="用户聚类结果评估"></a>用户聚类结果评估</h4><p>代码接上文代码：</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="false" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#6、模型评估—轮廓系数</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> silhouette_score</span><br><span class="line">print(silhouette_score(data_new,y_predict))</span><br></pre></td></tr></table></figure></div><p>输出结果：</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="false" data-rel="ANGELSCRIPT"><figure class="iseeu highlight /angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">0.5396819903993842</span></span><br></pre></td></tr></table></figure></div><h3 id="K-means总结"><a href="#K-means总结" class="headerlink" title="K-means总结"></a>K-means总结</h3><ul><li>特点分析：采用迭代式算法，直观易懂并且非常实用</li><li>缺点：容易收敛到局部最优解(多次聚类)</li></ul><p>注：聚类一般做在分类之前</p><h2 id="SVM算法"><a href="#SVM算法" class="headerlink" title="SVM算法"></a>SVM算法</h2><p>代码：</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="false" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#SVC算法进行癌症预测</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="comment"># 1、读取数据</span></span><br><span class="line">column_name = [<span class="string">'Sample code number'</span>, <span class="string">'Clump Thickness'</span>, <span class="string">'Uniformity of Cell Size'</span>, <span class="string">'Uniformity of Cell Shape'</span>,</span><br><span class="line">                   <span class="string">'Marginal Adhesion'</span>, <span class="string">'Single Epithelial Cell Size'</span>, <span class="string">'Bare Nuclei'</span>, <span class="string">'Bland Chromatin'</span>,</span><br><span class="line">                   <span class="string">'Normal Nucleoli'</span>, <span class="string">'Mitoses'</span>, <span class="string">'Class'</span>]</span><br><span class="line">data = pd.read_csv(<span class="string">"https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data"</span>,</span><br><span class="line">                       names=column_name)</span><br><span class="line"><span class="comment"># 2、数据处理—处理缺失值</span></span><br><span class="line">data = data.replace(to_replace=<span class="string">'?'</span>, value=np.nan)   <span class="comment">#1)替换np.nan</span></span><br><span class="line">data = data.dropna()    <span class="comment">#2)删除缺失值</span></span><br><span class="line">print(data.isnull().any()) <span class="comment">#确认不存在缺失值</span></span><br><span class="line"><span class="comment"># 取出特征值</span></span><br><span class="line">x = data[column_name[<span class="number">1</span>:<span class="number">10</span>]]  <span class="comment">#x=data.iloc[:,1:-1]</span></span><br><span class="line">y = data[column_name[<span class="number">10</span>]]   <span class="comment">#y=data['Class']</span></span><br><span class="line"><span class="comment">#3、分割数据集</span></span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=<span class="number">0.3</span>)</span><br><span class="line"><span class="comment">#4、特征工程—标准化</span></span><br><span class="line">std = StandardScaler()</span><br><span class="line">x_train = std.fit_transform(x_train)</span><br><span class="line">x_test = std.transform(x_test)</span><br><span class="line"><span class="comment"># 5、使用SVC算法</span></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm                                <span class="comment">#支持向量机#</span></span><br><span class="line">module = svm.SVC()</span><br><span class="line">module.fit(x_train, y_train)</span><br><span class="line"><span class="comment">#6、模型评估</span></span><br><span class="line"><span class="comment">#方法1：直接比对真实值和预测值</span></span><br><span class="line">y_predict=module.predict(x_test)</span><br><span class="line">print(<span class="string">'y_predict:\n'</span>,y_predict)</span><br><span class="line">print(<span class="string">'直接比对真实值和预测值：\n'</span>,y_test  y_predict)</span><br><span class="line"><span class="comment">#方法2：计算准确率</span></span><br><span class="line">score=module.score(x_test,y_test)</span><br><span class="line">print(<span class="string">'准确率为：\n'</span>,score)</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line">report=classification_report(y_test, module.predict(x_test), labels=[<span class="number">2</span>, <span class="number">4</span>], target_names=[<span class="string">'良性'</span>, <span class="string">'恶性'</span>])</span><br><span class="line">print(<span class="string">"精确率和召回率为："</span>,report)</span><br><span class="line">print(y_test.head())</span><br><span class="line"><span class="comment">#y_true:每个样本的真实类别，必须为0（反例），1（正例）标记</span></span><br><span class="line"><span class="comment">#将y_test 转换成0和1</span></span><br><span class="line">y_test = np.where(y_test &gt; <span class="number">2.5</span>, <span class="number">1</span>, <span class="number">0</span>) <span class="comment">#y_test数值大于2.5设置为1，不大于设置为0</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_auc_score</span><br><span class="line">print(<span class="string">"AUC指标："</span>, roc_auc_score(y_test, module.predict(x_test)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># ROC曲线</span></span><br><span class="line"><span class="comment"># y_score为模型预测正例的概率</span></span><br><span class="line"><span class="comment">###通过decision_function()计算得到的y_score的值，用在roc_curve()函数中</span></span><br><span class="line">y_score = module.fit(x_train, y_train).decision_function(x_test)</span><br><span class="line"><span class="comment"># 计算不同阈值下，fpr和tpr的组合之，fpr表示1-Specificity，tpr表示Sensitivity</span></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line">fpr, tpr, threshold = metrics.roc_curve(y_test, y_score)</span><br><span class="line"><span class="comment"># 计算AUC</span></span><br><span class="line">roc_auc = metrics.auc(fpr, tpr)</span><br><span class="line"><span class="comment"># 绘制面积图</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.stackplot(fpr, tpr, color=<span class="string">'steelblue'</span>, alpha=<span class="number">0.5</span>, edgecolor=<span class="string">'black'</span>)</span><br><span class="line"><span class="comment"># 添加ROC曲线的轮廓</span></span><br><span class="line">plt.plot(fpr, tpr, color=<span class="string">'black'</span>, lw=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 添加对角线作为参考线</span></span><br><span class="line">plt.plot([<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>], color=<span class="string">'red'</span>, linestyle=<span class="string">'--'</span>)</span><br><span class="line">plt.text(<span class="number">0.5</span>, <span class="number">0.3</span>, <span class="string">'ROC curve (area=%0.2f)'</span> % roc_auc)</span><br><span class="line">plt.xlabel(<span class="string">'1-Specificity'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Sensitivity'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></div><p>输出结果：</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="false" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">Sample code number             <span class="literal">False</span></span><br><span class="line">Clump Thickness                <span class="literal">False</span></span><br><span class="line">Uniformity of Cell Size        <span class="literal">False</span></span><br><span class="line">Uniformity of Cell Shape       <span class="literal">False</span></span><br><span class="line">Marginal Adhesion              <span class="literal">False</span></span><br><span class="line">Single Epithelial Cell Size    <span class="literal">False</span></span><br><span class="line">Bare Nuclei                    <span class="literal">False</span></span><br><span class="line">Bland Chromatin                <span class="literal">False</span></span><br><span class="line">Normal Nucleoli                <span class="literal">False</span></span><br><span class="line">Mitoses                        <span class="literal">False</span></span><br><span class="line">Class                          <span class="literal">False</span></span><br><span class="line">dtype: bool</span><br><span class="line">y_predict:</span><br><span class="line"> [<span class="number">2</span> <span class="number">4</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">4</span> <span class="number">2</span> <span class="number">2</span> <span class="number">4</span> <span class="number">4</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">4</span> <span class="number">4</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">4</span> <span class="number">4</span> <span class="number">4</span> <span class="number">2</span> <span class="number">4</span> <span class="number">4</span> <span class="number">4</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span></span><br><span class="line"> <span class="number">4</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">4</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">4</span> <span class="number">2</span> <span class="number">4</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">4</span> <span class="number">4</span> <span class="number">4</span> <span class="number">2</span> <span class="number">4</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">4</span> <span class="number">2</span> <span class="number">4</span> <span class="number">2</span> <span class="number">2</span> <span class="number">4</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">4</span> <span class="number">2</span></span><br><span class="line"> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">4</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">4</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">4</span> <span class="number">2</span> <span class="number">4</span> <span class="number">2</span> <span class="number">2</span> <span class="number">4</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">4</span> <span class="number">4</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">4</span> <span class="number">4</span> <span class="number">2</span> <span class="number">4</span></span><br><span class="line"> <span class="number">2</span> <span class="number">4</span> <span class="number">2</span> <span class="number">4</span> <span class="number">4</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">4</span> <span class="number">4</span> <span class="number">4</span> <span class="number">2</span> <span class="number">4</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">4</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">4</span> <span class="number">2</span> <span class="number">4</span> <span class="number">4</span> <span class="number">2</span> <span class="number">4</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">4</span> <span class="number">2</span></span><br><span class="line"> <span class="number">4</span> <span class="number">4</span> <span class="number">2</span> <span class="number">4</span> <span class="number">4</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">4</span> <span class="number">4</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">4</span> <span class="number">2</span> <span class="number">4</span> <span class="number">2</span> <span class="number">2</span> <span class="number">4</span> <span class="number">2</span> <span class="number">2</span> <span class="number">4</span> <span class="number">4</span> <span class="number">2</span> <span class="number">2</span> <span class="number">4</span> <span class="number">2</span> <span class="number">2</span> <span class="number">4</span> <span class="number">4</span> <span class="number">2</span> <span class="number">4</span></span><br><span class="line"> <span class="number">4</span> <span class="number">2</span> <span class="number">4</span> <span class="number">4</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">4</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">4</span> <span class="number">2</span> <span class="number">4</span> <span class="number">2</span> <span class="number">4</span> <span class="number">4</span> <span class="number">4</span> <span class="number">2</span>]</span><br><span class="line">直接比对真实值和预测值：</span><br><span class="line"> <span class="number">492</span>     <span class="literal">True</span></span><br><span class="line"><span class="number">236</span>     <span class="literal">True</span></span><br><span class="line"><span class="number">208</span>     <span class="literal">True</span></span><br><span class="line"><span class="number">518</span>     <span class="literal">True</span></span><br><span class="line"><span class="number">81</span>      <span class="literal">True</span></span><br><span class="line">       ...  </span><br><span class="line"><span class="number">653</span>     <span class="literal">True</span></span><br><span class="line"><span class="number">434</span>    <span class="literal">False</span></span><br><span class="line"><span class="number">196</span>    <span class="literal">False</span></span><br><span class="line"><span class="number">210</span>     <span class="literal">True</span></span><br><span class="line"><span class="number">137</span>     <span class="literal">True</span></span><br><span class="line">Name: Class, Length: <span class="number">205</span>, dtype: bool</span><br><span class="line">准确率为：</span><br><span class="line"> <span class="number">0.9658536585365853</span></span><br><span class="line">精确率和召回率为：               precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">          良性       <span class="number">0.99</span>      <span class="number">0.96</span>      <span class="number">0.97</span>       <span class="number">139</span></span><br><span class="line">          恶性       <span class="number">0.92</span>      <span class="number">0.98</span>      <span class="number">0.95</span>        <span class="number">66</span></span><br><span class="line"></span><br><span class="line">    accuracy                           <span class="number">0.97</span>       <span class="number">205</span></span><br><span class="line">   macro avg       <span class="number">0.95</span>      <span class="number">0.97</span>      <span class="number">0.96</span>       <span class="number">205</span></span><br><span class="line">weighted avg       <span class="number">0.97</span>      <span class="number">0.97</span>      <span class="number">0.97</span>       <span class="number">205</span></span><br><span class="line"></span><br><span class="line"><span class="number">492</span>    <span class="number">2</span></span><br><span class="line"><span class="number">236</span>    <span class="number">4</span></span><br><span class="line"><span class="number">208</span>    <span class="number">2</span></span><br><span class="line"><span class="number">518</span>    <span class="number">2</span></span><br><span class="line"><span class="number">81</span>     <span class="number">2</span></span><br><span class="line">Name: Class, dtype: int64</span><br><span class="line">AUC指标： <span class="number">0.9708415086112929</span></span><br></pre></td></tr></table></figure></div><p><img src="https://gitee.com/sky_mirrors_the_clouds/cloudimg/raw/master/https://gitee.com/sky_mirrors_the_clouds/cloudimg/机器学习图库/Figure_15.png" alt="Figure_15"></p><h2 id="GBDT算法"><a href="#GBDT算法" class="headerlink" title="GBDT算法"></a>GBDT算法</h2><p>代码：</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="false" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#GBDT进行癌症预测</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="comment"># 1、读取数据</span></span><br><span class="line">column_name = [<span class="string">'Sample code number'</span>, <span class="string">'Clump Thickness'</span>, <span class="string">'Uniformity of Cell Size'</span>, <span class="string">'Uniformity of Cell Shape'</span>,</span><br><span class="line">                   <span class="string">'Marginal Adhesion'</span>, <span class="string">'Single Epithelial Cell Size'</span>, <span class="string">'Bare Nuclei'</span>, <span class="string">'Bland Chromatin'</span>,</span><br><span class="line">                   <span class="string">'Normal Nucleoli'</span>, <span class="string">'Mitoses'</span>, <span class="string">'Class'</span>]</span><br><span class="line">data = pd.read_csv(<span class="string">"https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data"</span>,</span><br><span class="line">                       names=column_name)</span><br><span class="line"><span class="comment"># 2、数据处理—处理缺失值</span></span><br><span class="line">data = data.replace(to_replace=<span class="string">'?'</span>, value=np.nan)   <span class="comment">#1)替换np.nan</span></span><br><span class="line">data = data.dropna()    <span class="comment">#2)删除缺失值</span></span><br><span class="line">print(data.isnull().any()) <span class="comment">#确认不存在缺失值</span></span><br><span class="line"><span class="comment"># 取出特征值</span></span><br><span class="line">x = data[column_name[<span class="number">1</span>:<span class="number">10</span>]]  <span class="comment">#x=data.iloc[:,1:-1]</span></span><br><span class="line">y = data[column_name[<span class="number">10</span>]]   <span class="comment">#y=data['Class']</span></span><br><span class="line"><span class="comment">#3、分割数据集</span></span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=<span class="number">0.3</span>)</span><br><span class="line"><span class="comment">#4、特征工程—标准化</span></span><br><span class="line">std = StandardScaler()</span><br><span class="line">x_train = std.fit_transform(x_train)</span><br><span class="line">x_test = std.transform(x_test)</span><br><span class="line"><span class="comment"># 5、使用GBDT</span></span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> GradientBoostingClassifier      <span class="comment">#Gradient Boosting 和 AdaBoost算法#</span></span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> GradientBoostingRegressor</span><br><span class="line">module = GradientBoostingClassifier(n_estimators=<span class="number">100</span>, learning_rate=<span class="number">0.1</span>, max_depth=<span class="number">1</span>, random_state=<span class="number">0</span>)</span><br><span class="line">module.fit(x_train, y_train)</span><br><span class="line"><span class="comment">#6、模型评估</span></span><br><span class="line"><span class="comment">#方法1：直接比对真实值和预测值</span></span><br><span class="line">y_predict=module.predict(x_test)</span><br><span class="line">print(<span class="string">'y_predict:\n'</span>,y_predict)</span><br><span class="line">print(<span class="string">'直接比对真实值和预测值：\n'</span>,y_test  y_predict)</span><br><span class="line"><span class="comment">#方法2：计算准确率</span></span><br><span class="line">score=module.score(x_test,y_test)</span><br><span class="line">print(<span class="string">'准确率为：\n'</span>,score)</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line">report=classification_report(y_test, module.predict(x_test), labels=[<span class="number">2</span>, <span class="number">4</span>], target_names=[<span class="string">'良性'</span>, <span class="string">'恶性'</span>])</span><br><span class="line">print(<span class="string">"精确率和召回率为："</span>,report)</span><br><span class="line">print(y_test.head())</span><br><span class="line"><span class="comment">#y_true:每个样本的真实类别，必须为0（反例），1（正例）标记</span></span><br><span class="line"><span class="comment">#将y_test 转换成0和1</span></span><br><span class="line">y_test = np.where(y_test &gt; <span class="number">2.5</span>, <span class="number">1</span>, <span class="number">0</span>) <span class="comment">#y_test数值大于2.5设置为1，不大于设置为0</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_auc_score</span><br><span class="line">print(<span class="string">"AUC指标："</span>, roc_auc_score(y_test, module.predict(x_test)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># ROC曲线</span></span><br><span class="line"><span class="comment"># y_score为模型预测正例的概率</span></span><br><span class="line">y_score = module.predict_proba(x_test)[:, <span class="number">1</span>]</span><br><span class="line"><span class="comment"># 计算不同阈值下，fpr和tpr的组合之，fpr表示1-Specificity，tpr表示Sensitivity</span></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line">fpr, tpr, threshold = metrics.roc_curve(y_test, y_score)</span><br><span class="line"><span class="comment"># 计算AUC</span></span><br><span class="line">roc_auc = metrics.auc(fpr, tpr)</span><br><span class="line"><span class="comment"># 绘制面积图</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.stackplot(fpr, tpr, color=<span class="string">'steelblue'</span>, alpha=<span class="number">0.5</span>, edgecolor=<span class="string">'black'</span>)</span><br><span class="line"><span class="comment"># 添加ROC曲线的轮廓</span></span><br><span class="line">plt.plot(fpr, tpr, color=<span class="string">'black'</span>, lw=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 添加对角线作为参考线</span></span><br><span class="line">plt.plot([<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>], color=<span class="string">'red'</span>, linestyle=<span class="string">'--'</span>)</span><br><span class="line">plt.text(<span class="number">0.5</span>, <span class="number">0.3</span>, <span class="string">'ROC curve (area=%0.2f)'</span> % roc_auc)</span><br><span class="line">plt.xlabel(<span class="string">'1-Specificity'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Sensitivity'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></div></div><div><div class="my_post_copyright"><script src="//cdn.bootcss.com/clipboard.js/1.5.10/clipboard.min.js"></script><script type="text/javascript" src="https://cdn.bootcss.com/jquery/2.0.0/jquery.min.js"></script><script src="https://unpkg.com/sweetalert/dist/sweetalert.min.js"></script><p><span>本文标题:</span>机器学习课程</p><p><span>文章作者:</span>TJYS</p><p><span>发布时间:</span>2020年03月28日 - 00:00:00</p><p><span>最后更新:</span>2021年01月20日 - 18:56:02</p><p><span>原始链接:</span><a href="/Machine-learning-note.html" title="机器学习课程">https://qikaile.us/Machine-learning-note.html</a> <span class="copy-path" title="点击复制文章链接"><i class="fa fa-clipboard" data-clipboard-text="https://qikaile.us/Machine-learning-note.html" aria-label="复制成功！"></i></span></p><p><span>许可协议:</span><i class="fa fa-creative-commons"></i> <a rel="license external nofollow noreferrer" href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank" title="Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)">署名-非商业性使用-禁止演绎 4.0 国际</a> 转载请保留原文链接及作者。</p></div><script>var clipboard=new Clipboard(".fa-clipboard");$(".fa-clipboard").click(function(){clipboard.on("success",function(){swal({title:"",text:"复制成功",icon:"success",showConfirmButton:!0})})})</script></div><div><div><div class="read-over">-------------------本文结束 <i class="fa fa-paw"></i> 感谢您的阅读-------------------</div></div></div><div><div class="share_reward"><div>坚持原创技术分享，感谢您的支持和鼓励！</div><button id="rewardButton" disable="enable" onclick='var e=document.getElementById("QR");"none"===e.style.display?e.style.display="block":e.style.display="none"'><span>打赏</span></button><div id="QR" style="display:none"><div id="wechat" style="display:inline-block"><img id="wechat_qr" src="https://cdn.jsdelivr.net/gh/Qikaile/cdn/img/wechatpay.png" alt="TJYS 微信支付"><p>微信支付</p></div><div id="alipay" style="display:inline-block"><img id="alipay_qr" src="https://cdn.jsdelivr.net/gh/Qikaile/cdn/img/alipay.png" alt="TJYS 支付宝"><p>支付宝</p></div></div></div></div><footer class="post-footer"><div class="post-tags"><a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"><i class="fa fa-tag"></i> 机器学习</a></div><div class="post-widgets"><div class="wp_rating"><div style="color:rgba(0,0,0,.75);font-size:13px;letter-spacing:4px;margin-bottom:5px">(&gt;看完记得五星好评哦亲&lt;)</div><div id="wpac-rating"></div></div></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/hexo-theme-beautify.html" rel="next" title="Hexo搭建个人博客系列：主题美化篇"><i class="fa fa-chevron-left"></i> Hexo搭建个人博客系列：主题美化篇</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"><a href="/pandas-manual.html" rel="prev" title="Pandas中文手册">Pandas中文手册 <i class="fa fa-chevron-right"></i></a></div></div></footer></div></article><div class="post-spread"><div data-weibo-title="分享到微博" data-qq-title="分享到QQ" data-douban-title="分享到豆瓣" class="social-share" class="share-component" data-disabled="qzone,google+,linkedin" data-description="Share.js - 一键分享到微博，QQ空间，腾讯微博，人人，豆瓣...">分享到：</div></div></div></div><div class="comments" id="comments"></div><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><ins class="adsbygoogle" style="display:block;width:90%;margin:0 auto;opacity:80%" data-ad-client="ca-pub-7008389072962799" data-ad-slot="3109786475" data-ad-format="auto" data-full-width-responsive="true"></ins><script>(adsbygoogle=window.adsbygoogle||[]).push({})</script></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div id="sidebar-dimmer"></div><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">文章目录</li><li class="sidebar-nav-overview" data-target="site-overview-wrap">站点概览</li></ul><section class="site-overview-wrap sidebar-panel"><div class="site-overview"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><a href="/about"><img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="TJYS"></a><p class="site-author-name" itemprop="name">TJYS <a title="官方认证" href="javascript:void(0);" rel="external nofollow noreferrer"><style type="text/css">.fa_custom{color:#E94C3D}</style><i class="fa fa-vimeo-square fa_custom fa-1x"></i></a></p><p class="site-description motion-element" itemprop="description"></p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives/%7C%7Carchive"><a href="/archives"><span class="site-state-item-count">25</span> <span class="site-state-item-name">文章</span></a></a></div><div class="site-state-item site-state-categories"><a href="/categories/index.html"><span class="site-state-item-count">12</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/index.html"><span class="site-state-item-count">18</span> <span class="site-state-item-name">标签</span></a></div></nav><div class="feed-link motion-element"><a href="/atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS </a><a title="收藏到书签，偶尔High一下^_^" rel="alternate external nofollow noreferrer" class="mw-harlem_shake_slow wobble shake" href="javascript:void(0)" onclick="(    /*     * Copyright (C) 2015 Rocko (rocko.xyz) <rocko.zxp@gmail.com>     *     * Licensed under the Apache License, Version 2.0 (the 'License');     * you may not use this file except in compliance with the License.     * You may obtain a copy of the License at     *     *      http://www.apache.org/licenses/LICENSE-2.0     *     * Unless required by applicable law or agreed to in writing, software     * distributed under the License is distributed on an 'AS IS' BASIS,     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.     * See the License for the specific language governing permissions and     * limitations under the License.     */    function go() {        function c() {        var e = document.createElement('link');        e.setAttribute('type', 'text/css');        e.setAttribute('rel', 'stylesheet');        e.setAttribute('href', f);        e.setAttribute('class', l);        document.body.appendChild(e)    }     function h() {        var e = document.getElementsByClassName(l);        for (var t = 0; t < e.length; t++) {            document.body.removeChild(e[t])        }    }     function p() {        var e = document.createElement('div');        e.setAttribute('class', a);        document.body.appendChild(e);        setTimeout(function() {            document.body.removeChild(e)        }, 100)    }     function d(e) {        return {            height : e.offsetHeight,            width : e.offsetWidth        }    }     function v(i) {        var s = d(i);        return s.height > e &amp;&amp; s.height < n &amp;&amp; s.width > t &amp;&amp; s.width < r    }     function m(e) {        var t = e;        var n = 0;        while (!!t) {            n += t.offsetTop;            t = t.offsetParent        }        return n    }     function g() {        var e = document.documentElement;        if (!!window.innerWidth) {            return window.innerHeight        } else if (e &amp;&amp; !isNaN(e.clientHeight)) {            return e.clientHeight        }        return 0    }     function y() {        if (window.pageYOffset) {            return window.pageYOffset        }        return Math.max(document.documentElement.scrollTop, document.body.scrollTop)    }     function E(e) {        var t = m(e);        return t >= w &amp;&amp; t <= b + w    }     var songs = [                'http://s3.amazonaws.com/moovweb-marketing/playground/harlem-shake.mp3', 'http://www.ytmp3.cn/down/57563.mp3'  ];    function S() {        var e = document.getElementById('audio_element_id');        if(e != null){            var index = parseInt(e.getAttribute('curSongIndex'));            if(index > songs.length - 2) {                index = 0;            } else {                index++;            }            e.setAttribute('curSongIndex', index);            N();        }        e.src = i;        e.play()    }     function x(e) {        e.className += ' ' + s + ' ' + o    }     function T(e) {        e.className += ' ' + s + ' ' + u[Math.floor(Math.random() * u.length)]    }     function N() {        var e = document.getElementsByClassName(s);        var t = new RegExp('\\b' + s + '\\b');        for (var n = 0; n < e.length; ) {            e[n].className = e[n].className.replace(t, '')        }    }    function initAudioEle() {        var e = document.getElementById('audio_element_id');        if(e === null){            e = document.createElement('audio');            e.setAttribute('class', l);            e.setAttribute('curSongIndex', 0);            e.id = 'audio_element_id';            e.loop = false;            e.bgcolor = 0;            e.addEventListener('canplay', function() {            setTimeout(function() {                x(k)            }, 500);            setTimeout(function() {                N();                p();                for (var e = 0; e < O.length; e++) {                    T(O[e])                }            }, 15500)        }, true);        e.addEventListener('ended', function() {            N();            h();            go();        }, true);        e.innerHTML = ' <p>If you are reading this, it is because your browser does not support the audio element. We recommend that you get a new browser.</p> <p>';        document.body.appendChild(e);        }    }        initAudioEle();    var e = 30;    var t = 30;    var n = 350;    var r = 350;    var curSongIndex = parseInt(document.getElementById('audio_element_id').getAttribute('curSongIndex'));    var i = songs[curSongIndex];        var s = 'mw-harlem_shake_me';    var o = 'im_first';    var u = ['im_drunk', 'im_baked', 'im_trippin', 'im_blown'];    var a = 'mw-strobe_light';    var f = 'https://rocko.xyz/css/harlem-shake-style.css';        var l = 'mw_added_css';    var b = g();    var w = y();    var C = document.getElementsByTagName('*');    var k = null;    for (var L = 0; L < C.length; L++) {        var A = C[L];        if (v(A)) {            if (E(A)) {                k = A;                break            }        }    }    if (A === null) {        console.warn('Could not find a node of the right size. Please try a different page.');        return    }    c();    S();    var O = [];    for (var L = 0; L < C.length; L++) {        var A = C[L];        if (v(A)) {            O.push(A)        }    }    })()"><i class="fa fa-music"></i> High~</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a rel="noopener noreferrer" href="https://github.com/Qikaile" target="_blank" title="GitHub"><i class="fa fa-fw fa-github"></i></a> </span><span class="links-of-author-item"><a rel="noopener noreferrer" href="mailto:admin@qikaile.tk" target="_blank" title="E-Mail"><i class="fa fa-fw fa-envelope"></i></a> </span><span class="links-of-author-item"><a rel="noopener noreferrer" href="https://user.qzone.qq.com/2649019047" target="_blank" title="QQ"><i class="fa fa-fw fa-qq"></i></a> </span><span class="links-of-author-item"><a rel="noopener noreferrer" href="https://twitter.com/qikaile" target="_blank" title="Twitter"><i class="fa fa-fw fa-twitter"></i></a> </span><span class="links-of-author-item"><a rel="noopener noreferrer" href="https://www.facebook.com/qikaile" target="_blank" title="facebook"><i class="fa fa-fw fa-facebook"></i></a> </span><span class="links-of-author-item"><a rel="noopener noreferrer" href="https://youtube.com/channel/UCCY24D6Az4xT2XUHpqjHMpg" target="_blank" title="YouTube"><i class="fa fa-fw fa-youtube"></i></a></span></div><div id="music163player"><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="280" height="86" src="//music.163.com/outchain/player?type=2&id=28802028&auto=0&height=66"></iframe></div><div class="cc-license motion-element" itemprop="license"><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener noreferrer" class="cc-opacity" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a></div><marquee><a href="/links"><font size="4" face="STXingkai"><font color="#D1002E">点</font> <font color="#BA0045">我</font> <font color="#FF0000">欢</font> <font color="#E80017">迎</font> <font color="#A3005C">互</font> <font color="#A00073">换</font> <font color="#A0008A">友</font> <font color="#A000A1">链</font> <font color="#A000B8">哦</font> <font color="#A000CF">~</font> <font color="#A000E6">~</font></font></a></marquee><div class="links-of-blogroll motion-element links-of-blogroll-block"><div class="links-of-blogroll-title"><i class="fa fa-history fa-" aria-hidden="true"></i> 近期文章</div><ul class="links-of-blogroll-list"><li class="my-links-of-blogroll-li"><a href="/15-useful-functions-of-Github.html" title="你可能不知道的15个有用的Github功能" target="_blank">你可能不知道的15个有用的Github功能</a></li><li class="my-links-of-blogroll-li"><a href="/onedrive-cf-index-tutorial.html" title="onedrive-cf-index 搭建教程" target="_blank">onedrive-cf-index 搭建教程</a></li><li class="my-links-of-blogroll-li"><a href="/uptime-status.html" title="uptime-status站点状态监控" target="_blank">uptime-status站点状态监控</a></li><li class="my-links-of-blogroll-li"><a href="/github-fork-update.html" title="Github如何更新Fork的仓库" target="_blank">Github如何更新Fork的仓库</a></li><li class="my-links-of-blogroll-li"><a href="/hexo-next-Tag-Plugin.html" title="Hexo-NexT Tag 插件的使用" target="_blank">Hexo-NexT Tag 插件的使用</a></li></ul></div><div><canvas id="canvas" style="width:60%"></canvas></div><script>!function(){function t(t){var r=[];a.fillStyle="#005eac";var h=new Date,u=70,s=30,v=h.getHours(),g=Math.floor(v/10),m=v%10;r.push({num:g}),r.push({num:m}),r.push({num:10});var c=h.getMinutes(),g=Math.floor(c/10),m=c%10;r.push({num:g}),r.push({num:m}),r.push({num:10});var M=h.getSeconds(),g=Math.floor(M/10),m=M%10;r.push({num:g}),r.push({num:m});for(var p=0;p<r.length;p++)r[p].offsetX=u,u=f(u,s,r[p].num,t),p<r.length-1&&10!=r[p].num&&10!=r[p+1].num&&(u+=l);if(0==i.length)i=r;else for(var C=0;C<i.length;C++)i[C].num!=r[C].num&&(n(r[C]),i[C].num=r[C].num);return e(t),o(),h}function n(t){for(var n=t.num,e=m[n],o=0;o<e.length;o++)for(var f=0;f<e[o].length;f++)if(1==e[o][f]){var a={offsetX:t.offsetX+u+2*u*f,offsetY:30+u+2*u*o,color:g[Math.floor(Math.random()*g.length)],g:1.5+Math.random(),vx:4*Math.pow(-1,Math.ceil(10*Math.random()))+Math.random(),vy:-5};v.push(a)}}function e(t){for(var n=0;n<v.length;n++)t.beginPath(),t.fillStyle=v[n].color,t.arc(v[n].offsetX,v[n].offsetY,u,0,2*Math.PI),t.fill()}function o(){for(var t=0,n=0;n<v.length;n++){var e=v[n];e.offsetX+=e.vx,e.offsetY+=e.vy,e.vy+=e.g,e.offsetY>h-u&&(e.offsetY=h-u,e.vy=-e.vy*s),e.offsetX>u&&e.offsetX<r-u&&(v[t]=v[n],t++)}for(;t<v.length;t++)v.pop()}function f(t,n,e,o){for(var f=m[e],a=0;a<f.length;a++)for(var r=0;r<f[a].length;r++)1==f[a][r]&&(o.beginPath(),o.arc(t+u+2*u*r,n+u+2*u*a,u,0,2*Math.PI),o.fill());return o.beginPath(),t+=f[0].length*u*2}var a,r=820,h=250,u=7,l=10,s=.65,v=[];const g=["#33B5E5","#0099CC","#AA66CC","#9933CC","#99CC00","#669900","#FFBB33","#FF8800","#FF4444","#CC0000"];var i=[],m=[[[0,0,1,1,1,0,0],[0,1,1,0,1,1,0],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,0,1,1,0],[0,0,1,1,1,0,0]],[[0,0,0,1,1,0,0],[0,1,1,1,1,0,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[1,1,1,1,1,1,1]],[[0,1,1,1,1,1,0],[1,1,0,0,0,1,1],[0,0,0,0,0,1,1],[0,0,0,0,1,1,0],[0,0,0,1,1,0,0],[0,0,1,1,0,0,0],[0,1,1,0,0,0,0],[1,1,0,0,0,0,0],[1,1,0,0,0,1,1],[1,1,1,1,1,1,1]],[[1,1,1,1,1,1,1],[0,0,0,0,0,1,1],[0,0,0,0,1,1,0],[0,0,0,1,1,0,0],[0,0,1,1,1,0,0],[0,0,0,0,1,1,0],[0,0,0,0,0,1,1],[0,0,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,1,1,1,0]],[[0,0,0,0,1,1,0],[0,0,0,1,1,1,0],[0,0,1,1,1,1,0],[0,1,1,0,1,1,0],[1,1,0,0,1,1,0],[1,1,1,1,1,1,1],[0,0,0,0,1,1,0],[0,0,0,0,1,1,0],[0,0,0,0,1,1,0],[0,0,0,1,1,1,1]],[[1,1,1,1,1,1,1],[1,1,0,0,0,0,0],[1,1,0,0,0,0,0],[1,1,1,1,1,1,0],[0,0,0,0,0,1,1],[0,0,0,0,0,1,1],[0,0,0,0,0,1,1],[0,0,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,1,1,1,0]],[[0,0,0,0,1,1,0],[0,0,1,1,0,0,0],[0,1,1,0,0,0,0],[1,1,0,0,0,0,0],[1,1,0,1,1,1,0],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,1,1,1,0]],[[1,1,1,1,1,1,1],[1,1,0,0,0,1,1],[0,0,0,0,1,1,0],[0,0,0,0,1,1,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[0,0,1,1,0,0,0],[0,0,1,1,0,0,0],[0,0,1,1,0,0,0],[0,0,1,1,0,0,0]],[[0,1,1,1,1,1,0],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,1,1,1,0],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,1,1,1,0]],[[0,1,1,1,1,1,0],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,1,0,1,1],[0,0,0,0,0,1,1],[0,0,0,0,0,1,1],[0,0,0,0,1,1,0],[0,0,0,1,1,0,0],[0,1,1,0,0,0,0]],[[0,0,0,0],[0,0,0,0],[0,1,1,0],[0,1,1,0],[0,0,0,0],[0,0,0,0],[0,1,1,0],[0,1,1,0],[0,0,0,0],[0,0,0,0]]],c=document.getElementById("canvas");c.width=r,c.height=h,a=c.getContext("2d");new Date;setInterval(function(){a.clearRect(0,0,a.canvas.width,a.canvas.height),t(a)},50)}()</script><div id="days"></div><script language="javascript">function show_date_time(){window.setTimeout("show_date_time()",1e3),BirthDay=new Date("03/14/2020 00:00:00"),today=new Date,timeold=today.getTime()-BirthDay.getTime(),sectimeold=timeold/1e3,secondsold=Math.floor(sectimeold),msPerDay=864e5,e_daysold=timeold/msPerDay,daysold=Math.floor(e_daysold),e_hrsold=24*(e_daysold-daysold),hrsold=setzero(Math.floor(e_hrsold)),e_minsold=60*(e_hrsold-hrsold),minsold=setzero(Math.floor(60*(e_hrsold-hrsold))),seconds=setzero(Math.floor(60*(e_minsold-minsold))),document.getElementById("days").innerHTML="已运行"+daysold+"天"+hrsold+"时"+minsold+"分"+seconds+"秒"}function setzero(e){return 10>e&&(e="0"+e),e}show_date_time()</script></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#机器学习概述"><span class="nav-number">1.</span> <span class="nav-text">机器学习概述</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#什么是机器学习"><span class="nav-number">1.1.</span> <span class="nav-text">什么是机器学习</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#机器学习算法分类"><span class="nav-number">1.2.</span> <span class="nav-text">机器学习算法分类</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#监督学习"><span class="nav-number">1.2.0.0.1.</span> <span class="nav-text">监督学习</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#无监督学习"><span class="nav-number">1.2.0.0.2.</span> <span class="nav-text">无监督学习</span></a></li></ol></li></ol></li></ol><li class="nav-item nav-level-3"><a class="nav-link" href="#机器学习开发流程"><span class="nav-number">1.3.</span> <span class="nav-text">机器学习开发流程</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#特征工程"><span class="nav-number">2.</span> <span class="nav-text">特征工程</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#数据集"><span class="nav-number">2.1.</span> <span class="nav-text">数据集</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#可用数据集"><span class="nav-number">2.1.1.</span> <span class="nav-text">可用数据集</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#sklearn数据集"><span class="nav-number">2.1.2.</span> <span class="nav-text">sklearn数据集</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#特征工程介绍"><span class="nav-number">2.2.</span> <span class="nav-text">特征工程介绍</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#特征提取"><span class="nav-number">2.3.</span> <span class="nav-text">特征提取</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#数据预处理"><span class="nav-number">2.4.</span> <span class="nav-text">数据预处理</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#去除唯一属性"><span class="nav-number">2.4.1.</span> <span class="nav-text">去除唯一属性</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#处理缺失值"><span class="nav-number">2.4.2.</span> <span class="nav-text">处理缺失值</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#特征编码"><span class="nav-number">2.4.3.</span> <span class="nav-text">特征编码</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#无量纲化、正则化"><span class="nav-number">2.4.4.</span> <span class="nav-text">无量纲化、正则化</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#无量纲化"><span class="nav-number">2.4.4.0.1.</span> <span class="nav-text">无量纲化</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#正则化"><span class="nav-number">2.4.4.0.2.</span> <span class="nav-text">正则化</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#标准化与正则化的区别"><span class="nav-number">2.4.4.0.3.</span> <span class="nav-text">标准化与正则化的区别</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#特征选择"><span class="nav-number">2.5.</span> <span class="nav-text">特征选择</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Filter"><span class="nav-number">2.5.1.</span> <span class="nav-text">Filter</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Wrapper"><span class="nav-number">2.5.2.</span> <span class="nav-text">Wrapper</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Embedded"><span class="nav-number">2.5.3.</span> <span class="nav-text">Embedded</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#特征降维"><span class="nav-number">2.6.</span> <span class="nav-text">特征降维</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#主成分分析法（PCA）"><span class="nav-number">2.6.1.</span> <span class="nav-text">主成分分析法（PCA）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#线性判别分析法（LDA）"><span class="nav-number">2.6.2.</span> <span class="nav-text">线性判别分析法（LDA）</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#小结"><span class="nav-number">2.7.</span> <span class="nav-text">小结</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#分类算法"><span class="nav-number">3.</span> <span class="nav-text">分类算法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#sklearn转换器和估计器"><span class="nav-number">3.1.</span> <span class="nav-text">sklearn转换器和估计器</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#转换器"><span class="nav-number">3.1.1.</span> <span class="nav-text">转换器</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#估计器"><span class="nav-number">3.1.2.</span> <span class="nav-text">估计器</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#sklearn机器学习算法的实现"><span class="nav-number">3.1.2.0.1.</span> <span class="nav-text">sklearn机器学习算法的实现</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#估计器工作流程"><span class="nav-number">3.1.2.0.2.</span> <span class="nav-text">估计器工作流程</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#K-近邻算法"><span class="nav-number">3.2.</span> <span class="nav-text">K-近邻算法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#K-近邻算法（KNN）"><span class="nav-number">3.2.1.</span> <span class="nav-text">K-近邻算法（KNN）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#电影类型分析"><span class="nav-number">3.2.2.</span> <span class="nav-text">电影类型分析</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#K-近邻算法API"><span class="nav-number">3.2.3.</span> <span class="nav-text">K-近邻算法API</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#案例：鸢尾花种类预测"><span class="nav-number">3.2.4.</span> <span class="nav-text">案例：鸢尾花种类预测</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#K-近邻总结"><span class="nav-number">3.2.5.</span> <span class="nav-text">K-近邻总结</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#模型选择与调优"><span class="nav-number">3.3.</span> <span class="nav-text">模型选择与调优</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#什么是交叉验证-cross-validation"><span class="nav-number">3.3.1.</span> <span class="nav-text">什么是交叉验证(cross validation)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#超参数搜索-网格搜索-Grid-Search"><span class="nav-number">3.3.2.</span> <span class="nav-text">超参数搜索-网格搜索(Grid Search)</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#模型选择与调优-1"><span class="nav-number">3.3.2.0.1.</span> <span class="nav-text">模型选择与调优:</span></a></li></ol></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#鸢尾花案例调优"><span class="nav-number">3.3.3.</span> <span class="nav-text">鸢尾花案例调优</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#朴素贝叶斯算法"><span class="nav-number">3.4.</span> <span class="nav-text">朴素贝叶斯算法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#理论基础"><span class="nav-number">3.4.1.</span> <span class="nav-text">理论基础</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#案例"><span class="nav-number">3.4.1.1.</span> <span class="nav-text">案例</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#应用场景"><span class="nav-number">3.4.2.</span> <span class="nav-text">应用场景</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#常用贝叶斯分类器"><span class="nav-number">3.4.3.</span> <span class="nav-text">常用贝叶斯分类器</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1-高斯贝叶斯分类器"><span class="nav-number">3.4.3.1.</span> <span class="nav-text">1.高斯贝叶斯分类器</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-多项式贝叶斯分类器"><span class="nav-number">3.4.3.2.</span> <span class="nav-text">2.多项式贝叶斯分类器</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-伯努利贝叶斯分类器"><span class="nav-number">3.4.3.3.</span> <span class="nav-text">3.伯努利贝叶斯分类器</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#代码实现"><span class="nav-number">3.5.</span> <span class="nav-text">代码实现</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1-高斯贝叶斯分类器-1"><span class="nav-number">3.5.0.1.</span> <span class="nav-text">1.高斯贝叶斯分类器</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-多项式贝叶斯分类器-1"><span class="nav-number">3.5.0.2.</span> <span class="nav-text">2.多项式贝叶斯分类器</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-伯努利贝叶斯分类器-1"><span class="nav-number">3.5.0.3.</span> <span class="nav-text">3.伯努利贝叶斯分类器</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#案例：新闻分类"><span class="nav-number">3.5.1.</span> <span class="nav-text">案例：新闻分类</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#朴素贝叶斯总结"><span class="nav-number">3.5.2.</span> <span class="nav-text">朴素贝叶斯总结</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#决策树"><span class="nav-number">3.6.</span> <span class="nav-text">决策树</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#认识决策树"><span class="nav-number">3.6.1.</span> <span class="nav-text">认识决策树</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#决策树分类原理详解"><span class="nav-number">3.6.2.</span> <span class="nav-text">决策树分类原理详解</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#信息论基础"><span class="nav-number">3.6.2.1.</span> <span class="nav-text">信息论基础</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#决策树的三种算法实现"><span class="nav-number">3.6.3.</span> <span class="nav-text">决策树的三种算法实现</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#决策树算法API"><span class="nav-number">3.6.4.</span> <span class="nav-text">决策树算法API</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#案例1：鸢尾花种类预测"><span class="nav-number">3.6.5.</span> <span class="nav-text">案例1：鸢尾花种类预测</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#决策树可视化"><span class="nav-number">3.6.6.</span> <span class="nav-text">决策树可视化</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#案例：泰坦尼克号乘客生存预测"><span class="nav-number">3.6.7.</span> <span class="nav-text">案例：泰坦尼克号乘客生存预测</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#决策树总结"><span class="nav-number">3.6.8.</span> <span class="nav-text">决策树总结</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#集成学习方法之随机森林"><span class="nav-number">3.7.</span> <span class="nav-text">集成学习方法之随机森林</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#什么是集成学习方法"><span class="nav-number">3.7.1.</span> <span class="nav-text">什么是集成学习方法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#什么是随机森林"><span class="nav-number">3.7.2.</span> <span class="nav-text">什么是随机森林</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#随机森林原理过程"><span class="nav-number">3.7.3.</span> <span class="nav-text">随机森林原理过程</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#两个随机"><span class="nav-number">3.7.3.0.1.</span> <span class="nav-text">两个随机</span></a></li></ol></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#随机森林算法API"><span class="nav-number">3.7.4.</span> <span class="nav-text">随机森林算法API</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#案例：泰坦尼克号乘客生存预测-1"><span class="nav-number">3.7.5.</span> <span class="nav-text">案例：泰坦尼克号乘客生存预测</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#总结"><span class="nav-number">3.7.6.</span> <span class="nav-text">总结</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#回归算法"><span class="nav-number">4.</span> <span class="nav-text">回归算法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#线性回归"><span class="nav-number">4.1.</span> <span class="nav-text">线性回归</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#什么是线性回归"><span class="nav-number">4.1.1.</span> <span class="nav-text">什么是线性回归</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#线性关系"><span class="nav-number">4.1.1.0.1.</span> <span class="nav-text">线性关系</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#非线性关系"><span class="nav-number">4.1.1.0.2.</span> <span class="nav-text">非线性关系</span></a></li></ol></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#线性回归原理"><span class="nav-number">4.1.2.</span> <span class="nav-text">线性回归原理</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#1、误差"><span class="nav-number">4.1.2.0.1.</span> <span class="nav-text">1、误差</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#2、损失函数（Loss-Function）"><span class="nav-number">4.1.2.0.2.</span> <span class="nav-text">2、损失函数（Loss Function）</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#3、优化算法"><span class="nav-number">4.1.2.0.3.</span> <span class="nav-text">3、优化算法</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#4、优化动态图演示"><span class="nav-number">4.1.2.0.4.</span> <span class="nav-text">4、优化动态图演示</span></a></li></ol></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#线性回归算法API"><span class="nav-number">4.1.3.</span> <span class="nav-text">线性回归算法API</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#波士顿房价预测"><span class="nav-number">4.1.4.</span> <span class="nav-text">波士顿房价预测</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#正规方程和梯度下降对比"><span class="nav-number">4.1.5.</span> <span class="nav-text">正规方程和梯度下降对比</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#拓展-关于优化方法BGD、SGD、MBGD、SAG"><span class="nav-number">4.1.6.</span> <span class="nav-text">拓展-关于优化方法BGD、SGD、MBGD、SAG</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#欠拟合与过拟合"><span class="nav-number">4.2.</span> <span class="nav-text">欠拟合与过拟合</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#岭回归"><span class="nav-number">4.3.</span> <span class="nav-text">岭回归</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#参数推导"><span class="nav-number">4.3.1.</span> <span class="nav-text">参数推导</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#λ-的选择"><span class="nav-number">4.3.2.</span> <span class="nav-text">$λ$的选择</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#代码实现-1"><span class="nav-number">4.3.3.</span> <span class="nav-text">代码实现</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#LASSO回归"><span class="nav-number">4.4.</span> <span class="nav-text">LASSO回归</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#参数推导-1"><span class="nav-number">4.4.1.</span> <span class="nav-text">参数推导</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#λ-的选择-1"><span class="nav-number">4.4.2.</span> <span class="nav-text">$λ$的选择</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#代码实现-2"><span class="nav-number">4.4.3.</span> <span class="nav-text">代码实现</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#逻辑回归"><span class="nav-number">4.5.</span> <span class="nav-text">逻辑回归</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#逻辑回归的应用场景"><span class="nav-number">4.5.1.</span> <span class="nav-text">逻辑回归的应用场景</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#逻辑回归的原理"><span class="nav-number">4.5.2.</span> <span class="nav-text">逻辑回归的原理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#损失以及优化"><span class="nav-number">4.5.3.</span> <span class="nav-text">损失以及优化</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1、损失"><span class="nav-number">4.5.3.1.</span> <span class="nav-text">1、损失</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2、优化"><span class="nav-number">4.5.3.2.</span> <span class="nav-text">2、优化</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#逻辑回归算法API"><span class="nav-number">4.5.4.</span> <span class="nav-text">逻辑回归算法API</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#案例：癌症分类预测"><span class="nav-number">4.5.5.</span> <span class="nav-text">案例：癌症分类预测</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#分类模型的评估"><span class="nav-number">4.6.</span> <span class="nav-text">分类模型的评估</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#准确率、精确率、召回率、f1-score"><span class="nav-number">4.6.1.</span> <span class="nav-text">准确率、精确率、召回率、f1_score</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#混淆矩阵"><span class="nav-number">4.6.2.</span> <span class="nav-text">混淆矩阵</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ROC曲线和AUC计算"><span class="nav-number">4.6.3.</span> <span class="nav-text">ROC曲线和AUC计算</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#LIft和gain"><span class="nav-number">4.6.4.</span> <span class="nav-text">LIft和gain</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#回归模型的评估"><span class="nav-number">4.7.</span> <span class="nav-text">回归模型的评估</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#模型保存和加载"><span class="nav-number">4.8.</span> <span class="nav-text">模型保存和加载</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#sklearn模型的保存和加载API"><span class="nav-number">4.8.1.</span> <span class="nav-text">sklearn模型的保存和加载API</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#线性回归的模型保存加载案例"><span class="nav-number">4.8.2.</span> <span class="nav-text">线性回归的模型保存加载案例</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#无监督学习-K-means算法"><span class="nav-number">5.</span> <span class="nav-text">无监督学习-K-means算法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#什么是无监督学习"><span class="nav-number">5.1.</span> <span class="nav-text">什么是无监督学习</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#无监督学习包含算法"><span class="nav-number">5.2.</span> <span class="nav-text">无监督学习包含算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#K-means原理"><span class="nav-number">5.3.</span> <span class="nav-text">K-means原理</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#K-means聚类步骤"><span class="nav-number">5.3.1.</span> <span class="nav-text">K-means聚类步骤</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#K-means算法API"><span class="nav-number">5.4.</span> <span class="nav-text">K-means算法API</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#案例：k-means对Instacart-Market用户聚类"><span class="nav-number">5.5.</span> <span class="nav-text">案例：k-means对Instacart Market用户聚类</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Kmeans性能评估指标"><span class="nav-number">5.6.</span> <span class="nav-text">Kmeans性能评估指标</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#轮廓系数"><span class="nav-number">5.6.1.</span> <span class="nav-text">轮廓系数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#轮廓系数值分析"><span class="nav-number">5.6.2.</span> <span class="nav-text">轮廓系数值分析</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#结论"><span class="nav-number">5.6.3.</span> <span class="nav-text">结论</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#轮廓系数API"><span class="nav-number">5.6.4.</span> <span class="nav-text">轮廓系数API</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#用户聚类结果评估"><span class="nav-number">5.6.5.</span> <span class="nav-text">用户聚类结果评估</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#K-means总结"><span class="nav-number">5.7.</span> <span class="nav-text">K-means总结</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#SVM算法"><span class="nav-number">6.</span> <span class="nav-text">SVM算法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#GBDT算法"><span class="nav-number">7.</span> <span class="nav-text">GBDT算法</span></a></li></div></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div class="copyright">Copyright <i class="fa fa-copyright" aria-hidden="true"></i>2020 – <span itemprop="copyrightYear">2021</span> <span class="with-love" id="animate"><i class="fa fa-heartbeat"></i> </span><span class="author" itemprop="copyrightHolder">TJYS.&ensp;</span> <span title="博客总字数"><i class="fa fa-edit"></i>&ensp;<span class="post-count">70.9k</span>字</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-area-chart"></i> </span><span title="symbols_count_time.count_total">190k</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span title="symbols_count_time.time_total">2:53</span></div><span id="sitetime"></span><script language="javascript">function siteTime(){window.setTimeout("siteTime()",1e3);var e=1e3,t=60*e,o=60*t,i=24*o,a=365*i,n=new Date,r=n.getFullYear(),l=n.getMonth()+1,s=n.getDate(),M=n.getHours(),g=n.getMinutes(),m=n.getSeconds(),T=Date.UTC(2020,3,14,0,0,0),f=Date.UTC(r,l,s,M,g,m),h=f-T,u=Math.floor(h/a),d=Math.floor(h/i-365*u),D=Math.floor((h-(365*u+d)*i)/o),c=Math.floor((h-(365*u+d)*i-D*o)/t),w=Math.floor((h-(365*u+d)*i-D*o-c*t)/e);document.getElementById("sitetime").innerHTML=" 我已在此等候你 "+d+" 天 "+D+" 小时 "+c+" 分钟 "+w+" 秒"}siteTime()</script><div class="BbeiAn-info"><a style="text-decoration:none;color:#000;padding-left:30px;background:url(https://cdn.jsdelivr.net/gh/Qikaile/cdn/img/logo_看图王.png) no-repeat left center" target="_blank" href="https://icp.gov.moe" rel="noopener noreferrer">萌ICP备</a> - <a style="color:#000" target="_blank" href="https://icp.gov.moe/?keyword=20210516" rel="noopener noreferrer">20210516号</a></div><script async src="https://imgchr.com/sdk/pup.js" data-url="https://imgchr.com/upload" data-auto-insert="markdown-embed-medium"></script><script src="https://player.lmih.cn/player/js/player.js" id="myhk" key="15874415289" m="1"></script><div class="weixin-box"><div class="weixin-menu"><div class="weixin-hover"><div class="weixin-description">微信扫一扫，在线沟通</div></div></div></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span id="scrollpercent"><span>0</span>%</span></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="//cdn.jsdelivr.net/npm/jquery@2.1.3/dist/jquery.min.js"></script><script type="text/javascript" src="//cdn.jsdelivr.net/fastclick/1.0.6/fastclick.min.js"></script><script type="text/javascript" src="//cdn.jsdelivr.net/jquery.lazyload/1.9.3/jquery.lazyload.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/velocity-animate@1.2.1/velocity.min.js"></script><script type="text/javascript" src="//cdn.jsdelivr.net/npm/velocity-animate@1.2.1/velocity.ui.min.js"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script><script src="https://cdn.jsdelivr.net/gh/qikaile/cdn/js/Valine144.min.js"></script><style>.v .veditor{min-height:10rem;background-image:url(https://cdn.jsdelivr.net/gh/honjun/cdn@1.6/img/other/comment-bg.png);background-size:contain;background-repeat:no-repeat;background-position:right;background-color:rgba(255,255,255,0);resize:none}.v .vwrap{padding:0 0 44px}.v .vwrap .vedit{padding-top:20px}.v .vwrap .vheader{width:80%;bottom:0;position:absolute;background:#fff7f7f7}.v .vinput{padding:10px 15px;text-align:center}.v .vwrap .vheader .vinput{border-bottom:0}.v .vwrap .vedit .vctrl{margin-top:-44px;right:0;position:absolute;margin-right:-3px}.v .vwrap .vrow{position:absolute;right:0;bottom:0;width:20%;padding-top:0}.v .vrow{font-size:0;padding:0 0}.v .vicon{margin-right:15px}.v .vrow .vcol.vcol-30{display:none}.v .vedit .vctrl span.vpreview-btn{display:none}.v .vrow .vcol.vcol-70{width:100%}.v .vsubmit.vbtn{border-radius:0;padding:0;color:#fff;line-height:44px;width:100%;border:none;background:#f99}.v .vcards .vcard .vh .vhead .vsys{background:#fff7f7f7}.v .vcards .vcard .vh .vmeta .vat{font-size:12px;display:block;margin-left:15px;float:right;color:#fff;background-color:#f99;line-height:20px;padding:0 6px;border-radius:3px}@media screen and (max-width:520px){.v .vwrap .vheader .vinput{width:33.33%;padding:10px 5px}}</style><script type="text/javascript">new Valine({
        lang: 'zh-cn',
        admin_email: '2649019047@qq.com', //博主邮箱
        el: '#comments' ,
        notify: true, //邮件提醒！
        appId: 'QJxW5v8BlvRP9LYqYmrNRekF-MdYXbMMI',
        appKey: 'YtfMVlMfTdU4Y4zTo9k0Pj3h',
        avatar: 'monsterid',//小怪物头像,
        placeholder: "你是我一生只会遇见一次的惊喜 ..."
  });

  //   自定义邮箱审核规则
    document.body.addEventListener('click', function(e) {
        if (e.target.classList.contains('vsubmit')) {
            const email = document.querySelector('input[type=email]');
            const nick = document.querySelector('input[name=nick]');
            const reg = /^[A-Za-z0-9\u4e00-\u9fa5]+@[a-zA-Z0-9_-]+(\.[a-zA-Z0-9_-]+)+$/;
            if (!email.value || !nick.value || !reg.test(email.value)) {
                const str = `<div class="valert text-center"><div class="vtext">请填写正确的昵称和邮箱！</div></div>`;
                const vmark = document.querySelector('.vmark');
                vmark.innerHTML = str;
                vmark.style.display = 'block';
                setTimeout(function() {
                    vmark.style.display = 'none';
                    vmark.innerHTML = '';
                }, 2500);
            }
        }
    });

 // 点击回复直接评论,官方版本点击回复时都是跳回到页面上方的评论框进行回复，评论框是固定不动的
    // 参考https://immmmm.com/valine-diy,用到jQuery
    $(document).ready(function(){
        //$('.vemoji-btn').text('😀');
        $("#vcomments").on('click', 'span.vat',function(){
            $(this).parent('div.vmeta').next("div.vcontent").after($("div.vwrap"));
            $('textarea#veditor').focus();
        })
    })</script><script type="text/javascript">function proceedsearch(){$("body").append('<div class="search-popup-overlay local-search-pop-overlay"></div>').css("overflow","hidden"),$(".search-popup-overlay").click(onPopupClose),$(".popup").toggle();var t=$("#local-search-input");t.attr("autocapitalize","none"),t.attr("autocorrect","off"),t.focus()}var isfetched=!1,isXml=!0,search_path="search.xml";0===search_path.length?search_path="search.xml":/json$/i.test(search_path)&&(isXml=!1);var path="/"+search_path,onPopupClose=function(t){$(".popup").hide(),$("#local-search-input").val(""),$(".search-result-list").remove(),$("#no-result").remove(),$(".local-search-pop-overlay").remove(),$("body").css("overflow","")},searchFunc=function(t,e,o){"use strict";$("body").append('<div class="search-popup-overlay local-search-pop-overlay"><div id="search-loading-icon"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div>').css("overflow","hidden"),$("#search-loading-icon").css("margin","20% auto 0 auto").css("text-align","center"),$.ajax({url:t,dataType:isXml?"xml":"json",async:!0,success:function(t){isfetched=!0,$(".popup").detach().appendTo(".header-inner");var n=isXml?$("entry",t).map(function(){return{title:$("title",this).text(),content:$("content",this).text(),url:$("url",this).text()}}).get():t,r=document.getElementById(e),s=document.getElementById(o),a=function(){var t=r.value.trim().toLowerCase(),e=t.split(/[\s\-]+/);e.length>1&&e.push(t);var o=[];if(t.length>0&&n.forEach(function(n){function r(e,o,n,r){for(var s=r[r.length-1],a=s.position,i=s.word,l=[],h=0;a+i.length<=n&&0!=r.length;){i===t&&h++,l.push({position:a,length:i.length});var p=a+i.length;for(r.pop();0!=r.length&&(s=r[r.length-1],a=s.position,i=s.word,p>a);)r.pop()}return c+=h,{hits:l,start:o,end:n,searchTextCount:h}}function s(t,e){var o="",n=e.start;return e.hits.forEach(function(e){o+=t.substring(n,e.position);var r=e.position+e.length;o+='<b class="search-keyword">'+t.substring(e.position,r)+"</b>",n=r}),o+=t.substring(n,e.end)}var a=!1,i=0,c=0,l=n.title.trim(),h=l.toLowerCase(),p=n.content.trim().replace(/<[^>]+>/g,""),u=p.toLowerCase(),f=decodeURIComponent(n.url),d=[],g=[];if(""!=l&&(e.forEach(function(t){function e(t,e,o){var n=t.length;if(0===n)return[];var r=0,s=[],a=[];for(o||(e=e.toLowerCase(),t=t.toLowerCase());(s=e.indexOf(t,r))>-1;)a.push({position:s,word:t}),r=s+n;return a}d=d.concat(e(t,h,!1)),g=g.concat(e(t,u,!1))}),(d.length>0||g.length>0)&&(a=!0,i=d.length+g.length)),a){[d,g].forEach(function(t){t.sort(function(t,e){return e.position!==t.position?e.position-t.position:t.word.length-e.word.length})});var v=[];0!=d.length&&v.push(r(l,0,l.length,d));for(var $=[];0!=g.length;){var C=g[g.length-1],m=C.position,x=C.word,w=m-20,y=m+80;0>w&&(w=0),y<m+x.length&&(y=m+x.length),y>p.length&&(y=p.length),$.push(r(p,w,y,g))}$.sort(function(t,e){return t.searchTextCount!==e.searchTextCount?e.searchTextCount-t.searchTextCount:t.hits.length!==e.hits.length?e.hits.length-t.hits.length:t.start-e.start});var T=parseInt("1");T>=0&&($=$.slice(0,T));var b="";b+=0!=v.length?"<li><a href='"+f+"' class='search-result-title'>"+s(l,v[0])+"</a>":"<li><a href='"+f+"' class='search-result-title'>"+l+"</a>",$.forEach(function(t){b+="<a href='"+f+'\'><p class="search-result">'+s(p,t)+"...</p></a>"}),b+="</li>",o.push({item:b,searchTextCount:c,hitCount:i,id:o.length})}}),1===e.length&&""===e[0])s.innerHTML='<div id="no-result"><i class="fa fa-search fa-5x" /></div>';else if(0===o.length)s.innerHTML='<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>';else{o.sort(function(t,e){return t.searchTextCount!==e.searchTextCount?e.searchTextCount-t.searchTextCount:t.hitCount!==e.hitCount?e.hitCount-t.hitCount:e.id-t.id});var a='<ul class="search-result-list">';o.forEach(function(t){a+=t.item}),a+="</ul>",s.innerHTML=a}};r.addEventListener("input",a),$(".local-search-pop-overlay").remove(),$("body").css("overflow",""),proceedsearch()}})};$(".popup-trigger").click(function(t){t.stopPropagation(),isfetched===!1?searchFunc(path,"local-search-input","local-search-result"):proceedsearch()}),$(".popup-btn-close").click(onPopupClose),$(".popup").click(function(t){t.stopPropagation()}),$(document).on("keyup",function(t){var e=27===t.which&&$(".search-popup").is(":visible");e&&onPopupClose()})</script><script src="https://www.gstatic.com/firebasejs/4.6.0/firebase.js"></script><script src="https://www.gstatic.com/firebasejs/4.6.0/firebase-firestore.js"></script><script>!function(){function getCount(n,t,e){return n.get().then(function(i){var a;return i.exists?(a=i.data().count,e&&(window.localStorage&&window.localStorage.getItem(title)||(n.set({count:a+1,url:t}),a++))):e?(n.set({count:1,url:t}),a=1):a=0,window.localStorage&&e&&localStorage.setItem(title,!0),a})}function appendCountTo(n){return function(t){$(n).append($("<span>").addClass("post-visitors-count").append($("<span>").addClass("post-meta-divider").text("|")).append($("<span>").addClass("post-meta-item-icon").append($("<i>").addClass("fa fa-eye"))).append($("<span>").text("浏览 "+t+" 次")))}}firebase.initializeApp({apiKey:"AIzaSyBEMAtx_og-IwMIY4UeozFyrC0JtPf-Zpw",projectId:"tjys20210113"});var db=firebase.firestore(),articles=db.collection("articles"),isPost="机器学习课程".length>0,isArchive=!1,isCategory="".length>0,isTag="".length>0,urlPath="Machine-learning-note.html",urlFullPath="https://qikaile.us/Machine-learning-note.html",indexPath="index.html",isMenu=!1,menuLink="/";(urlPath.indexOf(menuLink)>0||urlPath==indexPath)&&(isMenu=!0);var menuLink="/about/";(urlPath.indexOf(menuLink)>0||urlPath==indexPath)&&(isMenu=!0);var menuLink="/tags/";(urlPath.indexOf(menuLink)>0||urlPath==indexPath)&&(isMenu=!0);var menuLink="/categories/";(urlPath.indexOf(menuLink)>0||urlPath==indexPath)&&(isMenu=!0);var menuLink="/archives/";(urlPath.indexOf(menuLink)>0||urlPath==indexPath)&&(isMenu=!0);var menuLink="https://travellings.now.sh/";(urlPath.indexOf(menuLink)>0||urlPath==indexPath)&&(isMenu=!0);var menuLink="/links/";(urlPath.indexOf(menuLink)>0||urlPath==indexPath)&&(isMenu=!0);var menuLink="/shuoshuo/";(urlPath.indexOf(menuLink)>0||urlPath==indexPath)&&(isMenu=!0);var menuLink="/bookmark/";(urlPath.indexOf(menuLink)>0||urlPath==indexPath)&&(isMenu=!0);var menuLink="/sitemap.xml";if((urlPath.indexOf(menuLink)>0||urlPath==indexPath)&&(isMenu=!0),isMenu){if(urlPath==indexPath){var titles=[],postsstr="";eval(postsstr);var promises=titles.map(function(n){return articles.doc(n)}).map(function(n){return getCount(n)});Promise.all(promises).then(function(n){var t=$(".post-wordcount");n.forEach(function(n,e){appendCountTo(t[e])(n)})})}}else{var title="机器学习课程",doc=articles.doc(title);getCount(doc,urlFullPath,!0).then(appendCountTo($(".post-wordcount")))}}()</script><script>!function(){var t=document.createElement("script"),s=window.location.protocol.split(":")[0];"https"===s?t.src="https://zz.bdstatic.com/linksubmit/push.js":t.src="http://push.zhanzhang.baidu.com/push.js";var e=document.getElementsByTagName("script")[0];e.parentNode.insertBefore(t,e)}()</script><script type="text/javascript">wpac_init=window.wpac_init||[],wpac_init.push({widget:"Rating",id:23975,el:"wpac-rating",color:"ff9800"}),function(){if(!("WIDGETPACK_LOADED"in window)){WIDGETPACK_LOADED=!0;var e=document.createElement("script");e.type="text/javascript",e.async=!0,e.src="//embed.widgetpack.com/widget.js";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t.nextSibling)}}()</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });</script><script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="https://cdn.jsdelivr.net/gh/theme-next/theme-next-bookmark@latest/bookmark.min.js"></script><script type="text/javascript">bookmark.scrollToMark("manual","#more")</script><script src="https://script-1256884783.file.myqcloud.com/cursor/love.min.js"></script><script>!function(t,h,e,s,j,n){t.hj=t.hj||function(){(t.hj.q=t.hj.q||[]).push(arguments)},t._hjSettings={hjid:1768886,hjsv:6},j=h.getElementsByTagName("head")[0],n=h.createElement("script"),n.async=1,n.src=e+t._hjSettings.hjid+s+t._hjSettings.hjsv,j.appendChild(n)}(window,document,"https://static.hotjar.com/c/hotjar-",".js?sv=")</script><script src="https://script-1256884783.file.myqcloud.com/activate-power-mode.min.js"></script><script>POWERMODE.colorful=!0,POWERMODE.shake=!1,document.body.addEventListener("input",POWERMODE)</script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/qikaile/cdn@1.1/js/clipboard.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/qikaile/cdn@1.1/js/clipboard-use.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/qikaile/cdn@1.3/css/sharejs.css"><script src="https://cdn.jsdelivr.net/gh/qikaile/cdn@1.3/js/social-share.min.js"></script></body></html><!-- rebuild by neat -->