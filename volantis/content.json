{"meta":{"title":"天镜云生","subtitle":"Welcome!","description":"心静思远，志行千里。","author":"TJYS","url":"https://qikaile.us/volantis","root":"/volantis/"},"pages":[{"title":"","date":"2020-11-17T08:40:38.000Z","updated":"2020-12-03T09:01:02.431Z","comments":true,"path":"bb/index.html","permalink":"https://qikaile.us/volantis/bb/index.html","excerpt":"","text":"说说"},{"title":"","date":"2020-11-17T08:40:38.000Z","updated":"2020-12-05T13:54:31.014Z","comments":true,"path":"about/index.html","permalink":"https://qikaile.us/volantis/about/index.html","excerpt":"","text":"关于我 📝简介 一个生于90年代，爱好玩博客的蓝孩子~😊 对HTML、CSS、JS（jQuery）、PHP以及Python的认识处于萌新阶段。 爱好运动、拍照。 关于当前小目标，搭建个性化博客！ 大目标，建站10年。 🔧技能条 .progress-bar{ -moz-appearance: none; -webkit-appearance: none; border: none; border-radius: 290486px; display: block; height: 1rem; overflow: hidden; padding: 0; width: 100%; background-color: #EFE7CC; } .progress-hong{ /* 进度部分 */ /* 利用继承父元素宽度的百分比控制进度 */ width: 90%; height: 100%; /* 内层背景色即进度达到的颜色 */ background-color: #F14668; border-radius: 3px; } .progress-huang{ /* 进度部分 */ /* 利用继承父元素宽度的百分比控制进度 */ width: 82%; height: 100%; /* 内层背景色即进度达到的颜色 */ background-color: #FFCB00; border-radius: 3px; } .progress-zi{ /* 进度部分 */ /* 利用继承父元素宽度的百分比控制进度 */ width: 75%; height: 100%; /* 内层背景色即进度达到的颜色 */ background-color: #AE66F3; border-radius: 3px; } .progress-lan{ /* 进度部分 */ /* 利用继承父元素宽度的百分比控制进度 */ width: 50%; height: 100%; /* 内层背景色即进度达到的颜色 */ background-color: #3273DC; border-radius: 3px; } .progress-lv{ /* 进度部分 */ /* 利用继承父元素宽度的百分比控制进度 */ width: 65%; height: 100%; /* 内层背景色即进度达到的颜色 */ background-color: #2DCE89; border-radius: 3px; } .progress-bar1{ -moz-appearance: none; -webkit-appearance: none; border: none; border-radius: 290486px; display: block; height: 1rem; overflow: hidden; padding: 0; width: 100%; background-color: #EFE7CC; } .progress-hong1{ /* 进度部分 */ /* 利用继承父元素宽度的百分比控制进度 */ height: 100%; /* 内层背景色即进度达到的颜色 */ background-color: #F14668; border-radius: 3px; } #loading-status{ -moz-appearance: none; -webkit-appearance: none; border: none; border-radius: 290486px; display: block; height: 1rem; overflow: hidden; padding: 0; width: 100%; background-color: #EFE7CC; } #process-zt {width: 100%;height: 100%;border-radius: 10px;background: -webkit-gradient(linear, 0 0, 0 100%, from(#7BC3FF), color-stop(0.5,#42A9FF), to(#7BC3FF));-webkit-animation: load 3s ease-out infinite;} @-webkit-keyframes load { 0% { width: 0%; } 100% { width: 100%; } } HTML CSS jQuery PHP Python 折腾力 📱联系方式 👨客观评价 📆建站历程2020年 11月 使用Volantis作为主题的博客 03月 搭建了第一个博客 云生博客"},{"title":"文章分类","date":"2020-11-17T08:40:08.000Z","updated":"2020-11-18T04:39:25.359Z","comments":true,"path":"categories/index.html","permalink":"https://qikaile.us/volantis/categories/index.html","excerpt":"","text":""},{"title":"","date":"2020-11-17T08:40:54.000Z","updated":"2020-12-03T09:07:07.850Z","comments":true,"path":"links/index.html","permalink":"https://qikaile.us/volantis/links/index.html","excerpt":"","text":"我 的 好 友 技术大佬Heson人生在勤，不索何获。 Leafacestay hangry stay foolish xaoxuuVolantis主题作者 Colsrch愿多年以后，我可以酌一杯清酒，烂醉如泥，梦中回到我们的曾经。 枋柚梓繁星永存，记忆亘古不变。"},{"title":"","date":"2020-11-17T08:41:06.000Z","updated":"2020-12-26T08:05:30.186Z","comments":true,"path":"guestbook/index.html","permalink":"https://qikaile.us/volantis/guestbook/index.html","excerpt":"","text":"#page{opacity: 0.85;}"},{"title":"标签","date":"2020-11-17T08:40:24.000Z","updated":"2020-11-17T11:58:56.899Z","comments":true,"path":"tags/index.html","permalink":"https://qikaile.us/volantis/tags/index.html","excerpt":"","text":""},{"title":"","date":"2020-11-17T08:41:06.000Z","updated":"2020-12-26T08:19:35.976Z","comments":false,"path":"vadmin/index.html","permalink":"https://qikaile.us/volantis/vadmin/index.html","excerpt":"","text":"评 论 管 理 跳转到评论管理页面 跳转中... var bar=0 var line=\"||\" var amount=\"||\" count() function count(){ bar=bar+2 amount =amount + line document.loading.chart.value=amount document.loading.percent.value=bar+\"%\" if (bar"},{"title":"demolist","date":"2020-12-18T08:31:35.000Z","updated":"2020-12-18T09:07:02.410Z","comments":true,"path":"demolist/index.html","permalink":"https://qikaile.us/volantis/demolist/index.html","excerpt":"","text":"calender body { background-color: #000; margin: 0; padding: 0; overflow: hidden; text-align: center; } .hidden { display: none; } #display { margin: auto; } var cvs = document.getElementById(\"cvs\"); var ctx = cvs.getContext(\"2d\"); var display = document.getElementById(\"display\"); var displayCtx = display.getContext(\"2d\"); var screenImage = document.getElementById(\"screenImage\"); var screenImageCtx = screenImage.getContext(\"2d\"); var rili = document.getElementById(\"rili\"); var riliCtx = rili.getContext(\"2d\"); // var songInfo = {}; // var AllTimeBak = 0; // var NowBak = 0; // var DrawWarning = false; // function updateSongInfo() { // var xhr = new XMLHttpRequest(); // xhr.open('GET', \"http://127.0.0.1:62333/BGMName\", true); // xhr.onreadystatechange = function () { // if (xhr.readyState == 4 && xhr.status == 200 || xhr.status == 304) { // songInfo = JSON.parse(xhr.responseText); // if (songInfo.AllTime -1) { countDay = 30; } else if (array_threeone.indexOf(mouth + 1) > -1) { countDay = 31; } else { if (year % 4 == 0 && year % 100 != 0 || year % 400 == 0) countDay = 29; else countDay = 28; } var row = 6; if (7 - firstDraw + 7 * 4 < countDay) { //确定表格行数，防止日期绘制不全 row = 7; } function drawTodaybg(i, j) { riliCtx.save(); riliCtx.beginPath(); riliCtx.strokeStyle = \"#900\"; riliCtx.arc(45 + i * cardSize * 1.7 + cardSize / 1.18, 50 + j * cardSize + cardSize / 2, cardSize / 2 - 10, - Math.PI, Math.PI * 1); riliCtx.stroke(); riliCtx.closePath(); riliCtx.beginPath(); riliCtx.arc(45 + i * cardSize * 1.7 + cardSize / 1.18, 50 + j * cardSize + cardSize / 2, cardSize / 2 - 9, - Math.PI, Math.PI * 0.9); riliCtx.stroke(); riliCtx.closePath(); riliCtx.beginPath(); riliCtx.arc(45 + i * cardSize * 1.7 + cardSize / 1.18, 50 + j * cardSize + cardSize / 2, cardSize / 2 - 8, - Math.PI, Math.PI * 0.8); riliCtx.stroke(); riliCtx.closePath(); riliCtx.beginPath(); riliCtx.arc(45 + i * cardSize * 1.7 + cardSize / 1.18, 50 + j * cardSize + cardSize / 2, cardSize / 2 - 7, - Math.PI, Math.PI * 0.7); riliCtx.stroke(); riliCtx.closePath(); riliCtx.beginPath(); riliCtx.arc(45 + i * cardSize * 1.7 + cardSize / 1.18, 50 + j * cardSize + cardSize / 2, cardSize / 2 - 6, - Math.PI, Math.PI * 0.6); riliCtx.stroke(); riliCtx.closePath(); riliCtx.restore(); } var isNum = /^\\d+(\\d+)?$/; function drawDate(txt, i, j) { riliCtx.textAlign = \"center\"; riliCtx.fillStyle = \"rgb(69,68,84)\"; riliCtx.font = (cardSize / 1.5) + 'px Impact'; var yOffest = 3; if ((j == 0 || j == 6) && isNum.test(txt)) { riliCtx.fillStyle = \"#900\"; } riliCtx.fillText(txt.toString(), 45 + j * cardSize * 1.7 + cardSize / 1.18, 50 + i * cardSize + cardSize / 3 * 2 + yOffest); if (txt == today) { drawTodaybg(j, i); } } riliCtx.fillStyle = \"rgb(69,68,84)\"; riliCtx.font = \"900 26pt SimHei\"; riliCtx.textAlign = \"center\"; var monthCN = [\"一\",\"二\",\"三\",\"四\",\"五\",\"六\",\"七\",\"八\",\"九\",\"十\",\"十一\",\"十二\"]; riliCtx.scale(1.1,1); riliCtx.fillText(monthCN[mouth] + \"月\", 260, 32); riliCtx.resetTransform(); riliCtx.font = \"20pt SimHei\"; riliCtx.textAlign = \"end\"; riliCtx.fillText(today + \"日\",520,38); for (var i = 0; i < row; i++) { for (var j = 0; j < 7; j++) { riliCtx.strokeRect(45 + j * cardSize * 1.70, 50 + i * cardSize, cardSize * 1.70, cardSize); } } dayIndex = 1; for (var i = 0; i < row; i++) { //开始绘制日期数 for (var j = 0; j < 7; j++) { if (i == 0) { //表格第一行绘制星期 drawDate(array_week[j], i, j); continue; } if (i == 1 && j < firstDraw) { //确定1号绘制位置 continue; } if (dayIndex > countDay) { //只绘制月份的天数 break; } drawDate(dayIndex++, i, j); } } } var riliInterval = setInterval(drawRili, 3600000); drawRili(); // Canvas奇妙的剪切蒙版实现 var screenMask = new Image(); screenMask.src = \"https://7.dusays.com/2020/11/02/e7199b4f3f87e.png\"; var screen = new Image(); screen.src = \"https://7.dusays.com/2020/11/02/e946a3582f394.png\"; var iv = setInterval(() => { if (screen.complete && screenMask.complete) { screenImageCtx.drawImage(screen, -1041, -210, 1280, 720); screenImageCtx.globalCompositeOperation = \"destination-atop\"; screenImageCtx.drawImage(screenMask, 0, 0); screenImageCtx.globalCompositeOperation = \"source-over\"; clearInterval(iv); } }, 14); // 奇妙的屏幕大小自适应 window.onresize = function () { if (window.innerWidth / window.innerHeight > 1.8333333333333) { display.width = window.innerWidth; display.height = window.innerWidth / 1980 * 1080; window.scrollTo(0, (window.innerHeight - 123) / 16); } else { display.width = window.innerHeight / 1080 * 1980; display.height = window.innerHeight; } } window.onresize(); // 加载图片 var bg = new Image(); bg.src = \"https://7.dusays.com/2020/11/02/5b48e188ad25a.png\"; var mask = new Image(); mask.src = \"https://7.dusays.com/2020/11/02/68b1e7ff42ee7.png\"; var light = new Image(); light.src = \"https://7.dusays.com/2020/11/02/4c4d1654bcb50.png\"; var caidai = new Image(); caidai.src = \"https://7.dusays.com/2020/11/02/7d07ef5c998f9.png\"; var two = new Image(); two.src = \"https://7.dusays.com/2020/11/02/a2d92ffe8b146.png\"; var screenLight = new Image(); screenLight.src = \"https://7.dusays.com/2020/11/02/23d44c4d692aa.png\"; var phoneLight = new Image(); phoneLight.src = \"https://7.dusays.com/2020/11/02/7d6996c7a6bc3.png\"; var phoneText = JSON.parse(\"[{\\\"time\\\":0,\\\"text\\\":\\\"凌晨啦!\\\"},{\\\"time\\\":6,\\\"text\\\":\\\"早上好!\\\"},{\\\"time\\\":8,\\\"text\\\":\\\"上午好!\\\"},{\\\"time\\\":11,\\\"text\\\":\\\"你吃了吗\\\"},{\\\"time\\\":13,\\\"text\\\":\\\"下午好鸭!\\\"},{\\\"time\\\":16,\\\"text\\\":\\\"傍晚咯!\\\"},{\\\"time\\\":19,\\\"text\\\":\\\"晚安!\\\"}]\"); var noRili = false; var updateSongInfoHandler = -1; // 设置左上角的壁纸 window.wallpaperPropertyListener = { applyUserProperties: function (properties) { if (properties.screenFile) { if (properties.screenFile.value) { screen.src = 'file:///' + properties.screenFile.value; var iv1 = setInterval(() => { if (screen.complete && screenMask.complete) { screenImageCtx.clearRect(0, 0, 1000, 1000); screenImageCtx.drawImage(screen, -1041, -210, 1280, 720); screenImageCtx.globalCompositeOperation = \"destination-atop\"; screenImageCtx.drawImage(screenMask, 0, 0); screenImageCtx.globalCompositeOperation = \"source-over\"; clearInterval(iv1); } }, 14); } } if (properties.phoneText) { if (properties.phoneText.value) { phoneText = JSON.parse(properties.phoneText.value); } } if (properties.disableRili) { if (properties.disableRili.value) { clearInterval(riliInterval); noRili = true; } else { riliInterval = setInterval(drawRili, 3600000); drawRili(); noRili = false; } } if (properties.showSongInfo) { if (properties.showSongInfo.value) { updateSongInfoHandler = setInterval(updateSongInfo, 250); updateSongInfo(); } else { clearInterval(updateSongInfoHandler); updateSongInfoHandler = -1; } } } } var data = new Array(128); var animData = new Array(128); var SoundPlaying = false; // 奇妙的初始化 for (var i = 0; i < 128; i++) { data[i] = animData[i] = 0; } // 奇妙的Normalize var peakValue = 1; if (window.wallpaperRegisterAudioListener) { window.wallpaperRegisterAudioListener(function (audioData) { var max = 0; for (var i = 0; i < 128; i++) { if (audioData[i] > max) max = audioData[i]; } peakValue = peakValue * 0.99 + max * 0.01; for (i = 0; i < 64; i++) { data[63 - i] = audioData[i] / peakValue; } for (i = 0; i < 64; i++) { data[127 - i] = audioData[127 - i]; } }); } else { var iva = setInterval(() => { for (var i = 0; i < 128; i++) { data[i] = Math.random(); } }, 10); } // .... function min(a, b) { return a > b ? b : a; } function max(a, b) { return a > b ? a : b; } // 奇妙的颜色变化 var targetColor = { r: 80, g: 120, b: 169 }; var currentColor = { r: 80, g: 120, b: 169 }; var lightColor = { r: 0, g: 34, b: 77, a: 0 }; function colorToRgb(color) { return (\"rgb(\" + color.r.toString() + \",\" + color.g.toString() + \",\" + color.b.toString() + \")\"); } function colorToRgba(colorWithA) { return (\"rgba(\" + colorWithA.r.toString() + \",\" + colorWithA.g.toString() + \",\" + colorWithA.b.toString() + \",\" + colorWithA.a.toString() + \")\"); } var night = false; var debug = false; // Canvas的奇妙冒险! function render() { for (var i = 0; i < 128; i++) { animData[i] += (data[i] - animData[i]) * 0.3; animData[i] = min(animData[i], 1); } currentColor.r += (targetColor.r - currentColor.r) * 0.01; currentColor.r = min(currentColor.r, 255); currentColor.r = max(currentColor.r, 0); currentColor.g += (targetColor.g - currentColor.g) * 0.01; currentColor.g = min(currentColor.g, 255); currentColor.g = max(currentColor.g, 0); currentColor.b += (targetColor.b - currentColor.b) * 0.01; currentColor.b = min(currentColor.b, 255); currentColor.b = max(currentColor.b, 0); ctx.clearRect(0, 0, 1980, 1080); ctx.drawImage(bg, 0, 0); ctx.drawImage(mask, 954, 99); ctx.fillStyle = \"#97adbb\"; ctx.font = \"32pt Impact\"; ctx.transform(1, 2.05 * (Math.PI / 180), 0, 1, 0, 0); var time = new Date(); ctx.fillText((time.getHours() < 10 ? \"0\" : \"\") + time.getHours().toString() + \":\" + (time.getMinutes() < 10 ? \"0\" : \"\") + time.getMinutes() + \":\" + (time.getSeconds() < 10 ? \"0\" : \"\") + time.getSeconds().toString(), 725, 318); ctx.resetTransform(); //日历本 ctx.transform(0.9645, 0, 0 * (Math.PI / 180), 0.96, 967, 100); ctx.rotate(6 * (Math.PI / 180)); if (!noRili) { ctx.drawImage(rili, 0, 0); ctx.resetTransform(); ctx.transform(0.9645, 0, 9 * (Math.PI / 180), 1, 825, 160); ctx.rotate(7 * (Math.PI / 180)); } targetColor = { r: 80, g: 120, b: 169 }; if (night) { targetColor = { r: 255, g: 75, b: 80 }; } if (!noRili) { ctx.fillStyle = \"rgba(0,0,0,0.5)\"; ctx.fillRect(-10, 320, 650, 2); } ctx.fillStyle = colorToRgb(currentColor); if (!noRili) { for (var i = 32; i < 95; i++) ctx.fillRect(10 * (i - 32), 20 + (300 - 300 * animData[i]), 4, 300 * animData[i]); } else for (var i = 32; i < 95; i++) ctx.fillRect(40 + 7.5 * (i - 32), 30 + (300 - 300 * animData[i]), 4, 300 * animData[i]); ctx.resetTransform(); ctx.globalCompositeOperation = \"overlay\"; ctx.drawImage(light, 971, 197); ctx.globalCompositeOperation = \"source-over\"; ctx.drawImage(caidai, 949, 25); ctx.drawImage(two, 1319, 345); // 夜间光照 if (night && lightColor.a < 0.7) { lightColor.a += 0.005; lightColor.a = min(lightColor.a, 0.7); } else if (!night) { lightColor.a -= 0.005; lightColor.a = max(lightColor.a, 0.0); } if (lightColor.a > 0) { ctx.globalCompositeOperation = \"hard-light\"; ctx.fillStyle = colorToRgba(lightColor); ctx.fillRect(0, 0, 1980, 1080); ctx.globalCompositeOperation = \"source-over\"; ctx.globalAlpha = lightColor.a / 0.7; ctx.drawImage(phoneLight, 860, 437); ctx.globalAlpha = 1; } //屏幕 ctx.drawImage(screenImage, 0, 0); if (lightColor.a > 0) { ctx.globalAlpha = lightColor.a / 0.7; ctx.drawImage(screenLight, 0, 0); ctx.globalAlpha = 1; } night = true; var greeting = \"凌晨啦!\"; phoneText.forEach((v) => { if (time.getHours() >= v.time) { greeting = v.text; } }); if (time.getHours() >= 6 && time.getHours()"}],"posts":[{"title":"Abaqus 有限元教程(气动网弯曲执行器)","slug":"FEM-analysis-of-PneuNet-actuator","date":"2020-12-14T11:51:55.000Z","updated":"2020-12-14T11:54:25.477Z","comments":true,"path":"posts/28586.html","link":"","permalink":"https://qikaile.us/volantis/posts/28586.html","excerpt":"","text":"课程1 导入零件和创建图纸表面 部件→导入→模型文件 模型B→表面→创建→名称随意设置例如“Top of B”→继续→选择面→完成 Your browser does not support the video tag. 课程2 创建材料材料→名称Elastosil→通用→ 质量密度 1130e-12 力学→弹性→超弹性→应变势能 yeoh→系数输入源：系数 C10 0.11 C20 0.02 材料→名称Paper→通用→ 质量密度 750e-12 力学→弹性→弹性→杨氏模量6500 泊松比0.2 Your browser does not support the video tag. 课程3 创建和指定截面截面→名称Sec-Elastic→实体→继续→材料→Elastosil 截面→名称Sec-Paper→壳→数值：0.1 材料：Paper 模型A→截面指派→点选模型A，取消下方【创建集合】√→截面选择Sec-Elastic 模型B→截面指派→点选模型B，取消下方【创建集合】√→截面选择Sec-Elastic 模型C→截面指派→点选模型C，取消下方【创建集合】√→截面选择Sec-Elastic Your browser does not support the video tag. 课程4 装配和定位带约束的零件装配→实例→部件ABC全选→【从其它的实例自动偏移】√ 位置约束→确保选择的是【共面】→箭头一致 Your browser does not support the video tag. 课程5 合并零件位置约束→→部件名Merged→保持→继续→全选→完成 Your browser does not support the video tag. 课程6 创建蒙皮和指定图纸部分Merged→双击蒙皮→然后点击菜单栏工具→显示组→管理器→All→创建→表面→Top of B→替换→关闭 选择面→完成 双击截面指派→选择面→完成→弹出框点击确定 Your browser does not support the video tag. 课程7 选择内腔曲面点击ABC模型 由面显示模型ABC Merged→双击表面→名称Surf-Inner Cavity→然后点击菜单栏工具→视图切片→管理器→ 选择面（内腔）（shift+鼠标左键点选多个面） Your browser does not support the video tag. 课程8 设置重力载荷Steps→名称Step-Gravity→继续→几何非线性 开→确定 Step-Gravity下【载荷】双击→名称Load-Gravity→力学 重力→继续→分量2：-9810(注意看方向)→确定 双击【边界条件】→名称Fixed End→继续→点击面 取消下方【创建集合】√→完成→选择最后一个完全固定→确定 Your browser does not support the video tag. 课程9 设置压力载荷Steps→名称Step-Pressure→继续→确定 Step-Pressure下【载荷】双击→名称Load-Pressure→力学 压强→点击表面 选择第二个Merged-1.surf-Inner Cavity →继续→大小：0.06→确定 Your browser does not support the video tag. 课程10 网格模型Merged→双击网格→点击【指派网格控制属性】 →左框选所有模型→完成→【单元形状】选择【四面体】→确定 点击【种子部件】→近似全局尺寸：2.5→确定 点击【为部件划分网格】→选择【是】 Your browser does not support the video tag. 课程11 设置网格类型Merged→网格→点击【菜单栏】网格→【单元类型】→点选三个模型表面→完成→选择【杂交公式】→确定 点击菜单栏工具→显示组→管理器→All→创建→表面→Top of B→替换→关闭 点击【菜单栏】网格→【单元类型】→若点不了点选模型 →完成→【几何阶次】选择二次→确定 Your browser does not support the video tag. 课程12 创建、提交和监控作业分析→作业→名称PneuNets→继续→确定 右击点选作业下PneuNets→【提交】→【监控】 Your browser does not support the video tag.","categories":[],"tags":[]},{"title":"Solidworks学习笔记","slug":"solidworks-note","date":"2020-11-19T02:40:26.000Z","updated":"2020-12-14T10:04:07.614Z","comments":true,"path":"posts/26833.html","link":"","permalink":"https://qikaile.us/volantis/posts/26833.html","excerpt":"","text":"1、建模原理【草图】： 步骤1：选择一个平面（选中此面） 步骤2：点击草工具→草图绘制 步骤3：绘制线条 步骤4：完成，退出此草图 【拉伸】特征： 步骤1：选中草图 步骤2：启用【拉伸】命令 步骤3：输入长度 步骤4：完成 确定的模型： 因素1：尺寸 一、草图尺寸 1、一个对象标注尺寸 对象属性（长度/直径/半径） 2、两个对象标注尺寸 对象关系（距离、角度） 3、对称尺寸（构造线） 其它操作 1、锁定标注方式 2、圆的标注 3、三个点的标注 二、特征尺寸 因素2：几何关系 添加方法： 1、添加 2、查看 3、删除 选中对象添加关系（多个对象）1、按住Ctrl； 2、框选 设计树特征操作 添加方法： 1、退回 回退栏、【退回】、【退回到尾】、【退回到前】 注：压缩/解压缩 2、特征 2.1 编辑草图—更改轮廓；2.2 更改特征—更改生成方式的细节 3、设计树操作 可拖动调整 4、父子关系 特征的依赖关系 如：【特征A】使用了【特征B】生成的点/线/面等 那么【特征A】依赖于【特征B】 【A】为子，【B】为父； 快捷键设置 2、高级技巧及优秀建模思路3、错误修复1、查看错误 右键→【错误】 2、常见错误及解决方法 1）合法性错误：开环，闭环，交叉，端点共享 工具→草图工具→检查草图合法性→检查 检查草图合法性、查看边角处连接 2）草图约束关系错误：尺寸及几何关系 检查颜色不正常的几何关系/草图对象 3）草图基准面遗失 右击左侧草图→编辑草图平面→点击选择需要的实体面 4）圆角出错 两种情况：a.顺序不对导致无法正常圆角 b.圆角过大 自动修复/测量大小再更改 4、配置及设计表理解配置 由一个零件，保存这个零件的多个型号； 优点：1、更容易创建；2、统一管理；3、调用简单 配置操作 1、创建新配置 新建零件→点击配置→右击零件1配置→添加配置 2、修改尺寸（不同配置不同尺寸） 3、修改特征（不同配置不同特征） 设计表 1、生成表格 （1）Office Excel （2）压缩；S 解压缩：U （3）“普通”——文本 2、设置为“此配置”的尺寸，才会出现在表格中 设计表-操作 1、更改参数 2、生成配置 扩展-【配置特征】 步骤： 1、创建一个新的配置 2、设置要添加到表格的尺寸（设置为“此配置”） 3、【配置特征】：表格中创建配置及修改参数 5、装配体1、零件基本操作 打开文件的方法： 1）【文件】→打开 2、拖拽打开： 注：放置到工具栏→打开 放置到其它文件视图区→可选插入到此文件 零件基本操作 1、点击插入/勾选插入 第一个固定； 勾选的固定：勾选会放置到中心 2、零件基本操作 旋转（选中右键拖动）/平移（选中左键拖动） 配合 掌握以下四项内容： （1）添加【配合】 自由度：零部件在配合或固定之前有六个自由度：三个轴向的移动和三个轴向的旋转。 装配体中切换零件型号/配置 选中零件右击→零部件属性→所参考的配置 使用技巧：智能配合 1）按住Alt，鼠标拖动配合 2）按住Ctrl，鼠标拖动复制 1）Alt快速配合时，同时可以转动模型 2）Alt快速配合时，建议缩放到容易选中及放置对象的位置 3）Alt快速配合时，选中圆线，然后按住Alt，拖拽圆线到所在面圆线（若相反，松开Alt，不松开左键，点击Tab键）：同心及重合 （2）查找【配合】 1）查看设计树配合/对应零件 2）关联工具栏【查看配合】 更加生动查看配合 右击选中零件→【查看配合】 3）压缩进行查找—案例 （3）删除【配合】 【删除】原配合，添加【宽度配合】（配合→高级配合→宽度→选中参考两面→选中要的两面） （4）编辑【配合】 通过草图或者基准面进行配合 题目要求：添加约束：将原本可以相对转动的两个圆盘保存在$30^。 $的相对位置 基准面与基准面（基准线与基准线）之间的角度配合添加到零件中，而不是装配体中。 右击选中零件→打开零件→编辑添加基准面（基准线）→然后返回装配体，选中两基准面（基准线）配合 3、固定/浮动零件 【间隙检查】两个零件之间的间隙大小 评估→间隙验证 问题：看不到的对象如何配合？ 【选择其它】 配合→右击选择其它→点选面 装配体设计合理性 1）干涉检查 评估→干涉检查 2）碰撞检测 3）使用碰撞检测测量极限位置 替换零部件步骤： 右击点击要替换的零部件→【替换零部件】→ 6、装配体高级操作案例 1、零部件阵列 2、特征驱动阵列 3、镜像 4、随配合复制 根据需要的新的位置配合，复制零件，并添加配合 步骤： 1）选择要复制的零件 2）选择要添加新的配合的对象（确定新位置） 注：对于配合不变的，使用“重复” 爆炸视图 1、爆炸视图 （1）爆炸视图步骤 （2）爆炸动画 （3）使用Motion制作动画 2、工程图-BOM表、序号 若出现问题使用以下方法 子装配体 1、理解子装配体 什么是子装配体？为什么需要子装配体？ 把装配体A放入装配体B中：A即为B的子装配体，B即为总装配体 零件 装配体 零部件（零件，装配体） 2、操作 生成方法： 1）插入 2）设计树生成/解散方法 3）移入/移出 3、刚性/柔性 4、子装配体内部的配合，必须在子装配体中操作 替换零部件 1、由N个替换N个零部件 右击点击要替换的零部件→【替换零部件】→ 2、由1个替换N个零部件 若有干涉先删除N-1个，再将装配体作为一个零部件，再替换 设计树那边点击【替换零部件】 镜像零部件 1）多个方位选择 2）生成相反方位的零部件 【线性零部件阵列】→【镜像零部件】 7、装配体自上而下建模思路【自上而下】设计方法 1、理解【自上而下】 零件：设计（形状、配置等） 装配体：配合、运动、动画 装配体中的零件编辑状态：具有外界参考的设计（形体，配置等） 2、操作步骤 前提：处于装配体中： （1）【新零件】 确定新零件方位：（2种方法） 1）【Esc】退出按钮 前提：装配体中零件编辑状态：取消“无外部参考” 插入零部件→▽→新零件→Esc 2）点面确定前视基准面 插入零部件→▽→新零件→点选前基准面→退出草图，编辑零部件 （2）【编辑】：在装配体中编辑零件 （3）引用其它的零件形状创建当前零件 （4）注：哪个是当前编辑状态的零件 1）设计树蓝色的零件 2）特殊显示状态的零件 3、其他注意的操作 8、零件工程图工程图基本操作 从零件/装配体制作工程图 （1）视图调色板/查看调色板：拖放视图 （2）注释→模型项目：添加尺寸 视图布局→标准三视图 视图布局→标模型视图 （1）倾斜面：辅助视图→点击倾斜的线 （2）点击视图→方向 （3）工程图选择配置：视图→属性→配置 （1）剖视图 剖面操作方法：（不同版本Solidworks有所区别） 1）先启用命令 2）先绘制剖面草图 最后选的决定方向 （2）剖面与旋转剖 【剖面视图】→ 注：1-2-3点顺序 尺寸 尺寸类型： 模型尺寸：与模型关联 参考尺寸：从动尺寸 （1）模型项目（模型尺寸） 尺寸分组： 为工程图标注 没为工程图标注 实例/圈数计数 异型孔向导轮廓 异型孔向导位置 孔标注 （2）参考尺寸 从动尺寸 自动标注尺寸 （3）尺寸的控制 显示/隐藏尺寸视图 【视图】→【隐藏/显示注解】 【视图】→【被隐藏的视图】 尺寸的移动和删除 按住Shift拖动尺寸，可以将尺寸移动到其它视图 按住Ctrl拖动尺寸，可以将尺寸复制到其它视图 对齐尺寸 尺寸快速设置 工程图模板制作 （1）图纸与图纸格式 图纸：工程视图 图纸格式：图纸、标题栏 （2）制作工程图及保存为模板 1）图纸属性设置大小 2）绘制外框与内框（两个矩形）（定位） 绘制标题栏（线型工具、图层工具、线粗、线条样式） 3）表格定位点（材料明细表、焊接切割清单、修订表） 4）文字链接（自动填写功能）（比例、图纸页数、模型名称、短日期） 5）插入图标 6）保存模板","categories":[],"tags":[{"name":"Solidworks","slug":"Solidworks","permalink":"https://qikaile.us/volantis/tags/Solidworks/"}]},{"title":"听力必考21大场景","slug":"Scenes-of-Listening","date":"2020-11-19T02:32:52.000Z","updated":"2020-11-19T02:34:47.163Z","comments":true,"path":"posts/44468.html","link":"","permalink":"https://qikaile.us/volantis/posts/44468.html","excerpt":"","text":"一、餐馆场景restaurant餐馆; cafeteria自助餐厅;buffet自助餐; canteen; dining hall食堂,餐厅;coffee shop: cafe咖啡屋; snack bar小吃街、大排挡go out for dinner / dinner out / eat out出去吃饭waiter/waitress男/女服务生;order点菜: serve上菜; menu菜单; bill账单; pay the bill付账;tip小费; change零钱 Keep the change！不用找零钱了！treat请客This is my treat: / It’s on me!我请客!go Dutch: Let’s go fifty fifty. AA制Help yourself.请随便吃 knife刀; fork叉; chopsticks筷子;spoon勺子; plate碟子; tray托盘;appetizer开胃菜; dessert甜品,水果;soup汤; steak牛排; cheese奶酪; sandwich三明治; hamburger汉堡包;French fries炸薯条: pizza披萨;bacon培根; chicken鸡肉; beef牛肉; pork猪肉drink喝,饮料; wine红酒; coke可口可乐; coffee咖啡;dressing调味酱; pepper胡椒; ketchup番茄酱delicious/yummy可口的; rare半熟的 二、邮局场景stamp邮票; envelope信封;package/parcel包裹; insurance保险;parcel form包裹单; zip code邮编; weight重量; overweight超重;postage邮费; extra postage额外邮资；send/post / deliver a letter/mail寄/发信；delivery投递,传送; claim领取; express mail快件; EMS特快专递;airmail航空信件; surface mail平邮;ordinary letter平信; registered letter挂号信 三、图书馆场景library card借书证; borrow借入; lend借出;keep持有,保管,存放; renew续借; return归还;overdue借书逾期; pay a fine交罚款; librarian图书管理员; bookshelf书架;novel小说; science fiction料幻小说;magazine杂志; periodical期刊; reference book参考书; 四、校园场景tuition学费; scholarship奖学金;term/semester学期: seminar研讨班;presentation课堂发言;assignment/homework作业; register注册; be absent缺席; final exam期末考试;review/go over复习; fail/pass不及格/通过;cut a class逃课: graduate毕业; stationery文具;essay/paper论文; paper试卷;attend a lecture听讲座; textbook课本; classroom教室; laboratory实验室；mark/grade成绩; course课程; subject学科； playground操场 五、医院及健康场景doctor医生; dentist牙医; physician内科医生;surgeon外科医生; nurse护士;hospital医院; clinic诊所; Health Center医疗中心;emergeney room/ department急诊室;emergent treatment急教处理; first aid急救; treatment治疗手段;check up检查; physical examination体检;take one’s temperature / blood pressure测量体温/血压;show one’s tongue伸出舌头; visiting hours探视时间; write a prescription开处方; give an injection打针；medicine药; pills /tablets药丸/药片;aspirin阿司匹林; suffer from受….苦;pain疼痛; headache头痛; stomachache肚子痛;backache背痛; sore-throat喉咙痛;cough咳嗽; fever发烧; cold /flu感冒/流感 have/catch a cold患感冒; heart attack心脏病; dizxy眩晕的-What’s up? / How is it going? / How are you?-I’m fine. /I feel good/ terrific. /I couldn’t be better. /Nothing is very wrong with me.好。I am not feeling good./I feel terrible/horrible/awful. /I am not myself these days.不好。（注意听语气：身体好的时候，语调上扬，语气非常欢快；身体不好的时候，降调，语气非常郁闷.） 六、电话场景operator接线员Extension six two two six, please.请转6226The line is bad/ busy / engaged. It kept a busy line. 电话占线。 long distance call长途电话collect call对方付费电话; put through接通电话;leave a message留口信;dial the wrong number拨错号码;call telephone/ ring/ phone sb.; give sb. a call ring 给某人打电话; Who’s speaking?/Who is that?您是谁?Hello! This is …speaking.您好,我是…Hold on /Hold the line, please.请稍等,别挂断I’ll call back later / again. I’ll ring him/ her up again. 我一会儿再打。 I couidn’t get through.打不通Sorry, I’m afraid you have the wrong number. 对不起，您拨错号啦。 七、酒店场景make a reservation预定房间: reception desk接待处:reception服务台; receptionist服务员；accommodation住宿; registration登记:check in入住; check out结帐; Identity/ID card身份证: fill out a form填表:single room单间; double room双人间;suite套间; room number房间号;room key房间钥匙; room service客房服务; wake-up call叫醒电话Do you have a reservation Sir?先生,您预订了吗?Have you got any vacant room?-Is there any room available here?有空房间吗? All the room are occupied.房间已满Can I have a suite please?有套间吗?How much do you charge for that?如何收费? Smoking or no smoking?吸烟或不吸烟?Meals included.包括饮食.Can I show you your room?我可以带你去你的房间?Can I carry your luggage?我可以帮你提行李吗? 八、交通场景plane飞机; ship轮船；bus公交车;coach长途客车; train火车;subway/underground地铁;railway铁路; highway公路; sidewalk/pavement人行道; one-way street单行道；rush hour交通高峰期；crowded拥挤的：heavy traffic 拥挤的交通; traffic jam交通阻塞traffic rules交通规则; traffic lights交通灯;driving license驾照; driver司机;garage修车厂、车库; car accident/crash车祸: speed超速;run the red light闯红灯; flat tire爆胎;scratches刮蹭: fix/repair修理: survive幸存by plane/air乘飞机; by sea/land水路、陆路; by subway/underground乘地铁transfer换乘、转机May I see your license, please? 我可以看看你的驾照，好吗？You will be fined by ＄20. 你将处以20元的罚款. 九、机场场景airline航空公司; airway航线; flight航班;book/serve预定: behind schedule晚点;cancel取消: customs海关: security check安检;passport护照: visa签证: departure lounge候机室;departure gate登机口: check in办理登机手续;on board登机: boarding pass/card登机牌;luggage/baggage行李; suitcase行李箱;take off起飞: land降落; safe landing安全着陆;bound for … 飞往…方向去的； departure to前往： destination目的地;departure time起飞时间;see sb. off为某人送行; pick up接机;on board在飞机上; a window sear靠窗的座位;an aisle seat靠走道的座位; captain机长pilot 飞行员; airhostess/stewardess空姐;fasten the safety/safe belt系紧安全带;airsick晕机; jet lag飞机时差反应; 十、在火车站（At the railway station)booking/ticket office售票处;booking/ticket office售票处;conductor售票员; train火车;express train特快列车; carriage车厢;dining car餐车: waiting room候车室:platform站台,月台：one way trip/ticket单程/单程票;round trip/ticket往返/往返票;a hard seat硬座; a soft seat软座 十一、天气场景fine晴朗的 sunny/bright/clear阳光明端的;fine晴朗的 sunny/bright/clear阳光明端的;evercast 多云; cloudy阴; humid湿润的;wet潮湿的; rainy有雨的; clear up天空放晴;freezing-cold冰冷的；hot炎热的; cool凉爽的;warm暖和的: mild温和的; windy有风的; calm无风的; breeze微风; moderate风力不大;strong/ high winds大风; tornado龙卷风;typhoon台风: fog雾: snow雪;drizzle毛毛细雨; light rain小雨; shower阵雨; storm暴风雨; downpour倾盆大雨;It rains cats and dogs.(=The rain is pouring.)下着倾盆大雨thunder打雷; blizzard大风雪: snowstorm暴风雪;weather in London / Seattle意指不好的天气;weather in California意指好天气 经常和天气相联系的情况vacation (假期)和flight(航班)be delayed延误/cancelled取消by the bad weatherput away clothes下雨收衣服; 十二、购物场景store商店; grocery食品店department store百货商场;shopping center/mall购物中心shop assistant店员;counter柜台: receipt收据; deliver送货: refund退款;bargain便宣货; out of stock脱销/缺货;in stock有货: selling season销售旺季;on sale打折; 50% off打五折: a 30% discount打七折; catalogue产品目录; commodity information商品信息;brand品牌; size尺码; color颜色: style样式: price价格:What color/size/kind do you want? 您需要什么样的颜色/多大尺寸/哪种？second-hand二手的: popular / fashionable流行的;in fashion流行,时尚: out of fashion过时的; 商品论贵贱expensive, cheap:价格论高低high, lowpay in cash用现金支付: pay in check用支票支付;credit card信用卡: pay by installment分期付款; 在服装店（in a clothing store/shop）trousers裤子; clothes衣服; skirt裙子shirt树衫: jacket夹克: coat上衣 a bigger size稍大一点尺寸;what size多大尺寸: this style这种式样;fit you合你身; try on试穿; match相配 十三、银行场景business hour营业时间: deposit存款;withdraw取款: open an account开户;savings account储蓄账户;check account支账户;interest rate利率; cash a check兑现支票 十四、 常见的身份与职业part-time job兼职工作; full-time job全职工作psychologist心理学家; principal校长；professor教授; postman邮递员;driver司机: policeman警察; firefighter消防员; assistant助手/助教 ;receptionist接待员; typist打字员;cashier收纳; waiter服务员;passenger乘客: ticket seller售票员;conductor售票员,列车长;salesman销售员; nurse护士;housewite家庭主妇: lawyer律师; engineer工程师; architect建筑师；chef厨师: guide导游;reporterjournalist记者; editor编辑;author作家; pianist钢琴家; musician音乐家; singer歌手;photographer摄影师: artist艺术家;designer设计师; scientist科学家；barber理发师; butcher屠夫; tailor裁缝 十五、人物关系relatioiship.strangers陌生人; neigbbors邻居,classmates同学; colleaguesico-workers同事;friends朋友; husband and wife夫妻guest and receptionist客人与接待员: doctor and patient医生与病人;teacher and student师生;boss and secretary老板和秘书; policeman and driver警察与司机interviewer and interviewee面试官与面试者;employer and employee雇主与员工:shop assistant/waiter and customer营业员与顾客;parent and child父母与子女 十六、工作job opportanity 工作机会; interview面试;resume简历; educational background教育背景;application letter求职信; recommendation推荐信;certificate助手: qualification资历; experience经验; position职位; salary薪水;allowance津贴: promotion晋升: vacation休假;break短暂休息: retire退休; work overtime加班;shift work倒班工作; ask for leave请假; sick leave病假: have a week off休一周假;clerk职员: secretary秘书; manager经理;HR manager人事经理: general manager总经理:client客户 十七、兴趣爱好hobby兴趣 entertainment娱乐;go to the cinema/movies/theatre去看电影;comedy喜剧: tragedy悲剧action movie动作片: cartoon动画片 documentary纪录片, thriller惊惊片;horror film恐怖片: detective movie侦探片;western film西部片；soap opera肥皂剧 hiking徒步旅行: camping野营:diving潜水; surfing冲浪: sking滑雪;seashore海岸: beach海滩;sun-burnt晒黑的: palace宫殿; castle城堡 十八、运动competition竞赛; sports field运动场club俱乐部: sportsman/athlete/player运动员:coach教练: champion冠军;gold/silver/bronze medal金/银/铜牌; basketball篮球 football美式足球soccer英式足球： volleyball 排球；baseball棒球; tennis网球;table tennis乒乓球; golf高尔夫; jogging慢跑, swimming游泳wrestling摔跤: shooting射击boxing拳击: weightlifting重; relay race接力赛break the world record打破世界纪录 十九、观光旅游vacation/holiday假日;historical spots/relics古迹: scenery风景;museum博物馆; mountain山脉; river河流lake湖泊; waterfall瀑布: beach沙滩; island岛屿; lawn草地; temple寺庙; zoo动物园travel/trip/tour/voyage旅行; travel ageney旅行社:a guided tour有导游的旅行: go sightseeing观光;hotel宾馆; souvenir纪念品; ticket票 景点评论:fascinating/breathtaking激动人心的;thrilling刺激的; refreshing清爽的:unforgettable难忘的; terrific/ fantastic 极好的; terrible 非常糟糕的 二十、租房landlord 房东; landlady女房东:rent 租/租金; deposit押金; contract合同;furnished有家具电器的; furniture家具;heat暖气: gas煤气: electricity电; lift电梯; stairs楼梯; baicony阳台living room起居室: guest room客厅:dinning room餐厅: kitchen厨房;bathroom浴室； toilet卫生间； basement地下室 二十一、日常生活housework家务; channel频道;cook烹饪; dining table饭桌: fridge冰箱;television电视; oven烤箱: cabinet橱柜; dishwasher洗碗机; sheet床单;blanket毯子; carpet地毯; curtain窗帘;towel毛巾; shampoo洗发水;soap肥皂: haircut理发: shaver剃须刀","categories":[],"tags":[{"name":"English","slug":"English","permalink":"https://qikaile.us/volantis/tags/English/"}]},{"title":"免费申请 Office365 E5 开发者订阅","slug":"Office365-E5","date":"2020-11-19T02:28:03.000Z","updated":"2020-11-26T04:04:03.860Z","comments":true,"path":"posts/61191.html","link":"","permalink":"https://qikaile.us/volantis/posts/61191.html","excerpt":"","text":"Office 365 开发版 E5 是为开发人员提供的微软官方活动，申请Office 365开发者计划，可以获得为期3个月的免费Office 365 E5。（注：之前是E3，一年时间的订阅现在更改为E5了）。目前注册该试用版有效期为90天，只要90天内需要证明是开发者理论上无限期续订。 Office 365 开发版 E5最大可以支持25个订阅者，每一个订阅者都可以获得5T的OneDrive网盘。（后期还可以扩展到25T） 准备：一个微软账号，如果没有，点击下面，注册outlook邮箱就可以了。https://outlook.live.com/owa/ 账号格式：&#x78;&#x78;&#x78;&#64;&#x6f;&#x75;&#116;&#x6c;&#111;&#111;&#x6b;&#46;&#99;&#111;&#x6d; 或者 xxx @hotmail.com 申请教程 首先进入【Office开发人员中心】https://developer.microsoft.com/zh-cn/office 登录你的微软帐号之后，点击立即加入 开通Office开发者账号，并且完善个人信息；设置好之后，下一步，开始勾选，具体如图： 进入个人中心，点击设置订阅； 填写设置开发者订阅相关的信息；注：保存信息，以后登录控制台管理用到账号 用户名@域名.onmicrosoft.com 设置手机号验证（需要代理翻墙）； 验证成功后，自动开始设置订阅； 管理地址控制台管理地址：https://portal.office.com/AdminPortal/Home注意：登录账号如要管理员账号（第3步设置开发人员订阅中设置的 用户名@域名.onmicrosoft.com），登录以后可以查看订阅，进行管理用户等功能！ 在这里登录以后可以查看订阅，进行管理用户等功能！ 1T扩招到5T的教程如果是容量是1T，先不要增加订阅者，方法1、登陆https://admin.onedrive.com/?v=StorageSettings 修改为 5012GBonedrive容量填写5012GB才是5T（可能计算方法不一样，不是1024*5=5120T！） ★方法2.使用 PowerShell 设置 OneDrive 存储空间 下载 SharePoint Online Management Shell 并安装https://www.microsoft.com/zh-cn/download/details.aspx?id=35588 运行代码检测安装是否成功 1Get-Module -Name Microsoft.Online.SharePoint.PowerShell -ListAvailable | Select Name,Version 使用管理员用户名和密码连接，运行如下代码： 12345678910111213141516171819201、$adminUPN&#x3D;&quot;TJYS@qikaile.onmicrosoft.com&quot;2、$orgName&#x3D;&quot;qikaile&quot;3、$userCredential &#x3D; Get-Credential -UserName $adminUPN -Message &quot;Type the password.&quot;4、Connect-SPOService -Url https:&#x2F;&#x2F;$orgName-admin.sharepoint.com -Credential $userCredential\\# adminUPN 是管理员邮箱，orgName 是你设置的组织名，将其替换即可5、然后会弹出一个窗口，会要求帐号的密码6、运行以下命令，对用户进行升级扩容：Set-SPOSite -Identity &lt;user&#39;s OneDrive URL&gt; -StorageQuota\\# &lt;user&#39;s OneDrive URL&gt; 更改为你要扩容的用户URL\\# 是储存空间值，以MB为单位（5242880MB&#x3D;5TB）Set-SPOSite -Identity https:&#x2F;&#x2F;qikaile-my.sharepoint.com&#x2F;personal&#x2F;TJYS_qikaile_onmicrosoft_com -StorageQuota 5242880 注意：上面几个标记的要改为你自己的，红色(TJYS)的为你的用户名，蓝色(qikaile)为组织名。 保持活跃因为注册该试用版有效期为90天，90天内需要证明是开发者才能自动续订。目前可以安装oneindex来保持活跃，还可以通过一个开源程序AutoApiSecret来续订。原理，因为该程序会触及API相关信息因此可以证明是开发者并保持活跃自动更新，所以说，只要能触及api的程序都是有效的。 如何登陆使用：1.网页https://login.microsoftonline.com/ 2.本地客户端https://www.onedrive.com/download （如果该页面无法打开，建议直接在百度上下载onedrive应用）","categories":[],"tags":[{"name":"Office","slug":"Office","permalink":"https://qikaile.us/volantis/tags/Office/"}]},{"title":"Shotcut","slug":"Shotcut","date":"2020-11-19T01:09:28.000Z","updated":"2020-11-19T02:36:22.370Z","comments":true,"path":"posts/48498.html","link":"","permalink":"https://qikaile.us/volantis/posts/48498.html","excerpt":"","text":"免费开源剪辑软件Shotcut推荐和使用教程 shotcut是一个免费、开源、跨平台的视频编辑软件，功能丰富强大，能够满足绝大多数情况下对视频编辑的需&gt;求，下面看看它如何使用吧。 1、下载软件去官网下载软件，支持Linux、MacOS和Windows平台。下载链接 2、打开软件不同的平台有不同的安装方式，根据自己平台来安装即可。安装后启动软件。官网进去是英文版的，软件下载下来界面有中文版。【设置】——【语言】 3、新建工程选择存放工程的目录，设置工程名称（个人随意设置），选择视频模式，点【开始】后新的工程就建好了。 4、导入素材1）点【打开文件】载入视频文件、音频文件、图片等等。或者将需要剪辑的素材拖动到【播放列表】区域，将素材导入到软件。 5、添加视频轨或音频轨在时间线下面的【三横线符号】选择添加轨道，点击+号可以将选中的素材添加进来，【删除】或-号可以删除轨道中选中的素材。","categories":[],"tags":[]},{"title":"Python绘图","slug":"Python-matplotlib-Seaborn","date":"2020-11-19T00:55:33.000Z","updated":"2020-11-19T02:36:23.836Z","comments":true,"path":"posts/16997.html","link":"","permalink":"https://qikaile.us/volantis/posts/16997.html","excerpt":"","text":"Matplotlib针对python不显示汉字解决方案import matplotlib.pyplot as pltplt.rcParams[‘font.sans-serif’]=[‘SimHei’] 在绘图结构中，figure创建窗口，subplot创建子图。 所有的绘画只能在子图上进行。plt表示当前子图，若没有就创建一个子图。 Figure：面板(图)，matplotlib中的所有图像都是位于figure对象中，一个图像只能有一个figure对象。 Subplot：子图，figure对象下创建一个或多个subplot对象(即axes)用于绘制图像。 配置参数 figure: 控制dpi、边界颜色、图形大小、和子区( subplot)设置 font: 字体集（font family）、字体大小和样式设置 grid: 设置网格颜色和线性 legend: 设置图例和其中的文本的显示 line: 设置线条（颜色、线型、宽度等）和标记 savefig: 可以对保存的图形进行单独设置。例如，设置渲染的文件的背景为白色。 xticks和yticks: 为x,y轴的主刻度和次刻度设置颜色、大小、方向，以及标签大小。 线条相关属性标记设置 线形：linestyle或ls ‘-‘ : 实线 ‘–’ : 虚线 ‘None’,’ ‘,’’ : 什么都不画 ‘-.’ : 点划线 点型：maker ‘o’ :圆圈 ‘.’ :点 ‘D’ :菱形 ‘s’ :正方形 ‘h’ :六边形1 ‘*’ :星号 ‘H’ :六边形2 ‘d’ :小菱形 ‘_’ :水平线 ‘v’ :一角朝下的三角形 ‘8’ :八边形 ‘&lt;’ :一角朝左的三角形 ‘p’ :五边形 ‘&gt;’ :一角朝右的三角形 ‘,’ :像素 ‘^’ :一角朝上的三角形 ‘+’ :加号 ‘’ :竖线 ‘None’,’ ‘,’’ : 无 ‘x’ :X 颜色：b:蓝色g:绿色r:红色y:黄色c:青色k:黑色m:洋红色w:白色 1、线图 plot()1234567891011121314151617181920import numpy as npimport matplotlib.pyplot as pltplt.rcParams[&#x27;font.sans-serif&#x27;] = [&#x27;SimHei&#x27;] # 解决中文显示问题-设置字体为黑体plt.rcParams[&#x27;axes.unicode_minus&#x27;] = False # 解决保存图像是负号&#x27;-&#x27;显示为方块的问题x = np.arange(-2*np.pi, 2*np.pi, 0.01)y1 = np.sin(x)y2 = np.cos(x)plt.figure(figsize=(10, 7))plt.plot(x, y1 ,&#x27;r-&#x27;,label=&#x27;$sinx$&#x27;)plt.plot(x, y2 ,&#x27;b--&#x27;,label=&#x27;$cosx$&#x27;)plt.legend(loc=&#x27;upper right&#x27;)plt.xlim(-2*np.pi-1, 2*np.pi+3)plt.xticks([-2*np.pi, -np.pi, 0, np.pi, 2*np.pi], [&#x27;$-2\\pi$&#x27;, &#x27;$-\\pi$&#x27;, &#x27;$0$&#x27;, &#x27;$\\pi$&#x27;, &#x27;$2\\pi$&#x27;])plt.title(&#x27;三角-函数&#x27;)plt.xlabel(&#x27;横坐标&#x27;)plt.ylabel(&#x27;纵坐标&#x27;)plt.axhline(y=0, c=&#x27;black&#x27;)plt.show() ** plot()参数** plot([x], y, [fmt], data=None, **kwargs) 可选参数[fmt] 是一个字符串来定义图的基本属性如：颜色（color），点型（marker），线型（linestyle）， 具体形式 fmt = ‘[color][marker][line]’ fmt接收的是每个属性的单个字母缩写，例如： plot(x, y, ‘bo-‘) # 蓝色圆点实线 2、散点图12345678910111213# 散点图import matplotlib.pyplot as pltplt.rcParams[&#x27;font.sans-serif&#x27;]=[&#x27;SimHei&#x27;] #显示中文fig = plt.figure(figsize=(6,5))x = [1.5, 1, 1, 2, 2, 3, 3, 4, 4, 5, 5]y = [6, 7, 8, 5, 9, 4, 9.5, 3, 9.5, 2, 9]x1 = [6, 6, 7, 7, 8, 8, 9, 9, 10, 10, 10.5, 11, 11]y1 = [1, 8, 2, 9, 3, 9, 4, 9.5, 5, 9, 6, 7, 8]plt.scatter(x, y, color=&#x27;r&#x27;, label=&#x27;左心房&#x27;)plt.scatter(x1, y1, color=&#x27;b&#x27;, label=&#x27;右心房&#x27;)plt.title(&#x27;love&#x27;)plt.legend()plt.show() 3、条形图123456789101112131415161718192021222324252627282930313233343536373839import matplotlibimport matplotlib.pyplot as pltimport numpy as nplabels = [&#x27;Monday&#x27;,&#x27;Tuesday&#x27;,&#x27;Friday&#x27;,&#x27;Sunday&#x27;]men_means = [1.5,0.6,7.8,6]women_means = [1,2,3,1]x = np.arange(len(labels)) # the label locationswidth = 0.35 # the width of the barsfig, ax = plt.subplots()rects1 = ax.bar(x - width/2, men_means, width, label=&#x27;boy&#x27;)rects2 = ax.bar(x + width/2, women_means, width, label=&#x27;girl&#x27;)# Add some text for labels, title and custom x-axis tick labels, etc.ax.set_ylabel(&#x27;Scores&#x27;) #标题ax.set_title(&#x27;Scores by group and gender&#x27;) #标题ax.set_xticks(x)ax.set_xticklabels(labels)ax.legend()def autolabel(rects): &quot;&quot;&quot;Attach a text label above each bar in *rects*, displaying its height.&quot;&quot;&quot; for rect in rects: height = rect.get_height() ax.annotate(&#x27;&#123;&#125;&#x27;.format(height), xy=(rect.get_x() + rect.get_width() / 2, height), xytext=(0, 3), # 3 points vertical offset textcoords=&quot;offset points&quot;, ha=&#x27;center&#x27;, va=&#x27;bottom&#x27;)autolabel(rects1)autolabel(rects2)fig.tight_layout()plt.show() 12345678910111213141516171819202122232425262728293031323334353637383940import matplotlibimport matplotlib.pyplot as pltimport numpy as np labels = [&#x27;Monday&#x27;,&#x27;Tuesday&#x27;,&#x27;Friday&#x27;]men_means = [1.5,0.6,7]women_means = [1,2,3]child=[1,1,2] x = np.arange(len(labels)) # the label locationswidth = 0.35 # the width of the bars fig, ax = plt.subplots()rects1 = ax.bar(x - 3*width/4, men_means, 3*width/4, label=&#x27;boy&#x27;)rects2 = ax.bar(x, women_means,3*width/4, label=&#x27;girl&#x27;)rects3 = ax.bar(x + 3*width/4, child, 3*width/4, label=&#x27;child&#x27;) # Add some text for labels, title and custom x-axis tick labels, etc.ax.set_ylabel(&#x27;Scores&#x27;) #标题ax.set_title(&#x27;Scores by group and gender&#x27;) #标题ax.set_xticks(x)ax.set_xticklabels(labels)ax.legend() def autolabel(rects): &quot;&quot;&quot;Attach a text label above each bar in *rects*, displaying its height.&quot;&quot;&quot; for rect in rects: height = rect.get_height() ax.annotate(&#x27;&#123;&#125;&#x27;.format(height), xy=(rect.get_x() + rect.get_width() / 2, height), xytext=(0, 3), # 3 points vertical offset textcoords=&quot;offset points&quot;, ha=&#x27;center&#x27;, va=&#x27;bottom&#x27;) autolabel(rects1)autolabel(rects2)autolabel(rects3)fig.tight_layout()plt.show() 4、直方图直方图与条形图基本类似，不过直方图通常用来对单个数据的单一属性进行描述，而不是用于比较 data:必选参数，绘图数据 bins:直方图的长条形数目，可选项，默认为10 normed:是否将得到的直方图向量归一化，可选项，默认为0，代表不归一化，显示频数。normed=1，表示归一化，显示频率。 facecolor:长条形的颜色 edgecolor:长条形边框的颜色 alpha:透明度 12345678910import matplotlib.pyplot as pltplt.rcParams[&#x27;font.sans-serif&#x27;] = [&#x27;SimHei&#x27;] # 中文显示plt.rcParams[&#x27;axes.unicode_minus&#x27;]=False # 正常显示负号import numpy as npdata = np.random.randn(10000)plt.hist(data, bins=40, normed=0, facecolor=&#x27;blue&#x27;, edgecolor=&#x27;black&#x27;, alpha=0.7)plt.xlabel(&#x27;区间&#x27;)plt.ylabel(&#x27;频数&#x27;)plt.title(&#x27;频数分布直方图&#x27;)plt.show() 5、饼图 x :(每一块)的比例，如果sum(x) &gt; 1会使用sum(x)归一化； labels :(每一块)饼图外侧显示的说明文字； explode :(每一块)离开中心距离； startangle :起始绘制角度,默认图是从x轴正方向逆时针画起,如设定=90则从y轴正方向画起； shadow :在饼图下面画一个阴影。默认值：False，即不画阴影； autopct :控制饼图内百分比设置,可以使用format字符串或者format function ‘%1.1f’指小数点前后位数(没有用空格补齐)； 123456789import matplotlib.pyplot as pltplt.rcParams[&#x27;font.sans-serif&#x27;] = [&#x27;SimHei&#x27;] # 中文显示plt.rcParams[&#x27;axes.unicode_minus&#x27;]=False # 正常显示负号labels = [&#x27;育儿&#x27;,&#x27;饮食&#x27;,&#x27;房贷&#x27;,&#x27;其他&#x27;]x = [5,12,50,9]explode = (0,0,0.01,0) #0.01调整中间空格大小plt.pie(x,labels=labels,explode=explode,autopct=&#x27;%1.1f%%&#x27;)plt.title(&#x27;家庭支出比例&#x27;)plt.show() 123456789101112131415161718192021222324252627282930import numpy as npimport matplotlib.pyplot as pltfig, ax = plt.subplots(figsize=(6, 3), subplot_kw=dict(aspect=&quot;equal&quot;))recipe = [&quot;375 g flour&quot;, &quot;75 g sugar&quot;, &quot;250 g butter&quot;, &quot;300 g berries&quot;]data = [float(x.split()[0]) for x in recipe]ingredients = [x.split()[-1] for x in recipe]def func(pct, allvals): absolute = int(pct/100.*np.sum(allvals)) return &quot;&#123;:.1f&#125;%\\n(&#123;:d&#125; g)&quot;.format(pct, absolute)wedges, texts, autotexts = ax.pie(data, autopct=lambda pct: func(pct, data), textprops=dict(color=&quot;w&quot;))ax.legend(wedges, ingredients, title=&quot;Ingredients&quot;, loc=&quot;center left&quot;, bbox_to_anchor=(1, 0, 0.5, 1))plt.setp(autotexts, size=8, weight=&quot;bold&quot;)ax.set_title(&quot;Matplotlib bakery: A pie&quot;)plt.show() 1234567891011121314151617181920212223242526272829303132import numpy as npimport matplotlib.pyplot as pltfig, ax = plt.subplots(figsize=(6, 3), subplot_kw=dict(aspect=&quot;equal&quot;))recipe = [&quot;225 g flour&quot;, &quot;90 g sugar&quot;, &quot;1 egg&quot;, &quot;60 g butter&quot;, &quot;100 ml milk&quot;, &quot;1/2 package of yeast&quot;]data = [225, 90, 50, 60, 100, 5]wedges, texts = ax.pie(data, wedgeprops=dict(width=0.5), startangle=-40)bbox_props = dict(boxstyle=&quot;square,pad=0.3&quot;, fc=&quot;w&quot;, ec=&quot;k&quot;, lw=0.72)kw = dict(arrowprops=dict(arrowstyle=&quot;-&quot;), bbox=bbox_props, zorder=0, va=&quot;center&quot;)for i, p in enumerate(wedges): ang = (p.theta2 - p.theta1)/2. + p.theta1 y = np.sin(np.deg2rad(ang)) x = np.cos(np.deg2rad(ang)) horizontalalignment = &#123;-1: &quot;right&quot;, 1: &quot;left&quot;&#125;[int(np.sign(x))] connectionstyle = &quot;angle,angleA=0,angleB=&#123;&#125;&quot;.format(ang) kw[&quot;arrowprops&quot;].update(&#123;&quot;connectionstyle&quot;: connectionstyle&#125;) ax.annotate(recipe[i], xy=(x, y), xytext=(1.35*np.sign(x), 1.4*y), horizontalalignment=horizontalalignment, **kw)ax.set_title(&quot;Matplotlib bakery: A donut&quot;)plt.show() Seaborn1、线图plot()12345678910111213141516171819202122import numpy as npimport matplotlib.pyplot as pltimport seaborn as snsplt.rcParams[&#x27;font.sans-serif&#x27;] = [&#x27;SimHei&#x27;] # 中文字体设置-黑体plt.rcParams[&#x27;axes.unicode_minus&#x27;] = False # 解决保存图像是负号&#x27;-&#x27;显示为方块的问题sns.set(font=&#x27;SimHei&#x27;) # 解决Seaborn中文显示问题x = np.arange(-2*np.pi, 2*np.pi, 0.01)y1 = np.sin(x)y2 = np.cos(x)plt.figure(figsize=(10, 7))plt.plot(x, y1,&#x27;r-&#x27;,label=&#x27;$sinx$&#x27;)plt.plot(x, y2,&#x27;b--&#x27;,label=&#x27;$cosx$&#x27;)plt.legend(loc=&#x27;upper right&#x27;)plt.xlim(-2*np.pi-1, 2*np.pi+3)plt.xticks([-2*np.pi, -np.pi, 0, np.pi, 2*np.pi], [&#x27;$-2\\pi$&#x27;, &#x27;$-\\pi$&#x27;, &#x27;$0$&#x27;, &#x27;$\\pi$&#x27;, &#x27;$2\\pi$&#x27;])plt.title(&#x27;三角-函数&#x27;)plt.xlabel(&#x27;横坐标&#x27;)plt.ylabel(&#x27;纵坐标&#x27;)plt.axhline(y=0, c=&#x27;black&#x27;)plt.show() 4、直方图12345678910111213import numpy as npimport matplotlib.pyplot as pltimport seaborn as snsplt.rcParams[&#x27;font.sans-serif&#x27;] = [&#x27;SimHei&#x27;] # 中文显示plt.rcParams[&#x27;axes.unicode_minus&#x27;]=False # 正常显示负号sns.set(font=&#x27;SimHei&#x27;) # 解决Seaborn中文显示问题np.random.seed(sum(map(ord, &quot;distributions&quot;)))x = np.random.normal(size=100)sns.distplot(x)plt.xlabel(&#x27;区间&#x27;)plt.ylabel(&#x27;频数&#x27;)plt.title(&#x27;频数分布直方图&#x27;)plt.show()","categories":[],"tags":[{"name":"Python","slug":"Python","permalink":"https://qikaile.us/volantis/tags/Python/"}]},{"title":"机器学习课程","slug":"Machine-learning-note","date":"2020-11-18T06:16:05.000Z","updated":"2020-11-18T06:26:21.013Z","comments":true,"path":"posts/17213.html","link":"","permalink":"https://qikaile.us/volantis/posts/17213.html","excerpt":"","text":"机器学习概述什么是机器学习机器学习是从**数据中自动分析获得规律(模型)，并利用规律对未知数据进行预测**。 机器学习算法分类监督学习目标值：类别—分类问题 KNN、贝叶斯分类、决策树与随机森林、逻辑回归 目标值：连续型数据—回归问题 线性回归、岭回归 无监督学习目标值：无 聚类K-means 机器学习开发流程1）获取数据 2）数据处理 3）特征工程 4）机器学习算法训练—模型 5）模型评估 6）应用 特征工程数据集 数据———数据集的构成———特征值 + 目标值 可用数据集Kaggle特点：1、大数据竞赛平台 2、80万科学家 3、真实数据 4、数据量巨大 UCI特点：1、收录了360个数据集 2、覆盖科学、生活、经济等领域 3、数据量几十万 scikit-learn特点：1、数据量较小 2、方便学习 网址： Kaggle网址：https://www.kaggle.com/datasets UCI数据集网址： http://archive.ics.uci.edu/ml/ scikit-learn网址：http://scikit-learn.org/stable/datasets/index.html#datasets sklearn数据集load_*小规模的数据集 fetch_*大规模的数据集 Bunch类型 数据集划分—model_selection.train_test_split() 训练数据：用于训练，构建模型 测试数据：在模型检验时使用，用于评估模型是否有效 特征工程介绍特征工程是将原始数据转换为更好地代表预测模型的潜在问题的特征的过程，从而提高了对未知数据的模型准确性。 sklearn 特征工程 Scikit-learn包含的内容：分类、聚类、回归、特征工程、模型选择和调优。 pandas 数据清洗、数据处理 特征处理**是特征工程的核心部分，包括特征提取、数据预处理、特征选择、特征降维**等。 特征提取特征提取包括将任意数据（如文本或图像）转换为可用于机器学习的数字特征。注：特征值化是为了计算机更好的去理解数据 包：sklearn.feature_extraction 字典特征提取 应用DictVectorizer实现对类别特征进行数值化、离散化 sklearn.feature_extraction.DictVectorizer(sparse=True,…)DictVectorizer.fit_transform(X) X:字典或者包含字典的迭代器返回值：返回sparse矩阵DictVectorizer.inverse_transform(X) X:array数组或者sparse矩阵 返回值:转换之前数据格式DictVectorizer.get_feature_names() 返回类别名称应用： 代码： 12345678910#对字典类型的数据进行特征抽取from sklearn.feature_extraction import DictVectorizerdata = [&#123;&#x27;city&#x27;: &#x27;北京&#x27;,&#x27;temperature&#x27;:100&#125;, &#123;&#x27;city&#x27;: &#x27;上海&#x27;,&#x27;temperature&#x27;:60&#125;, &#123;&#x27;city&#x27;: &#x27;深圳&#x27;,&#x27;temperature&#x27;:30&#125;]# 1、实例化一个转换器类transfer = DictVectorizer(sparse=False)# 2、调用fit_transformdata = transfer.fit_transform(data)print(&quot;返回的结果:\\n&quot;, data)# 打印特征名字print(&quot;特征名字：\\n&quot;, transfer.get_feature_names()) 输出结果： 123456返回的结果: [[ 0. 1. 0. 100.] [ 1. 0. 0. 60.] [ 0. 0. 1. 30.]]特征名字： [&#x27;city=上海&#x27;, &#x27;city=北京&#x27;, &#x27;city=深圳&#x27;, &#x27;temperature&#x27;] 文本特征提取 独热编码（One-HotEncoding） 应用CountVectorizer实现对文本特征进行数值化 应用TfidfVectorizer(TF-IDF)实现对文本特征进行数值化 sklearn.feature_extraction.text.CountVectorizer(stop_words=[]) 返回词频矩阵CountVectorizer.fit_transform(X) X:文本或者包含文本字符串的可迭代对象 返回值：返回sparse矩阵CountVectorizer.inverse_transform(X) X:array数组或者sparse矩阵 返回值:转换之前数据格CountVectorizer.get_feature_names() 返回值:单词列表sklearn.feature_extraction.text.TfidfVectorizer TF-IDF的主要思想是：如果某个词或短语在一篇文章中出现的概率高，并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类。 TF-IDF作用：用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。 12345678from sklearn.feature_extraction.text import TfidfVectorizer# 对于文本数据，进行特征抽取tf = TfidfVectorizer()x_train = tf.fit_transform(x_train)#20类新闻分类数据集#这里打印出来的列表是：训练集当中的所有不同词的组成的一个列表print(tf.get_feature_names())# print(x_train.toarray())x_test = tf.transform(x_test) # 不需要fit_transform 应用 代码： 1234567891011121314151617181920212223from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizerimport jiebadef cut_word(text): # 用jieba对中文字符串进行分词 text = &quot; &quot;.join(list(jieba.cut(text))) return text#对中文进行特征抽取data = [&quot;一种还是一种今天很残酷，明天更残酷，后天很美好，但绝对大部分是死在明天晚上，所以每个人不要放弃今天。&quot;, &quot;我们看到的从很远星系来的光是在几百万年之前发出的，这样当我们看到宇宙时，我们是在看它的过去。&quot;, &quot;如果只用一种方式了解某样事物，你就不会真正了解它。了解事物真正含义的秘密取决于如何将其与我们所了解的事物相联系。&quot;, &quot;life is short,i like like python&quot;, &quot;life is too long,i dislike python&quot;]# 将原始数据转换成分好词的形式text_list = []for sent in data: text_list.append(cut_word(sent))print(text_list)# 1、实例化一个转换器类transfer = CountVectorizer()#transfer = TfidfVectorizer()# 2、调用fit_transformdata = transfer.fit_transform(text_list)print(&quot;文本特征抽取的结果：\\n&quot;, data.toarray())print(&quot;返回特征名字：\\n&quot;, transfer.get_feature_names()) 输出结果： 1234567891011121314[&#x27;一种 还是 一种 今天 很 残酷 ， 明天 更 残酷 ， 后天 很 美好 ， 但 绝对 大部分 是 死 在 明天 晚上 ， 所以 每个 人 不要 放弃 今天 。&#x27;, &#x27;我们 看到 的 从 很 远 星系 来 的 光是在 几百万年 之前 发出 的 ， 这样 当 我们 看到 宇宙 时 ， 我们 是 在 看 它 的 过去 。&#x27;, &#x27;如果 只用 一种 方式 了解 某样 事物 ， 你 就 不会 真正 了解 它 。 了解 事物 真正 含义 的 秘密 取决于 如何 将 其 与 我们 所 了解 的 事物 相 联系 。&#x27;, &#x27;life is short , i like like python&#x27;, &#x27;life is too long , i dislike python&#x27;]文本特征抽取的结果： [[0 0 0 0 0 0 0 0 2 0 1 0 0 0 2 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 2 0 1 0 2 1 0 0 0 1 1 0 0 1 0] [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 1 3 0 0 0 0 1 0 0 0 0 2 0 0 0 0 0 1 0 1] [0 0 0 0 0 0 0 0 1 1 0 0 4 3 0 0 0 0 1 1 0 1 0 1 1 0 1 0 0 1 0 0 0 1 0 0 0 2 1 0 0 1 0 0 0] [0 1 1 2 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [1 1 1 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]返回特征名字： [&#x27;dislike&#x27;, &#x27;is&#x27;, &#x27;life&#x27;, &#x27;like&#x27;, &#x27;long&#x27;, &#x27;python&#x27;, &#x27;short&#x27;, &#x27;too&#x27;, &#x27;一种&#x27;, &#x27;不会&#x27;, &#x27;不要&#x27;, &#x27;之前&#x27;, &#x27;了解&#x27;, &#x27;事物&#x27;, &#x27;今天&#x27;, &#x27;光是在&#x27;, &#x27;几百万年&#x27;, &#x27;发出&#x27;, &#x27;取决于&#x27;, &#x27;只用&#x27;, &#x27;后天&#x27;, &#x27;含义&#x27;, &#x27;大部分&#x27;, &#x27;如何&#x27;, &#x27;如果&#x27;, &#x27;宇宙&#x27;, &#x27;我们&#x27;, &#x27;所以&#x27;, &#x27;放弃&#x27;, &#x27;方式&#x27;, &#x27;明天&#x27;, &#x27;星系&#x27;, &#x27;晚上&#x27;, &#x27;某样&#x27;, &#x27;残酷&#x27;, &#x27;每个&#x27;, &#x27;看到&#x27;, &#x27;真正&#x27;, &#x27;秘密&#x27;, &#x27;绝对&#x27;, &#x27;美好&#x27;, &#x27;联系&#x27;, &#x27;过去&#x27;, &#x27;还是&#x27;, &#x27;这样&#x27;] 图像特征提取（深度学习将介绍） 数据预处理去除唯一属性唯一属性通常是一些id属性，这些属性并不能刻画样本自身的分布规律，所以简单地删除这些属性即可。 处理缺失值缺失值处理的三种方法：直接使用含有缺失值的特征；删除含有缺失值的特征（该方法在包含缺失值的属性含有大量缺失值而仅仅包含极少量有效值时是有效的）；缺失值补全。 常见的缺失值补全方法：均值插补、同类均值插补、建模预测、高维映射、多重插补、极大似然估计、压缩感知和矩阵补全。 （1）均值插补 如果样本属性的距离是可度量的，则使用该属性有效值的平均值来插补缺失的值； 如果的距离是不可度量的，则使用该属性有效值的众数来插补缺失的值。如果使用众数插补，出现数据倾斜会造成什么影响？ （2）同类均值插补 首先将样本进行分类，然后以该类中样本的均值来插补缺失值。 （3）建模预测 将缺失的属性作为预测目标来预测，将数据集按照是否含有特定属性的缺失值分为两类，利用现有的机器学习算法对待预测数据集的缺失值进行预测。 该方法的根本的缺陷是如果其他属性和缺失属性无关，则预测的结果毫无意义；但是若预测结果相当准确，则说明这个缺失属性是没必要纳入数据集中的；一般的情况是介于两者之间。 （4）高维映射 将属性映射到高维空间，采用独热码编码（one-hot）技术。将包含K个离散取值范围的属性值扩展为K+1个属性值，若该属性值缺失，则扩展后的第K+1个属性值置为1。 这种做法是最精确的做法，保留了所有的信息，也未添加任何额外信息，若预处理时把所有的变量都这样处理，会大大增加数据的维度。这样做的好处是完整保留了原始数据的全部信息、不用考虑缺失值；缺点是计算量大大提升，且只有在样本量非常大的时候效果才好。 （5）多重插补（MultipleImputation，MI） 多重插补认为待插补的值是随机的，实践上通常是估计出待插补的值，再加上不同的噪声，形成多组可选插补值，根据某种选择依据，选取最合适的插补值。 （6）压缩感知和矩阵补全 （7）手动插补 插补处理只是将未知值补以我们的主观估计值，不一定完全符合客观事实。在许多情况下，根据对所在领域的理解，手动对缺失值进行插补的效果会更好。 特征编码（1）标签处理 通常我们会把字符型的标签转换成数值型的 代码： 123456789101112131415161718import pandas as pddf = pd.DataFrame([ [&#x27;green&#x27;, &#x27;M&#x27;, 10.1, &#x27;class1&#x27;], [&#x27;red&#x27;, &#x27;L&#x27;, 13.5, &#x27;class2&#x27;], [&#x27;blue&#x27;, &#x27;XL&#x27;, 15.3, &#x27;class1&#x27;]])df.columns = [&#x27;color&#x27;, &#x27;size&#x27;, &#x27;prize&#x27;, &#x27;class label&#x27;]#标签处理class_mapping = &#123;label:idx for idx,label in enumerate(set(df[&#x27;class label&#x27;]))&#125;df[&#x27;class label&#x27;] = df[&#x27;class label&#x27;].map(class_mapping)print(df)print(&#x27;-----------------------------------&#x27;)size_mapping = &#123; &#x27;XL&#x27;: 3, &#x27;L&#x27;: 2, &#x27;M&#x27;: 1&#125;df[&#x27;size&#x27;] = df[&#x27;size&#x27;].map(size_mapping)print(df) 输出结果: 123456789 color size prize class label0 green M 10.1 11 red L 13.5 02 blue XL 15.3 1----------------------------------- color size prize class label0 green 1 10.1 11 red 2 13.5 02 blue 3 15.3 1 （2）二值化 二值化的过程是将数值型的属性转换为布尔值的属性，设定一个阈值作为划分属性值为0和1的分隔点。 使用preproccessing库的Binarizer类对数据进行二值化的代码如下： 123from sklearn.preprocessing import Binarizer#二值化，阈值设置为3，返回值为二值化后的数据Binarizer(threshold=3).fit_transform(X) #X=iris.data（鸢尾花）数据集 （3）scikit DictVectorizer 代码： 123456789101112131415161718192021222324252627import pandas as pddf = pd.DataFrame([ [&#x27;green&#x27;, &#x27;M&#x27;, 10.1, &#x27;class1&#x27;], [&#x27;red&#x27;, &#x27;L&#x27;, 13.5, &#x27;class2&#x27;], [&#x27;blue&#x27;, &#x27;XL&#x27;, 15.3, &#x27;class1&#x27;]])df.columns = [&#x27;color&#x27;, &#x27;size&#x27;, &#x27;prize&#x27;, &#x27;class label&#x27;]print(df)print(&#x27;----------------------------------------------------------------------&#x27;)#print(df.transpose().to_dict().values())#print(&#x27;----------------------------------------------------------------------&#x27;)feature = df.iloc[:, :-1]print(feature)print(&#x27;----------------------------------------------------------------------&#x27;)# ②对于x转换成字典数据feature=feature.to_dict(orient=&quot;records&quot;)#feature=feature.transpose().to_dict().values() #对所有的数据都做了映射#使用 DictVectorizer将得到特征的字典from sklearn.feature_extraction import DictVectorizerdvec = DictVectorizer(sparse=False)X = dvec.fit_transform(feature)print(dvec.get_feature_names())print(&#x27;----------------------------------------------------------------------&#x27;)print(X)print(&#x27;----------------------------------------------------------------------&#x27;)#可以调用 get_feature_names 来返回新的列的名字，其中0和1就代表是不是这个属性.data=pd.DataFrame(X, columns=dvec.get_feature_names())print(data) 输出结果: 1234567891011121314151617181920 color size prize class label0 green M 10.1 class11 red L 13.5 class22 blue XL 15.3 class1---------------------------------------------------------------------- color size prize0 green M 10.11 red L 13.52 blue XL 15.3----------------------------------------------------------------------[&#x27;color=blue&#x27;, &#x27;color=green&#x27;, &#x27;color=red&#x27;, &#x27;prize&#x27;, &#x27;size=L&#x27;, &#x27;size=M&#x27;, &#x27;size=XL&#x27;]----------------------------------------------------------------------[[ 0. 1. 0. 10.1 0. 1. 0. ] [ 0. 0. 1. 13.5 1. 0. 0. ] [ 1. 0. 0. 15.3 0. 0. 1. ]]---------------------------------------------------------------------- color=blue color=green color=red prize size=L size=M size=XL0 0.0 1.0 0.0 10.1 0.0 1.0 0.01 0.0 0.0 1.0 13.5 1.0 0.0 0.02 1.0 0.0 0.0 15.3 0.0 0.0 1.0 （4）独热编码（One-HotEncoding） 独热编码采用N位状态寄存器来对N个可能的取值进行编码，每个状态都由独立的寄存器来表示，并且在任意时刻只有其中一位有效。 独热编码的优点：能够处理非数值属性；在一定程度上扩充了特征；编码后的属性是稀疏的，存在大量的零元分量。 代码： 12345678910111213141516import pandas as pddf = pd.DataFrame([ [&#x27;green&#x27;, &#x27;M&#x27;, 10.1, &#x27;class1&#x27;], [&#x27;red&#x27;, &#x27;L&#x27;, 13.5, &#x27;class2&#x27;], [&#x27;blue&#x27;, &#x27;XL&#x27;, 15.3, &#x27;class1&#x27;]])df.columns = [&#x27;color&#x27;, &#x27;size&#x27;, &#x27;prize&#x27;, &#x27;class label&#x27;]#OneHotEncoder 必须使用整数作为输入，所以得先使用scikit LabelEncoder处理一下from sklearn.preprocessing import LabelEncoderclass_le = LabelEncoder()df[&#x27;class label&#x27;] = class_le.fit_transform(df[&#x27;class label&#x27;])print(df)print(&#x27;-----------------------------------&#x27;)from sklearn.preprocessing import OneHotEncoderohe = OneHotEncoder(sparse=False)X = ohe.fit_transform(df[[&#x27;color&#x27;]].values)print(X) 输出结果: 12345678 color size prize class label0 green M 10.1 01 red L 13.5 12 blue XL 15.3 0-----------------------------------[[0. 1. 0.] [0. 0. 1.] [1. 0. 0.]] 注：Pandas库中同样有类似的操作，使用get_dummies也可以得到相应的特征 （5）pandas get_dummies 代码： 12345678import pandas as pddf = pd.DataFrame([ [&#x27;green&#x27;,10.1], [&#x27;red&#x27;, 13.5], [&#x27;blue&#x27;,15.3]])df.columns = [&#x27;color&#x27;, &#x27;prize&#x27;]df=pd.get_dummies(df)print(df) 输出结果: 1234 prize color_blue color_green color_red0 10.1 0 1 01 13.5 0 0 12 15.3 1 0 0 无量纲化、正则化无量纲化数据标准化是将样本的属性缩放到某个指定的范围。 标准化：基于原始数据的均值（mean）和标准差（standarddeviation）进行数据的标准化。 要求 均值$\\mu = 0$ 和标准差 $\\sigma = 1$ 公式表达为：$$\\begin{equation} z = \\frac{x - \\mu}{\\sigma}\\end{equation}$$使用preproccessing库的StandardScaler类对数据进行标准化的代码如下： 1234from sklearn.preprocessing import StandardScaler #标准化，返回值为标准化后的数据ss=StandardScaler()X1=ss.fit_transform(X) #X=iris.data（鸢尾花）数据集 归一化：处理后的所有特征的值都会被压缩到 0到1区间上.这样做还可以抑制离群值对结果的影响. 公式表达为：$${\\begin{equation} X_{norm} = \\frac{X - X_{min}}{X_{max}-X_{min}} \\end{equation}}$$使用preproccessing库的MinMaxScaler类对数据进行区间缩放的代码如下： 1234from sklearn.preprocessing import MinMaxScaler#区间缩放，返回值为缩放到[0, 1]区间的数据mms=MinMaxScaler()X2=mms.fit_transform(X) #X=iris.data（鸢尾花）数据集 正则化正则化的过程是将每个样本缩放到单位范数（每个样本的范数为1），如果后面要使用如二次型（点积）或者其它核方法计算两个样本之间的相似性这个方法会很有用。 该方法主要应用于文本分类和聚类中。 123from sklearn.preprocessing import Normalizerss=Normalizer()X3=ss.fit_transform(X) #X=iris.data（鸢尾花）数据集 标准化与正则化的区别简单来说，**标准化是依照特征矩阵的列处理数据，其通过求z-score的方法，将样本的特征值转换到同一量纲下。正则化是依照特征矩阵的行处理数据**，其目的在于样本向量在点乘运算或其他核函数计算相似性时，拥有统一的标准，也就是说都转化为“单位向量”。 特征选择特征选择：数据中包含冗余或相关变量（或特征、属性、指标），旨在从原有特征中找出主要特征。 包：sklearn.feature_selection 当数据预处理完成后，我们需要选择有意义的特征输入机器学习的算法和模型进行训练。通常来说，从两个方面考虑来选择特征： 特征是否发散：如果一个**特征不发散，例如方差接近于0，也就是说样本在这个特征上基本上没有差异，这个特征对于样本的区分并没有什么用**。 特征与目标的相关性：这点比较显见，**与目标相关性高的特征，应当优选选择**。除移除低方差法外，本文介绍的其他方法均从相关性考虑。 根据特征选择的形式又可以将特征选择方法分为3种： FilterFilter：过滤法，按照发散性或者相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，选择特征。 方差选择法：低方差特征过滤 代码： 12345from sklearn.feature_selection import VarianceThresholdX = [[0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 1, 1], [0, 1, 0], [0, 1, 1]]sel = VarianceThreshold(threshold=(.8 * (1 - .8)))X1=sel.fit_transform(X)print(X1) 输出结果: 123456[[0 1] [1 0] [0 0] [1 1] [1 0] [1 1]] 果然, VarianceThreshold 移除了第一列特征，第一列中特征值为0的概率达到了5/6. 相关系数：特征与特征之间的相关程度（**与目标相关性高的特征，应当优选选择**） 对于**分类问题(y离散)**，可采用： 卡方检验*，f_classif*, mutual_info_classif，互信息** 对于**回归问题(y连续)，可采用：**皮尔森相关系数**，f_regression**, mutual_info_regression，最大信息系数 卡方(Chi2)检验 ​ 经典的卡方检验是检验定性自变量对定性因变量的相关性。假设自变量有N种取值，因变量有M种取值，考虑自变量等于i且因变量等于j的样本频数的观察值与期望的差距，构建统计量：$$\\chi^{2}=\\sum \\frac{(A-E)^{2}}{E}$$假设有两个分类变量X和Y，它们的值域分别为{x1, x2}和{y1, y2}，其样本频数列联表为： 经典的卡方检验是检验定性自变量对定性因变量的相关性，针对分类问题。比如，我们可以对样本进行一次chi2测试来选择最佳的两项特征： 代码： 12345678from sklearn.datasets import load_irisfrom sklearn.feature_selection import SelectKBestfrom sklearn.feature_selection import chi2iris = load_iris()X, y = iris.data, iris.targetprint(X.shape)X_new = SelectKBest(chi2, k=2).fit_transform(X, y)print(X_new.shape) 输出结果: 12(150, 4)(150, 2) Pearson相关系数 (Pearson Correlation) ​ 皮尔森相关系数是一种最简单的，能帮助理解特征和响应变量之间关系的方法，该方法衡量的是变量之间的线性相关性，结果的取值区间为[-1，1]，-1表示完全的负相关，+1表示完全的正相关，0表示没有线性相关。 ​ Pearson Correlation速度快、易于计算，经常在拿到数据(经过清洗和特征提取之后的)之后第一时间就执行。Scipy的 pearsonr 方法能够同时计算相关系数和p-value. 代码： 12345678import numpy as npfrom scipy.stats import pearsonrnp.random.seed(0)size = 300x = np.random.normal(0, 1, size)# pearsonr(x, y)的输入为特征矩阵和目标向量print(&quot;Lower noise&quot;, pearsonr(x, x + np.random.normal(0, 1, size)))print(&quot;Higher noise&quot;, pearsonr(x, x + np.random.normal(0, 10, size))) 输出结果： 123#输出为二元组(sorce, p-value)的数组Lower noise (0.7182483686213842, 7.324017312997672e-49)Higher noise (0.05796429207933815, 0.31700993885325246) 这个例子中，我们比较了变量在加入噪音之前和之后的差异。当噪音比较小的时候，相关性很强，p-value很低。 实例分析：股票的财务指标相关性计算 分析 两两特征之间进行相关性计算 代码： 12345678import numpy as npfrom scipy.stats import pearsonrimport pandas as pddata = pd.read_csv(&quot;factor_returns.csv&quot;)factor = [&#x27;pe_ratio&#x27;, &#x27;pb_ratio&#x27;, &#x27;market_cap&#x27;, &#x27;return_on_asset_net_profit&#x27;, &#x27;du_return_on_equity&#x27;, &#x27;ev&#x27;,&#x27;earnings_per_share&#x27;, &#x27;revenue&#x27;, &#x27;total_expense&#x27;]for i in range(len(factor)): for j in range(i, len(factor) - 1): print(&quot;指标%s与指标%s之间的相关性大小为%f&quot; % (factor[i], factor[j + 1], pearsonr(data[factor[i]], data[factor[j + 1]])[0])) 输出结果：（展示部分数据结果） 1234567指标pe_ratio与指标pb_ratio之间的相关性大小为-0.004389指标pe_ratio与指标market_cap之间的相关性大小为-0.068861………………………………………………………………………………………………………………………………………………………………指标return_on_asset_net_profit与指标du_return_on_equity之间的相关性大小为0.818697指标return_on_asset_net_profit与指标ev之间的相关性大小为-0.101225………………………………………………………………………………………………………………………………………………………………指标revenue与指标total_expense之间的相关性大小为0.995845 从中我们得出 指标revenue与指标total_expense之间的相关性大小为0.995845 指标return_on_asset_net_profit与指标du_return_on_equity之间的相关性大小为0.818697 我们也可以通过画图来观察结果 代码： 1234import matplotlib.pyplot as pltplt.figure(figsize=(20, 8), dpi=100)plt.scatter(data[&#x27;revenue&#x27;], data[&#x27;total_expense&#x27;])plt.show() 注：特征与特征之间相关性很高：1）选取其中一个；2）权重加权求和；3）主成分分析 Scikit-learn提供的 f_regrssion 方法能够批量计算特征的f_score和p-value，非常方便，参考sklearn的 pipeline Pearson相关系数的一个明显缺陷是，作为特征排序机制，他只对线性关系敏感。如果关系是非线性的，即便两个变量具有一一对应的关系，Pearson相关性也可能会接近0。例如： 代码： 12x = np.random.uniform(-1, 1, 100000)print pearsonr(x, x**2)[0] 输出结果： 1-0.00230804707612 更多类似的例子参考 sample plots 。另外，如果仅仅根据相关系数这个值来判断的话，有时候会具有很强的误导性，如 Anscombe’s quartet ，最好把数据可视化出来，以免得出错误的结论。 互信息和最大信息系数 (Mutual information and maximal information coefficient (MIC) 经典的互信息（互信息为随机变量X与Y之间的互信息$I(X;Y)$为单个事件之间互信息的数学期望）也是评价定性自变量对定性因变量的相关性的，互信息计算公式如下：$$ I(X ; Y)=\\sum_{x \\in X} \\sum_{y \\in Y} p(x, y) \\log \\frac{p(x, y)}{p(x) p(y)}$$ 互信息直接用于特征选择其实不是太方便： 1、它不属于度量方式，也没有办法归一化，在不同数据及上的结果无法做比较； 2、对于连续变量的计算不是很方便（X和Y都是集合，x，y都是离散的取值），通常变量需要先离散化，而互信息的结果对离散化的方式很敏感。 最大信息系数克服了这两个问题。它首先寻找一种最优的离散化方式，然后把互信息取值转换成一种度量方式，取值区间在[0，1]。 minepy 提供了MIC功能。 反过头来看y=x^2这个例子，MIC算出来的互信息值为1(最大的取值)。 代码： 123456import numpy as npfrom minepy import MINEm = MINE()x = np.random.uniform(-1, 1, 10000)m.compute_score(x, x**2)print(m.mic()) 输出结果： 11.0000000000000009 MIC的统计能力遭到了 一些质疑 ，当零假设不成立时，MIC的统计就会受到影响。在有的数据集上不存在这个问题，但有的数据集上就存在这个问题。 距离相关系数 (Distance Correlation) 距离相关系数是为了克服Pearson相关系数的弱点而生的。在$X$和$X^2$这个例子中，即便Pearson相关系数是0，我们也不能断定这两个变量是独立的（有可能是非线性相关）；但如果距离相关系数是0，那么我们就可以说这两个变量是独立的。 R的 energy 包里提供了距离相关系数的实现，另外这是 Python gist 的实现。 尽管有 MIC 和 距离相关系数在了，但当变量之间的关系接近线性相关的时候，Pearson相关系数仍然是不可替代的。 第一，Pearson相关系数计算速度快，这在处理大规模数据的时候很重要。 第二，Pearson相关系数的取值区间是[-1，1]，而MIC和距离相关系数都是[0，1]。这个特点使得Pearson相关系数能够表征更丰富的关系，符号表示关系的正负，绝对值能够表示强度。当然，Pearson相关性有效的前提是两个变量的变化关系是单调的。 基于模型的特征排序 (Model based ranking) 这种方法的思路是直接使用你要用的机器学习算法，针对每个单独的特征和响应变量建立预测模型。假如特征和响应变量之间的关系是非线性的，可以用基于树的方法(决策树、随机森林)、或者扩展的线性模型等。基于树的方法比较易于使用，因为他们对非线性关系的建模比较好，并且不需要太多的调试。但要注意过拟合问题，因此树的深度最好不要太大，再就是运用交叉验证。 在波士顿房价数据集上使用sklearn的随机森林回归给出一个单变量选择的例子(这里使用了交叉验证)： 代码： 12345678910111213141516from sklearn.model_selection import cross_val_score, ShuffleSplitfrom sklearn.datasets import load_bostonfrom sklearn.ensemble import RandomForestRegressorimport numpy as np# Load boston housing dataset as an exampleboston = load_boston()X = boston[&quot;data&quot;]Y = boston[&quot;target&quot;]names = boston[&quot;feature_names&quot;]rf = RandomForestRegressor(n_estimators=20, max_depth=4)scores = []# 单独采用每个特征进行建模，并进行交叉验证for i in range(X.shape[1]): score = cross_val_score(rf, X[:, i:i+1], Y, scoring=&quot;r2&quot;, cv=ShuffleSplit(len(X), 3, .3)) # 注意X[:, i]和X[:, i:i+1]的区别 scores.append((format(np.mean(score), &#x27;.3f&#x27;), names[i]))print(sorted(scores, reverse=True)) 输出结果： 1[(&#x27;-8.082&#x27;, &#x27;TAX&#x27;), (&#x27;-6.871&#x27;, &#x27;CHAS&#x27;), (&#x27;-6.420&#x27;, &#x27;RM&#x27;), (&#x27;-6.315&#x27;, &#x27;DIS&#x27;), (&#x27;-4.833&#x27;, &#x27;INDUS&#x27;), (&#x27;-4.816&#x27;, &#x27;AGE&#x27;), (&#x27;-4.742&#x27;, &#x27;LSTAT&#x27;), (&#x27;-4.638&#x27;, &#x27;RAD&#x27;), (&#x27;-3.411&#x27;, &#x27;NOX&#x27;), (&#x27;-3.123&#x27;, &#x27;CRIM&#x27;), (&#x27;-26.603&#x27;, &#x27;PTRATIO&#x27;), (&#x27;-12.284&#x27;, &#x27;B&#x27;), (&#x27;-1.995&#x27;, &#x27;ZN&#x27;)] WrapperWrapper：包装法，根据目标函数（通常是预测效果评分），每次选择若干特征，或者排除若干特征。 主要方法有：recursive feature elimination algorithm(递归特征消除算法) EmbeddedEmbedded：嵌入法，先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。类似于Filter方法，但是是通过训练来确定特征的优劣。比如，Lasso和RF（随机森林）都有各自的特征选择方法。 注：使用SelectFromModel选择特征 1from sklearn.feature_selection import SelectFromMode 特征降维二维数组 此处的降维：降低特征的个数 效果：特征与特征之间不相关 降维**是指在某种限定条件下，降低随机变量（特征）个数，得到一组“不相关”主变量**的过程。 当特征选择完成后，可以直接训练模型了，但是可能由于特征矩阵过大，导致计算量大，训练时间长的问题，因此降低特征矩阵维度也是必不可少的。常见的降维方法：主成分分析法（PCA）和线性判别分析（LDA），**线性判别分析本身也是一个分类模型。PCA和LDA有很多的相似点，其本质是要将原始的样本映射到维度更低的样本空间中，但是PCA和LDA的映射目标不一样：PCA是为了让映射后的样本具有最大的发散性；而LDA是为了让映射后的样本有最好的分类性能**。所以说PCA是一种无监督的降维方法，而LDA是一种有监督的降维方法。 主成分分析法（PCA）基本思想：构造原变量的一系列线性组合形成几个综合指标，以**去除数据的相关性**，并使低维数据最大程度保持原始高维数据的方差信息。 主成分个数的确定： 贡献率：第i个主成分的方差在全部方差中所占比重，反映第i个主成分所提取的总信息的份额。 累计贡献率：前k个主成分在全部方差中所占比重 主成分个数的确定：累计贡献率&gt;0.85 相关系数矩阵or协方差阵？当涉及变量的量纲不同或取值范围相差较大的指标时，应考虑从相关系数矩阵出发进行主成分分析；（相关系数矩阵消除了量纲的影响。）对同度量或取值范围相差不大的数据，从协方差阵出发。 使用decomposition库的PCA类选择特征的代码如下： 1234from sklearn.decomposition import PCA#主成分分析法，返回降维后的数据#参数n_components为主成分数目PCA(n_components=2).fit_transform(X) #X=iris.data（鸢尾花）数据集 n_components： 小数：表示保留百分之多少的信息 整数：减少到多少特征 线性判别分析法（LDA）至多能把C类数据降维到C-1维子空间 使用lda库的LDA类选择特征的代码如下： 1234from sklearn.lda import LDA#线性判别分析法，返回降维后的数据#参数n_components为降维后的维数LDA(n_components=2).fit_transform(X,Y) #X=iris.data,Y= iris.target（鸢尾花）数据集 小结 分类算法分类问题：目标值—类别 sklearn转换器和估计器转换器1.实例化 (实例化的是一个转换器类(Transformer)) 2 调用fit_transform(对于文档建立分类词频矩阵，不能同时调用) 标准化： (x - mean) / std fit_transform() fit() #计算 每一列的平均值、标准差 transform() # (x - mean) / std进行最终的转换 估计器sklearn机器学习算法的实现1、用于分类的估计器： sklearn.neighbors k-近邻算法 sklearn.naive_bayes 贝叶斯 sklearn.linear_model.LogisticRegression 逻辑回归 sklearn.tree 决策树与随机森林 2、用于回归的估计器： sklearn.linear_model.LinearRegression 线性回归 sklearn.linear_model.Ridge 岭回归 3、用于无监督学习的估计器 sklearn.cluster.KMeans 聚类 估计器工作流程1、实例化一个estimator 2、estimator.fit(x_train, y_train) 计算 —— 调用完毕，模型生成 3 模型评估： 1）直接比对真实值和预测值 y_predict = estimator.predict(x_test) y_test y_predict 2）计算准确率 accuracy = estimator.score(x_test, y_test) K-近邻算法K-近邻算法（KNN）K-近邻算法(KNN)理论/原理：“物以类聚，人以群分” 相同/近似样本在样本空间中是比较接近的，所以可以使用和当前样本比较近的其他样本的目标属性值作为当前样本的预测值。 k-近邻算法的工作机制很简单： 给定测试样本，基于某种距离度量（一般使用欧几里德距离）找出训练集中与其最靠近的k个训练样本，然后基于这k个“邻居”的信息来进行预测。 如何确定谁是邻居？ 计算距离： 距离公式： 欧氏距离：$$d = \\sqrt{(x1 - y1)^2 + (x2 - y2)^2 + (x3 - y3)^2 + ……}$$ 曼哈顿距离—绝对值距离 明可夫斯基距离：欧氏距离和曼哈顿距离的推广 metric_params=None 图中**红线代表曼哈顿距离，绿线代表欧氏距离，也就是直线距离，而蓝线和黄线**代表等价的曼哈顿距离。 电影类型分析假设我们有现在几部电影 其中？ 号电影不知道类别，如何去预测？我们可以利用K近邻算法的思想 k = 1 ——&gt;最近距离18.7——&gt;电影为爱情片——&gt;预测？号电影为爱情片 k = 2 ——&gt;最近距离18.7和19.2——两部电影都是爱情片——预测？号电影为爱情片 …… k = 6——&gt; 六部电影爱情片和动作片一样多——&gt;无法确定 k = 7 ——&gt;若4部动作片，3部爱情片——&gt;预测？号电影为动作片,但实际电影为爱情片 如果取的最近的电影数量不一样？会是什么结果？ k 值取得过小，容易受到异常点的影响 k 值取得过大，样本不均衡的影响 K-近邻算法APIsklearn.neighbors.KNeighborsClassifier(n_neighbors=5,weights=’uniform’,algorithm=’auto’,leaf_size=30,p=2,metric=’minkowski’,metric_params=None,n_jobs=None,**kwargs) 邻居数k: n_neighbors:int,可选(默认= 5) 权重weights: weights = ‘uniform’ weights = ‘distance’ 距离度量: p=1距离度量采用曼哈顿距离；p=2距离度量采用欧氏距离 algorithm：{‘auto’，‘ball_tree’，‘kd_tree’，‘brute’}，可选用于计算最近邻居的算法：‘ball_tree’将会使用 BallTree，‘kd_tree’将使用 KDTree。‘auto’将尝试根据传递给fit方法的值来决定最合适的算法。 (不同实现方式影响效率) 案例：鸢尾花种类预测代码： 12345678910111213141516171819202122232425#1)导入库import numpy as npfrom sklearn import datasetsfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import StandardScalerfrom sklearn.neighbors import KNeighborsClassifier#2)获取数据x,y = datasets.load_iris(True)#3）划分数据集x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.2)#4）特征工程：标准化ss=StandardScaler()x_train=ss.fit_transform(x_train)x_test=ss.transform(x_test)#5)KNN算法预估器(训练数据)estimator=KNeighborsClassifier(n_neighbors=3) #设置k=3estimator.fit(x_train,y_train)#6)模型评估#方法1：直接比对真实值和预测值y_predict=estimator.predict(x_test)print(&#x27;y_predict:\\n&#x27;,y_predict)print(&#x27;直接比对真实值和预测值：\\n&#x27;,y_test y_predict)#方法2：计算准确率score=estimator.score(x_test,y_test)print(&#x27;准确率为：\\n&#x27;,score) 输出结果： 12345678y_predict: [0 2 0 2 2 0 2 1 0 1 2 0 2 2 0 0 2 1 0 2 2 1 2 0 1 0 1 0 1 1]直接比对真实值和预测值： [ True True True True True True True True True True True True True True True True True True True True True True True True True True True True False True]准确率为： 0.9666666666666667 K-近邻总结优点：简单，易于理解，易于实现，无需训练 缺点： 1）必须指定K值，K值选择不当则分类精度不能保证 2）懒惰算法，对测试样本分类时的计算量大，内存开销大 使用场景：小数据场景，几千～几万样本，具体场景具体业务去测试 模型选择与调优什么是交叉验证(cross validation)交叉验证：将拿到的训练数据，分为训练和验证集。以下图为例：将数据分成4份，其中一份作为验证集。然后经过4次(组)的测试，每次都更换不同的验证集。即得到4组模型的结果，取平均值作为最终结果。又称4折交叉验证。 训练集：训练集+验证集 测试集：测试集 超参数搜索-网格搜索(Grid Search)通常情况下，有很多参数是需要手动指定的（如k-近邻算法中的K值），这种叫超参数。但是手动过程繁杂，所以需要对模型预设几种超参数组合。每组超参数都采用交叉验证来进行评估。最后选出最优参数组合建立模型。 模型选择与调优:sklearn.model_selection.GridSearchCV(estimator, param_grid=None,cv=None) 对估计器的指定参数值进行详尽搜索 estimator：估计器对象 param_grid：估计器参数(dict){“n_neighbors”:[1,3,5]} cv：指定几折交叉验证 fit：输入训练数据 score：准确率 结果分析：bestscore:在交叉验证中验证的最好结果_bestestimator：最好的参数模型cvresults:每次交叉验证后的验证集准确率结果和训练集准确率结果 鸢尾花案例调优代码： 12345678910111213141516171819202122232425262728293031323334353637383940414243import numpy as npfrom sklearn import datasetsfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import StandardScalerfrom sklearn.neighbors import KNeighborsClassifier# grid网格，search搜索，cv：cross_validation# 搜索算法最合适的参数from sklearn.model_selection import GridSearchCV#1)获取数据x,y = datasets.load_iris(True)#2）划分数据集x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.2)#3）特征工程：标准化ss=StandardScaler()x_train=ss.fit_transform(x_train)x_test=ss.transform(x_test)#4)KNN算法预估器estimator=KNeighborsClassifier()#网格搜索GridSearchCV进行最佳参数的查找params = &#123;&#x27;n_neighbors&#x27;:[i for i in range(1,30)], &#x27;weights&#x27;:[&#x27;distance&#x27;,&#x27;uniform&#x27;], &#x27;p&#x27;:[1,2]&#125;# cross_val_score类似estimator = GridSearchCV(estimator,param_grid=params,scoring=&#x27;accuracy&#x27;,cv = 6)estimator.fit(x_train,y_train)#5)模型评估#方法1：直接比对真实值和预测值y_predict=estimator.predict(x_test)print(&#x27;y_predict:\\n&#x27;,y_predict)print(&#x27;直接比对真实值和预测值：\\n&#x27;,y_test y_predict)#方法2：计算准确率score=estimator.score(x_test,y_test)print(&#x27;准确率为：\\n&#x27;,score)#查看了GridSearchCV最佳的参数组合#最佳参数：best_params_print(&#x27;最佳参数：\\n&#x27;,estimator.best_params_)#最佳结果：best_score_print(&#x27;最佳结果：\\n&#x27;,estimator.best_score_)#最佳估计器：best_estimator_print(&#x27;最佳估计器：\\n&#x27;,estimator.best_estimator_)#交叉验证结果：cv_results_#print(&#x27;交叉验证结果：\\n&#x27;,estimator.cv_results_) 输出结果： 12345678910111213141516y_predict: [1 1 2 2 1 0 1 0 1 2 1 2 1 0 1 2 1 0 0 2 1 1 1 2 2 2 0 0 1 1]直接比对真实值和预测值： [ True True True True True True True True False True True True True True True True False False True True True True True True True True True True False True]准确率为： 0.8666666666666667最佳参数： &#123;&#x27;n_neighbors&#x27;: 15, &#x27;p&#x27;: 2, &#x27;weights&#x27;: &#x27;distance&#x27;&#125;最佳结果： 0.9833333333333333最佳估计器： KNeighborsClassifier(algorithm=&#x27;auto&#x27;, leaf_size=30, metric=&#x27;minkowski&#x27;, metric_params=None, n_jobs=None, n_neighbors=15, p=2, weights=&#x27;distance&#x27;) 注：最佳结果—训练集再分为训练集+验证集，验证集的效果；准确率—整体测试集的效果。数据集不同 朴素贝叶斯算法朴素贝叶斯分类器是基于概率论的分类模型，其思想是先计算样本的先验概率，然后利用贝叶斯公式测算未知样本属于某个类别的后验概率，最终以最大后验概率对应的类别作为未知样本的预测类别。之所以叫”朴素”，是因为整个形式化过程只做最简单、最原始的假设。 理论基础案例 问题： 1、女神喜欢的概率？ $P(喜欢)=\\frac{4}{7}$ 2、职业是程序员并且体型匀称的概率？ $P(程序员，匀称)=\\frac{1}{7}$ (联合概率) 3、在女神喜欢的条件下，职业是程序员的概率？ $P(程序员|喜欢)=\\frac{2}{4}=\\frac{1}{2}$ （条件概率） 4、在女神喜欢的条件下，职业是产品，体重是超重的概率？ ​ $P(程序员，超重|喜欢)=\\frac{1}{4}$ （联合概率、条件概率） 相互独立：$P(AB)=P(A)P(B)$&lt;=&gt;事件A与事件B相互独立 $P(程序员，匀称)=\\frac{1}{7}$ $P(程序员)=\\frac{3}{7}$ $P(匀称)=\\frac{4}{7}$ $P(程序员，匀称)≠P(程序员)P(匀称)$ 不独立 贝叶斯公式：$P(A|B)=\\frac{P(B|A)P(A)}{P(B)}$ $P(喜欢|产品经理,超重)=\\frac{P(产品经理,超重|喜欢)P(喜欢)}{P(产品经理,超重)}=0$ 朴素? 假设:特征与特征之间是相互独立的 朴素贝叶斯算法=朴素+贝叶斯 $P(喜欢|产品经理,超重)=\\frac{P(产品经理,超重|喜欢)P(喜欢)}{P(产品经理,超重)}=\\frac{P(产品经理|喜欢)P(超重|喜欢)P(喜欢)}{P(产品经理)P(超重)}=\\frac{7}{12}$ 应用场景朴素贝叶斯算法应用场景：文本分类 公式：$P(C|W)=\\frac{P(W|C)P(C)}{P(W)}$ 注：$W$为给定文档的特征值（频数统计，预测文档提供），$C$为文档类别 公式如果应用在文章分类的场景当中，我们可以这样看：$$P(C|F1,F2,…)=\\frac{P(F1,F2,…|C)P(C)}{P(F1,F2,…)}$$其中$C$可以是不同类别 公式分为三个部分： $P(C)$：每个文档类别的概率(某文档类别数／总文档数量) $P(W|C)$：给定类别下特征（被预测文档中出现的词）的概率 ​ 计算方法：$P(F1│C)=Ni/N $（训练文档中去计算） ​ $Ni$为该$F1$词在$C$类别所有文档中出现的次数​ $N$为所属类别$C$下的文档所有词出现的次数和 $P(F1,F2,…) $预测文档中每个词的概率 常用贝叶斯分类器1.高斯贝叶斯分类器适用条件：自变量X均为连续的数值型假设条件：自变量X服从高斯正态分布自变量X的条件概率：$P(x_j∣C_i)=\\frac{1}{\\sqrt{2\\pi}\\sigma_{ji}}exp(-\\frac{(x_j-\\mu_{ji})^2}{2\\sigma_{ji}^2})$ 其中$x_j$为第$j$个自变量的取值，$μ{ji}$为训练集中自变量$x_j$属于类别 $C_i$的均值，$σ{ji}$为训练集中自变量$x_j$属于类别 $C_i$的标准差。 2.多项式贝叶斯分类器适用条件：自变量X均为离散型变量假设条件：自变量X的条件概率服从多项式分布自变量X的条件概率：$P(x_j=x_{jk}∣C_i)=\\frac{N_{ik}+\\alpha}{N_i+n\\alpha}$ 其中$x_{jk}$为自变量$x_j$的第$k$个取值，$N_{ik}$表示因变量为类别$C_i$时自变量$x_j$取值$x_{jk}$的样本个数，$N_i$为类别$C_i$的样本个数，$α$为平滑系数（防止条件概率等于0，通常取1），n为训练文档中统计出的**特征词**个数。 3.伯努利贝叶斯分类器适用条件：自变量X均为0-1二元变量假设条件：自变量X的条件概率服从伯努利分布自变量X的条件概率 $P(x_j∣C_i)=px_j+(1−p)(1−x_j)$ 其中$x_j$为第$j$个自变量，其取值为0或1；$p$表示类别为$C_i$时自变量取1的概率，可以用经验频率代替 $p=P(x_j=1∣C_i)=\\frac{N_{i1}+α}{N_i+nα}$ $N_{i1}$表示在类别$C_i$时自变量$x_j$取1的样本量。 代码实现1.高斯贝叶斯分类器代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475#高斯贝叶斯分类器进行癌症预测import numpy as npimport pandas as pdfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import StandardScaler# 1、读取数据column_name = [&#x27;Sample code number&#x27;, &#x27;Clump Thickness&#x27;, &#x27;Uniformity of Cell Size&#x27;, &#x27;Uniformity of Cell Shape&#x27;, &#x27;Marginal Adhesion&#x27;, &#x27;Single Epithelial Cell Size&#x27;, &#x27;Bare Nuclei&#x27;, &#x27;Bland Chromatin&#x27;, &#x27;Normal Nucleoli&#x27;, &#x27;Mitoses&#x27;, &#x27;Class&#x27;]data = pd.read_csv(&quot;https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data&quot;, names=column_name)# 2、数据处理—处理缺失值data = data.replace(to_replace=&#x27;?&#x27;, value=np.nan) #1)替换np.nandata = data.dropna() #2)删除缺失值print(data.isnull().any()) #确认不存在缺失值print(&#x27;-----------------------------------&#x27;)# 取出特征值x = data[column_name[1:10]] #x=data.iloc[:,1:-1]y = data[column_name[10]] #y=data[&#x27;Class&#x27;]#设置正例和负例y = y.map(&#123;2:0,4:1&#125;)print(y.value_counts())print(&#x27;-----------------------------------&#x27;)#3、分割数据集x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3)#4、特征工程—标准化std = StandardScaler()x_train = std.fit_transform(x_train)x_test = std.transform(x_test)#5、estimator估计器流程from sklearn import naive_bayes#调用高斯朴素贝叶斯分类器的“类”gnb = naive_bayes.GaussianNB()#模型拟合gnb.fit(x_train,y_train)#6、进行预测（模型评估）#模型在测试数据集上的预测gnb_pred = gnb.predict(x_test)#各类别的预测数量print(pd.Series(gnb_pred).value_counts())print(&#x27;-----------------------------------&#x27;)#导入第三方包from sklearn import metricsimport matplotlib.pyplot as pltimport seaborn as sns#构建混淆矩阵cm = pd.crosstab(gnb_pred,y_test)#绘制混淆矩阵图sns.heatmap(cm,annot = True , cmap = &#x27;GnBu&#x27; , fmt = &#x27;d&#x27;)#去除x轴和y轴的标签plt.xlabel(&#x27;Real&#x27;)plt.ylabel(&#x27;Predict&#x27;)#显示图形plt.show()print(&#x27;模型的准确率：\\n&#x27;,metrics.accuracy_score(y_test,gnb_pred))print(&#x27;模型的评估报告：\\n&#x27;,metrics.classification_report(y_test,gnb_pred))#计算正例的预测概率，用于生成ROC曲线的数据y_score = gnb.predict_proba(x_test)[:,1]fpr,tpr,threshold = metrics.roc_curve(y_test,y_score)#计算AUC的值roc_auc = metrics.auc(fpr,tpr)#绘制面积图plt.stackplot(fpr,tpr,color = &#x27;steelblue&#x27;,alpha = 0.5,edgecolor = &#x27;black&#x27;)#添加边际线plt.plot(fpr,tpr,color= &#x27;black&#x27;,lw =1)#添加对角线plt.plot([0,1],[0,1],color = &#x27;red&#x27;,linestyle = &#x27;--&#x27;)#添加文本信息plt.text(0.5,0.3,&#x27;ROC curve(area = %0.2f)&#x27;% roc_auc)#添加x轴与y轴标签plt.xlabel(&#x27;1-Specificity&#x27;)plt.ylabel(&#x27;Sensitivity&#x27;)#显示图形plt.show() 输出结果： 1234567891011121314151617181920212223242526272829303132Sample code number FalseClump Thickness FalseUniformity of Cell Size FalseUniformity of Cell Shape FalseMarginal Adhesion FalseSingle Epithelial Cell Size FalseBare Nuclei FalseBland Chromatin FalseNormal Nucleoli FalseMitoses FalseClass Falsedtype: bool-----------------------------------0 4441 239Name: Class, dtype: int64-----------------------------------0 1231 82dtype: int64-----------------------------------模型的准确率： 0.9707317073170731模型的评估报告： precision recall f1-score support 0 0.99 0.96 0.98 127 1 0.94 0.99 0.96 78 accuracy 0.97 205 macro avg 0.97 0.97 0.97 205weighted avg 0.97 0.97 0.97 205 2.多项式贝叶斯分类器代码： 12345678910111213141516171819202122232425262728293031import pandas as pdfrom sklearn import model_selection,naive_bayes,metricsimport matplotlib.pyplot as pltdata=pd.read_csv(r&#x27;mushrooms.csv&#x27;)print(data.head())#使用factorize函数将字符型数据做因子化处理，将其转换为整数型数据#factorize函数返回的是两个元素的元组，第一个元素为转换成的数值，第二个元素为数值对应的字符水平columns=data.columns[1:]for column in columns: data[column]=pd.factorize(data[column])[0]#拆分为训练集和测试集x_train,x_test,y_train,y_test=model_selection.train_test_split(data[columns],data.type,test_size=0.25,random_state=1234)#调用多项式朴素贝叶斯mnb=naive_bayes.MultinomialNB()mnb.fit(x_train,y_train)mnb_pred=mnb.predict(x_test)#显示预测结果，各类别的预测数量#模型检验print(&#x27;模型的准确率为：&#x27;,metrics.accuracy_score(y_test,mnb_pred))print(&#x27;模型的评估报告：\\n&#x27;,metrics.classification_report(y_test,mnb_pred))#绘制ROC曲线y_score=mnb.predict_proba(x_test)[:,1]fpr,tpr,threshold=metrics.roc_curve(y_test.map(&#123;&#x27;e&#x27;:0,&#x27;p&#x27;:1&#125;),y_score)roc_auc=metrics.auc(fpr,tpr)plt.stackplot(fpr,tpr,color=&#x27;steelblue&#x27;,alpha=0.5,edgecolor=&#x27;black&#x27;)plt.plot(fpr,tpr,color=&#x27;black&#x27;,lw=1)plt.plot([0,1],[0,1],color=&#x27;red&#x27;,linestyle=&#x27;--&#x27;)plt.text(0.5,0.3,&#x27;ROC Curve (area=%0.2f)&#x27; % roc_auc)plt.xlabel(&#x27;l-Specificity&#x27;)plt.ylabel(&#x27;Sensitivity&#x27;)plt.show() 输出结果： 123456789101112131415161718 type cap-shape cap-surface ... spore-print-color population habitat0 p x s ... k s u1 e x s ... n n g2 e b s ... n n m3 p x y ... k s u4 e x s ... n a g[5 rows x 23 columns]模型的准确率为： 0.8877400295420975模型的评估报告： precision recall f1-score support e 0.85 0.95 0.89 1017 p 0.94 0.83 0.88 1014 accuracy 0.89 2031 macro avg 0.89 0.89 0.89 2031weighted avg 0.89 0.89 0.89 2031 3.伯努利贝叶斯分类器未完待续 案例：新闻分类代码： 123456789101112131415161718192021222324252627282930313233#朴素贝叶斯对新闻数据集进行预测#1）导入库import numpy as npfrom sklearn import datasetsfrom sklearn.datasets import fetch_20newsgroupsfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import StandardScalerfrom sklearn.feature_extraction.text import TfidfVectorizerfrom sklearn.naive_bayes import MultinomialNBfrom sklearn import metrics#2）获取新闻的数据，20个类别news = fetch_20newsgroups(subset=&#x27;all&#x27;)#3）进行数据集分割x_train, x_test, y_train, y_test = train_test_split(news.data, news.target, test_size=0.3)#4）对于文本数据，进行特征抽取tf = TfidfVectorizer()x_train = tf.fit_transform(x_train)#这里打印出来的列表是：训练集当中的所有不同词的组成的一个列表print(tf.get_feature_names())# print(x_train.toarray())x_test = tf.transform(x_test) # 不需要fit_transform#5）estimator估计器流程mnb = MultinomialNB(alpha=1.0) #默认alpha=1.0mnb.fit(x_train, y_train)#6）进行预测（模型评估）y_predict = mnb.predict(x_test)print(&#x27;预测每篇文章的类别：\\n&#x27;, y_predict[:100])print(&#x27;真实类别为：\\n&#x27;, y_test[:100])print(&#x27;模型预测的准确率为：\\n&#x27;, mnb.score(x_test, y_test))#print(&#x27;模型预测的准确率为：\\n&#x27;,metrics.accuracy_score(y_test,y_predict))#混淆矩阵from sklearn.metrics import classification_reportprint(&#x27;模型的评估报告：\\n&#x27;,classification_report(y_test,y_predict)) 输出结果： 123456789101112131415161718192021222324252627[&#x27;00&#x27;, &#x27;000&#x27;, &#x27;0000&#x27;, &#x27;00000&#x27;, &#x27;000000&#x27;, &#x27;00000000&#x27;, &#x27;0000000004&#x27;, &#x27;0000000005&#x27;, &#x27;00000000b&#x27;, &#x27;00000001&#x27;, &#x27;00000001b&#x27;, &#x27;00000010&#x27;, ··································································· &#x27;zzrk&#x27;, &#x27;zzs&#x27;, &#x27;zzt&#x27;, &#x27;zzvsi&#x27;, &#x27;zzx&#x27;, &#x27;zzy_3w&#x27;, &#x27;zzzzzz&#x27;, &#x27;zzzzzzt&#x27;, &#x27;³ation&#x27;, &#x27;íålittin&#x27;, &#x27;ñaustin&#x27;, &#x27;ýé&#x27;, &#x27;ÿhooked&#x27;]预测每篇文章的类别： [16 7 2 4 10 3 18 7 8 12 0 13 11 11 0 15 18 10 12 8 12 2 15 7 9 12 15 16 9 0 3 15 8 3 14 17 1 0 15 16 9 2 3 6 5 4 6 14 8 9 13 9 11 12 10 10 3 11 11 0 6 12 15 3 15 6 5 1 9 14 10 3 7 11 0 3 7 16 13 9 5 15 1 13 15 16 7 4 1 16 16 18 14 15 7 16 15 14 0 14]真实类别为： [19 7 2 4 10 3 18 3 8 12 0 13 11 11 0 0 18 10 12 15 12 2 15 7 9 12 15 16 9 0 3 15 8 3 14 17 1 0 15 16 9 2 3 8 5 4 6 14 8 9 13 9 11 12 10 10 3 11 11 0 6 12 0 3 0 6 5 1 9 14 12 6 8 12 0 3 7 18 13 9 5 15 1 13 15 16 7 4 1 16 18 18 14 15 7 13 15 1 0 14]模型预测的准确率为： 0.8593915811814644 模型的评估报告： precision recall f1-score support 0 0.89 0.76 0.82 235 1 0.93 0.72 0.81 320 ··································································· 18 0.99 0.63 0.77 235 19 1.00 0.21 0.35 177 accuracy 0.86 5654 macro avg 0.88 0.84 0.84 5654weighted avg 0.88 0.86 0.85 5654 朴素贝叶斯总结 优点： 朴素贝叶斯模型发源于古典数学理论，有稳定的分类效率。 对缺失数据不太敏感，算法也比较简单，常用于文本分类。 分类准确度高，速度快 缺点： 由于使用了样本属性独立性的假设，所以如果特征属性有关联时其效果不好 决策树认识决策树​ 如何高效的进行决策？​ 特征的先后顺序 决策树分类原理详解​ 已知 四个特征值 预测 是否贷款给某个人​ 先看房子，再工作 -&gt; 是否贷款 只看了两个特征​ 年龄，信贷情况，工作 看了三个特征 信息论基础1）信息：消除随机不确定性的东西 小明：“我今年18岁” - 信息 小华： ”小明明年19岁” - 不是信息(当小明告诉我他今年18岁，这就消除了对小明年龄的不确定性) 2）信息的衡量标准 - 熵 熵：表示随机变量不确定性的度量 熵单位为比特（bit） （解释：说白了就是物体内部的混乱程度，比如杂货市场里面什么都有那肯定混乱，专卖店里面只卖一个牌子那就稳定多了） 公式：$$H（D）=-\\sum_{i=1}^{n} p(x i) * \\log _{2} p(x i)$$熵：不确定性越大，得到的熵值也就越大 3）决策树的划分依据之一——信息增益 特征A对训练数据集D的信息增益$g(D,A)$,定义为集合D的熵$H(D)$与特征A给定条件下D的信息条件熵$H(D|A)$之差，即公式为：$$g(D,A)=H(D)-H(D|A)$$ 公式的详细解释： 熵的计算：$$H(D)=-\\sum_{k=1}^{K} \\frac{\\left|C_{k}\\right|}{|D|} \\log_{2} \\frac{\\left|C_{k}\\right|}{|D|}$$ 条件熵的计算公式：$$H(D | A)=\\sum_{i=1}^{n} \\frac{\\left|D_{i}\\right|}{|D|} H\\left(D_{i}\\right)=-\\sum_{i=1}^{n} \\frac{\\left|D_{i}\\right|}{|D|} \\sum_{k=1}^{K} \\frac{\\left|D_{i k}\\right|}{\\left|D_{i}\\right|} \\log_{2} \\frac{\\left|D_{i k}\\right|}{\\left|D_{i}\\right|}$$ 注：$C_{k}$表示属于某个类别的样本数 注：信息增益表示得知特征X的信息而信息的不确定性减少的程度使得类Y的信息熵减少的程度 例子 根据某人年龄、工作、房子和信贷情况，判断是否贷款？ 在历史数据中（15次贷款）有6次没贷款，9次贷款，所以此时的熵应为： $$H(D)=-(\\frac{6}{15}*log_{2}\\frac{6}{15}+\\frac{9}{15}*log_{2}\\frac{9}{15})=0.971$$ $H(青年)=-(\\frac{2}{5}*log_{2}\\frac{2}{5}+\\frac{3}{5}*log_{2}\\frac{3}{5})=0.971$ $H(中年)=-(\\frac{2}{5}*log_{2}\\frac{2}{5}+\\frac{3}{5}*log_{2}\\frac{3}{5})=0.971$ $H(老年)=-(\\frac{1}{5}*log_{2}\\frac{1}{5}+\\frac{4}{5}*log_{2}\\frac{4}{5})=0.722$ $H(D|年龄)=\\frac{5}{15}H(青年)+\\frac{5}{15}H(中年)+\\frac{5}{15}H(老年)=0.888$ $g(D,年龄)=H(D)-H(D|年龄)=0.083$ 注：别人计算为0.313 我们以A1、A2、A3、A4代表年龄、有工作、有自己的房子和贷款情况。最终计算的结果g(D, A1) = 0.313, g(D, A2) = 0.324, g(D, A3) = 0.420,g(D, A4) = 0.363。所以我们选择A3 作为划分的第一个特征。这样我们就可以一棵树慢慢建立 决策树的三种算法实现当然决策树的原理不止信息增益这一种，还有其他方法。但是原理都类似，我们就不去举例计算。 ID3 信息增益 最大的准则 C4.5 信息增益比 最大的准则 CART分类树: 基尼系数 最小的准则 在sklearn中可以选择划分的默认原则优势：划分更加细致（从后面例子的树显示来理解） 决策树算法APIclass sklearn.tree.DecisionTreeClassifier(criterion=’gini’, max_depth=None,random_state=None)决策树分类器 criterion:默认是’gini’系数，也可以选择信息增益的熵’entropy’ max_depth:树的深度大小 random_state:随机数种子 其中会有些超参数：max_depth:树的深度大小 其它超参数我们会结合随机森林讲解 案例1：鸢尾花种类预测代码： 1234567891011121314151617181920212223#决策树对鸢尾花进行分类#1)导入库import numpy as npfrom sklearn import datasetsfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import StandardScalerfrom sklearn.tree import DecisionTreeClassifier#2)获取数据x,y = datasets.load_iris(True)#3）划分数据集x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.2)#决策树不需要特征工程：标准化#4)决策树算法预估器(训练数据)estimator=DecisionTreeClassifier(criterion=&#x27;entropy&#x27;)estimator.fit(x_train,y_train)#5)模型评估#方法1：直接比对真实值和预测值y_predict=estimator.predict(x_test)print(&#x27;y_predict:\\n&#x27;,y_predict)print(&#x27;直接比对真实值和预测值：\\n&#x27;,y_test y_predict)#方法2：计算准确率score=estimator.score(x_test,y_test)print(&#x27;准确率为：\\n&#x27;,score) 输出结果： 12345678y_predict: [1 2 0 0 1 2 0 1 2 2 2 1 1 0 1 2 1 0 0 2 2 2 2 2 2 2 0 2 1 2]直接比对真实值和预测值： [ True True True True True True True False True True False True True True True True True True True True False True True True True True True True True True]准确率为： 0.9 注：比对kNN算法准确率低；由于鸢尾花数据少，而kNN算法使用场景为小数据场景 决策树可视化1、sklearn.tree.export_graphviz() 该函数能够导出DOT格式 tree.export_graphviz(estimator,out_file=’tree.dot’,feature_names=[‘’,’’]) 2、工具:(能够将dot文件转换为pdf、png) 安装graphviz ubuntu:sudo apt-get install graphviz Mac:brew install graphviz 3、运行命令 然后我们运行这个命令 dot -Tpng tree.dot -o tree.png 以上文鸢尾花数据为例： 12from sklearn.tree import export_graphvizexport_graphviz(estimator,out_file=&#x27;iris_tree.dot&#x27;,feature_names=iris.feature_names) 导出文档iris_tree.dot，打开文档全选复制，粘贴到http://www.webgraphviz.com/网页版上可以实现可视化。无需安装软件。 案例：泰坦尼克号乘客生存预测泰坦尼克号数据在泰坦尼克号和titanic2数据帧描述泰坦尼克号上的个别乘客的生存状态。这里使用的数据集是由各种研究人员开始的。其中包括许多研究人员创建的旅客名单，由Michael A. Findlay编辑。我们提取的数据集中的特征是票的类别，存活，乘坐班，年龄，登陆，home.dest，房间，票，船和性别。 代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748#决策树进行乘客生存预测#1、导入库import numpy as npimport pandas as pdimport matplotlib.pyplot as pltfrom sklearn import datasetsfrom sklearn.feature_extraction import DictVectorizerfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import StandardScalerfrom sklearn.tree import DecisionTreeClassifier,export_graphvizfrom sklearn import tree#2、获取数据titan = pd.read_csv(&quot;http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic.txt&quot;)#3、数据的处理x = titan[[&#x27;pclass&#x27;, &#x27;age&#x27;, &#x27;sex&#x27;]]y = titan[&#x27;survived&#x27;]# ①缺失值处理，将特征当中有类别的这些特征进行字典特征抽取x[&#x27;age&#x27;].fillna(x[&#x27;age&#x27;].mean(), inplace=True)# ②对于x转换成字典数据x=x.to_dict(orient=&quot;records&quot;) # [&#123;&quot;pclass&quot;: &quot;1st&quot;, &quot;age&quot;: 29.00, &quot;sex&quot;: &quot;female&quot;&#125;, &#123;&#125;]#4、划分数据集x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.2)#5、字典特征抽取transfer = DictVectorizer()x_train = transfer.fit_transform(x_train)x_test=transfer.transform(x_test) #不需要调用fit_transformprint(transfer.get_feature_names())print(&#x27;-----------------------------------------------------&#x27;)#print(x)#决策树不需要特征工程：标准化#6、决策树算法预估器(训练数据)estimator=DecisionTreeClassifier(criterion=&#x27;entropy&#x27;)estimator.fit(x_train,y_train)#7、模型评估#方法1：直接比对真实值和预测值y_predict=estimator.predict(x_test)print(&#x27;y_predict:\\n&#x27;,y_predict)print(&#x27;-----------------------------------------------------&#x27;)print(&#x27;直接比对真实值和预测值：\\n&#x27;,y_test y_predict)print(&#x27;-----------------------------------------------------&#x27;)#方法2：计算准确率score=estimator.score(x_test,y_test)print(&#x27;准确率为：\\n&#x27;,score)#8、可视化导出titan_tree.dot文档export_graphviz(estimator,out_file=&#x27;titan_tree.dot&#x27;,feature_names=transfer.get_feature_names()) 输出结果： 12345678910111213141516171819202122232425262728[&#x27;age&#x27;, &#x27;pclass=1st&#x27;, &#x27;pclass=2nd&#x27;, &#x27;pclass=3rd&#x27;, &#x27;sex=female&#x27;, &#x27;sex=male&#x27;]-----------------------------------------------------y_predict: [0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 1 0 1 0 0 1 1 0 0 1 1 0 0 1 0 0 0 0 0 1 0 0 0 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 1 1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1 0 1 0 0 0 0 0 0 0 1 1 0 1]-----------------------------------------------------直接比对真实值和预测值： 199 True1308 True814 True393 False560 True ... 955 True147 True30 True923 False404 TrueName: survived, Length: 263, dtype: bool-----------------------------------------------------准确率为： 0.779467680608365 决策树总结优点：可视化 - 可解释能力强 缺点：容易产生过拟合 改进： 减枝cart算法(决策树API当中已经实现，随机森林参数调优有相关介绍) 随机森林 集成学习方法之随机森林什么是集成学习方法集成学习通过建立几个模型组合的来解决单一预测问题。它的工作原理是生成多个分类器/模型，各自独立地学习和作出预测。这些预测最后结合成组合预测，因此优于任何一个单分类的做出预测。 什么是随机森林在机器学习中，随机森林是一个包含多个决策树的分类器，并且其输出的类别是由个别树输出的类别的众数而定。 随机森林原理过程训练集：特征值 +目标值 N个样本 M个特征 两个随机训练集随机 - N个样本中随机有放回的抽样N个 bootstrap(随机有放回抽样) [1, 2, 3, 4, 5]经过随机有放回抽样得到新的树的训练 [2, 2, 3, 1, 5] 特征随机 - 从M个特征中随机抽取m个特征​ 要求：M &gt;&gt; m 效果：降维 随机森林算法APIclass sklearn.ensemble.RandomForestClassifier(n_estimators=10, criterion=’gini’, max_depth=None, bootstrap=True, random_state=None, min_samples_split=2) 随机森林分类器 n_estimators：integer，optional（default = 10）森林里的树木数量120,200,300,500,800,1200 criterian：string，可选（default =“gini”）分割特征的测量方法 max_depth：integer或None，可选（默认=无）树的最大深度 5,8,15,25,30 max_features=”auto”,每个决策树的最大特征数量 ​ If “auto”, then max_features=sqrt(n_features). 对M求平方根​ If “sqrt”, then max_features=sqrt(n_features) (same as “auto”).​ If “log2”, then max_features=log2(n_features).​ If None, then max_features=n_features. bootstrap：boolean，optional（default = True） 是否在构建树时使用放回抽样 min_samples_split:节点划分最少样本数 min_samples_leaf:叶子节点的最小样本数 超参数：n_estimator, max_depth, min_samples_split,min_samples_leaf 案例：泰坦尼克号乘客生存预测代码： 1234567891011121314151617181920212223242526272829303132#复制上文中决策树代码比对决策树与随机森林效果#随机森林进行乘客生存预测from sklearn.ensemble import RandomForestClassifierestimator= RandomForestClassifier()# grid网格，search搜索，cv：cross_validation# 搜索算法最合适的参数from sklearn.model_selection import GridSearchCV#网格搜索GridSearchCV进行最佳参数的查找params =&#123;&quot;n_estimators&quot;: [120,200,300,500,800,1200], &quot;max_depth&quot;: [5, 8, 15, 25, 30]&#125;# cross_val_score类似estimator = GridSearchCV(estimator,param_grid=params,cv = 6)estimator.fit(x_train, y_train)#模型评估#方法1：直接比对真实值和预测值y_predict=estimator.predict(x_test)print(&#x27;---------------以下为随机森林预测效果-----------------&#x27;)print(&#x27;y_predict:\\n&#x27;,y_predict)print(&#x27;随机森林预测的直接比对真实值和预测值：\\n&#x27;,y_test y_predict)print(&#x27;-----------------------------------------------------&#x27;)#方法2：计算准确率score=estimator.score(x_test,y_test)print(&#x27;随机森林预测的准确率为：\\n&#x27;,score)print(&#x27;-----------------------------------------------------&#x27;)#查看了GridSearchCV最佳的参数组合#最佳参数：best_params_print(&#x27;最佳参数：\\n&#x27;,estimator.best_params_)#最佳结果：best_score_print(&#x27;最佳结果：\\n&#x27;,estimator.best_score_)#最佳估计器：best_estimator_print(&#x27;最佳估计器：\\n&#x27;,estimator.best_estimator_)#交叉验证结果：cv_results_#print(&#x27;交叉验证结果：\\n&#x27;,estimator.cv_results_) 输出结果：注：随机森林输出结果等待时间长 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970#决策树输出结果：[&#x27;age&#x27;, &#x27;pclass=1st&#x27;, &#x27;pclass=2nd&#x27;, &#x27;pclass=3rd&#x27;, &#x27;sex=female&#x27;, &#x27;sex=male&#x27;]-----------------------------------------------------y_predict: [1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 0 1 0 1 1 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 1 0 0 1 1 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 1 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 1 1 1 1 0 0]-----------------------------------------------------直接比对真实值和预测值： 379 False1099 True970 True482 True573 True ... 80 True295 True4 True1173 True684 TrueName: survived, Length: 263, dtype: bool-----------------------------------------------------准确率为： 0.7490494296577946 #随机森林输出结果：---------------以下为随机森林预测效果-----------------y_predict: [1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 0 1 0 1 1 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 1 1 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 0 0 0 0 1 1 1 1 1 0 0]随机森林预测的直接比对真实值和预测值： 379 False1099 True970 True482 True573 True ... 80 True295 True4 True1173 True684 TrueName: survived, Length: 263, dtype: bool-----------------------------------------------------随机森林预测的准确率为： 0.7908745247148289-----------------------------------------------------最佳参数： &#123;&#x27;max_depth&#x27;: 5, &#x27;n_estimators&#x27;: 300&#125;最佳结果： 0.8295238095238096最佳估计器： RandomForestClassifier(bootstrap=True, class_weight=None, criterion=&#x27;gini&#x27;, max_depth=5, max_features=&#x27;auto&#x27;, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=300, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False) 总结 在当前所有算法中，具有极好的准确率 能够有效地运行在大数据集上，处理具有高维特征的输入样本，而且不需要降维 能够评估各个特征在分类问题上的重要性 回归算法回归问题：目标值 - 连续型的数据 线性回归什么是线性回归线性回归(Linear regression)是利用回归方程(函数)**对一个或多个自变量(特征值)和因变量(目标值)之间**关系进行建模的一种分析方式。 特点：只有一个自变量的情况称为单变量回归（如：$y=kx+b$），大于一个自变量情况的叫做多元回归。 通用公式：$h_w(x)=w_1x_1+w_2x_2+w_3x_3…+b=w^Tx+b$ 其中$w$权重，$b$偏置 $w$和$x$可以理解为矩阵：$\\mathbf{w}=\\left(\\begin{array}{c}b \\ w_{1} \\ w_{2}\\end{array}\\right)$,$\\mathbf{x}=\\left(\\begin{array}{c}1 \\ x_{1} \\ x_{2}\\end{array}\\right)$ 线性回归当中的关系有两种，一种是线性关系，另一种是非线性关系。 例子： 线性关系注：如果在单特征与目标值的关系呈直线关系，或者两个特征与目标值呈现平面的关系 数据：工资和年龄（2个特征） 目标：预测银行会贷款给我多少钱（标签） 考虑：工资和年龄都会影响最终银行贷款的结果那么它们各自有多大的影响呢？（参数） $X1,X2$就是我们的两个特征（年龄，工资）$ Y$是银行最终会借给我们多少钱 找到最合适的一条线（想象一个高维）来最好的拟合我们的数据点 假设$w_1$是年龄的参数，$w_2$是工资的参数 拟合的平面：$h_w(x)=w_1x_1+w_2x_2$ 整合：$h_w(x)=w^Tx$ 非线性关系 如果是非线性关系，那么回归方程可以理解为：$w_1x_1+w_2x_2^2+w_3x_3^2$ 线性回归原理1、误差真实值和预测值之间肯定是要存在差异的（用$ε$来表示该误差） 对于每个样本：$y^{(i)}=w^{T} x^{(i)}+\\varepsilon^{(i)}$ 误差$\\varepsilon^{(i)}$是独立同分布，并且服从均值为0方差为$θ^2$的高斯分布(正态分布) 独立：张三和李四一起来贷款，他俩没关系 同分布：他俩都来得是我们假定的这家银行 高斯分布：银行可能会多给，也可能会少给，但是绝大多数情况下这个浮动不会太大，极小情况下浮动会比较大，符合正常情况 预测值与误差：$y^{(i)}=w^{T} x^{(i)}+\\varepsilon^{(i)}$ $(1)$ 由于误差服从高斯分布：$p(\\varepsilon^{(i)})=\\frac{1}{\\sqrt{2 \\pi} \\sigma} \\exp \\left(-\\frac{(\\varepsilon^{(i)})^{2}}{2 \\sigma^{2}}\\right)$ $(2)$ 将$(1)$式代入$(2)$式：$p(y^{(i)}|x^{(i)};w)=\\frac{1}{\\sqrt{2 \\pi} \\sigma} \\exp \\left(-\\frac{(y^{(i)}-w^Tx^{(i)})^{2}}{2 \\sigma^{2}}\\right)$ 似然函数：$L(w)=\\prod_{i=1}^{m} p\\left(y^{(i)} | x^{(i)} ; w\\right)=\\prod_{i=1}^{m} \\frac{1}{\\sqrt{2 \\pi} \\sigma} \\exp \\left(-\\frac{\\left(y^{(i)}-w^{T} x^{(i)}\\right)^{2}}{2 \\sigma^{2}}\\right)$ 解释：什么样的参数跟我们的数据组合后恰好是真实值 对数似然：$logL(w)=log\\prod_{i=1}^{m} \\frac{1}{\\sqrt{2 \\pi} \\sigma} \\exp \\left(-\\frac{\\left(y^{(i)}-w^{T} x^{(i)}\\right)^{2}}{2 \\sigma^{2}}\\right)$ 解释：乘法难解，加法就容易了，对数里面乘法可以转换成加法 展开化简：$\\sum_{i=1}^{m} \\log \\frac{1}{\\sqrt{2 \\pi} \\sigma} \\exp \\left(-\\frac{\\left(y^{(i)}-w^{T} x^{(i)}\\right)^{2}}{2 \\sigma^{2}}\\right)$ $=m \\log \\frac{1}{\\sqrt{2 \\pi} \\sigma}-\\frac{1}{\\sigma^{2}} \\cdot \\frac{1}{2} \\sum_{i=1}^{m}\\left(y^{(i)}-w^{T} x^{(i)}\\right)^{2}$ 目标：让似然函数（对数变换后也一样）越大越好 2、损失函数（Loss Function）$J(w)=\\frac{1}{2} \\sum_{i=1}^{m}\\left(y^{(i)}-w^{T} x^{(i)}\\right)^{2}=\\frac{1}{2} \\sum_{i=1}^{m}\\left(h_w(x^{(i)})-y^{(i)}\\right)^{2}$ 这里的这个损失函数就是著名的最小二乘损失函数 3、优化算法如何去求模型当中的W，使得损失最小？（目的是找到最小损失对应的W值） 线性回归经常使用的两种优化算法: 1)正规方程——天才（直接求解$w$） 目标函数：$J(w)=\\frac{1}{2} \\sum_{i=1}^{m}\\left(h_w(x^{(i)})-y^{(i)}\\right)^{2}=\\frac{1}{2}\\left(Xw-y\\right)^T\\left(Xw-y\\right)$ 求偏导：$$\\begin{aligned}&amp;\\nabla_{w} J(w)=\\nabla_{w}\\left(\\frac{1}{2}(X w-y)^{r}(X w-y)\\right)=\\nabla_{w}\\left(\\frac{1}{2}\\left(w^{T} X^{T}-y^{T}\\right)(X w-y)\\right)\\&amp;=\\nabla_{w}\\left(\\frac{1}{2}\\left(w^{T} X^{T} X w-w^{T} X^{T} y-y^{T} X w+y^{T} y\\right)\\right)\\&amp;=\\frac{1}{2}\\left(2 X^{T} X w-X^{T} y-\\left(y^{T} X\\right)^{\\bar{T}}\\right)=X^{T} X w-X^{T} y\\end{aligned}$$令偏导等于0得：$w=\\left(X^TX\\right)^{-1}X^Ty$ 理解：X为特征值矩阵，y为目标值矩阵。直接求到最好的结果 缺点：当特征过多过复杂时，求解速度太慢并且得不到结果 2）**梯度下降(Gradient Descent)**——勤奋努力的普通人（一步一步的求$w$） 理解：α为学习速率，需要手动指定（超参数），α旁边的整体表示方向 沿着这个函数下降的方向找，最后就能找到山谷的最低点，然后更新W值 使用：面对训练数据规模十分庞大的任务 ，能够找到较好的结果 我们通过两个图更好理解梯度下降的过程: 4、优化动态图演示 线性回归算法API方法一 sklearn.linear_model.LinearRegression(fit_intercept=True) 通过正规方程优化 fit_intercept：是否计算偏置 LinearRegression.coef_：回归系数_ LinearRegression.intercept_：偏置_ 方法二 sklearn.linear_model.SGDRegressor(loss=”squared_loss”, fit_intercept=True, learning_rate =’invscaling’, eta0=0.01) SGDRegressor类实现了随机梯度下降学习，它支持不同的loss函数和正则化惩罚项来拟合线性回归模型。 loss:损失类型 loss=”squared_loss”: 普通最小二乘法 fit_intercept：是否计算偏置 learning_rate : string, optional ​ 学习率填充 ​ ‘constant’: eta = eta0 ​ ‘optimal’: eta = 1.0 / (alpha * (t + t0)) [default] ​ ‘invscaling’: eta = eta0 / pow(t, power_t) power_t=0.25:存在父类当中 对于一个常数值的学习率来说，可以使用learning_rate=’constant’ ，并使用eta0来指定学习率。 SGDRegressor.coef_：回归系数_ _SGDRegressor.intercept_：偏置 注：sklearn提供给我们两种实现的API， 可以根据选择使用 波士顿房价预测代码：（补充完善https://blog.csdn.net/weixin_41890393/article/details/83589860） 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162#正规方程的优化方法对波士顿房价进行预测import numpy as npfrom sklearn import datasetsfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import StandardScalerfrom sklearn.linear_model import LinearRegressionfrom sklearn import metrics#1)获取数据x,y= datasets.load_boston(True)#平滑处理预测值y#平滑处理y值，x不处理。（x代表特征，y代表预测值）#y=np.log(y)#2）划分数据集x_train,x_test,y_train,y_test = train_test_split(x,y,random_state=22)#3)特征工程：标准化 不适合用标准化数据 一般用平滑处理#transfer=StandardScaler()#x_train=transfer.fit_transform(x_train)#x_test=transfer.transform(x_test)#4)正规方程的优化算法预估器estimator=LinearRegression()estimator.fit(x_train,y_train)#5)得出模型print(&#x27;正规方程权重系数为:\\n&#x27;,estimator.coef_)print(&#x27;正规方程偏置为:\\n&#x27;,estimator.intercept_)#6）模型评估y_predict=estimator.predict(x_test)print(&#x27;预测房价:\\n&#x27;,y_predict)print(&#x27;正规方程—均方误差MSE为:&#x27;, metrics.mean_squared_error(y_test, y_predict))#7）交叉验证from sklearn.model_selection import cross_val_predictpredicted = cross_val_predict(estimator,x,y,cv=10)print(&quot;MSE:&quot;, metrics.mean_squared_error(y, predicted))#6）可视化import matplotlib.pyplot as plt# scatterplt.scatter(y, predicted)# plotplt.plot([y.min(), y.max()], [y.min(), y.max()], &#x27;k--&#x27;, lw=4)plt.xlabel(&quot;Measured&quot;)plt.ylabel(&quot;Predicted&quot;)plt.show()#展示结果#创建画布plt.figure(figsize=(20,8),dpi=80)#绘图plt.rcParams[&#x27;font.sans-serif&#x27;]=&#x27;SimHei&#x27;plt.rcParams[&#x27;axes.unicode_minus&#x27;]=Falsex=range(len(y_predict))y1=y_predicty2=y_test#折线图plt.plot(x,y1,linestyle=&#x27;-&#x27;)plt.plot(x,y2,linestyle=&#x27;-.&#x27;)#增加图例plt.legend([&#x27;房价预测值&#x27;,&#x27;房价真实值&#x27;])plt.title(&#x27;波士顿房价走势图&#x27;)#展示plt.show() 输出结果：（注：特征有几个权重就有几个） 123456789101112正规方程权重系数为: [-0.64817766 1.14673408 -0.05949444 0.74216553 -1.95515269 2.70902585 -0.07737374 -3.29889391 2.50267196 -1.85679269 -1.75044624 0.87341624 -3.91336869]正规方程偏置为: 22.62137203166228预测房价: [28.22944896 31.5122308 21.11612841 32.6663189 20.0023467 19.07315705........................................................... 28.58237108]正规方程—均方误差MSE为: 20.6275137630954MSE: 34.53965953999329 代码： 123456789101112131415161718192021222324252627#梯度下降的优化方法对波士顿房价进行预测import numpy as npfrom sklearn import datasetsfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import StandardScalerfrom sklearn.linear_model import SGDRegressorfrom sklearn import metrics#1)获取数据x,y= datasets.load_boston(True)#2）划分数据集x_train,x_test,y_train,y_test = train_test_split(x,y,random_state=22)#3)特征工程：标准化 transfer=StandardScaler()x_train=transfer.fit_transform(x_train)x_test=transfer.transform(x_test)#4)梯度下降的优化算法预估器estimator=SGDRegressor(learning_rate=&#x27;constant&#x27;, eta0=0.01,max_iter=10000)#调参数estimator.fit(x_train,y_train)#5)得出模型print(&#x27;梯度下降权重系数为:\\n&#x27;,estimator.coef_)print(&#x27;梯度下降截距为:\\n&#x27;,estimator.intercept_)#6）模型评估y_predict=estimator.predict(x_test)print(&#x27;预测房价:\\n&#x27;,y_predict)print(&#x27;梯度下降—均方误差MSE为:&#x27;, metrics.mean_squared_error(y_test, y_predict)) 输出结果： 1234567891011梯度下降权重系数为: [-0.529811 0.93849958 -0.43482307 0.77305338 -1.71206925 2.82393382 -0.16256326 -3.09703859 1.63812767 -0.93714655 -1.72483573 0.88640923 -3.90806571]梯度下降截距为: [22.61725005]预测房价: [28.29992805 31.66102185 21.46408381 32.63060514 20.23508805 18.98298548........................................................... 28.33964857]梯度下降—均方误差MSE为: 21.004247881383602 正规方程和梯度下降对比 文字对比 梯度下降 正规方程 需要选择学习率 不需要 需要迭代求解 一次运算得出 特征数量较大可以使用 需要计算方程，时间复杂度高O(n3) 选择： 小规模数据： LinearRegression(不能解决拟合问题) 岭回归 大规模数据：SGDRegressor 拓展-关于优化方法BGD、SGD、MBGD、SAG目标函数：$J(w)=\\frac{1}{2m} \\sum_{i=1}^{m}\\left(y^{(i)}-h_w(x^{(i)})\\right)^{2}$ ① 批量梯度下降（batch gradient descent）：（容易得到最优解，但是由于每次考虑所有样本，速度很慢） $\\frac{\\partial J(w)}{\\partial w_{j}}=-\\frac{1}{m} \\sum_{i=1}^{m}\\left(y^{(i)}-h_{w}\\left(x^{(i)}\\right)\\right) x_{j}^{(i)}$ $w_{j}^{\\prime}=w_{j}+\\frac{α}{m} \\sum_{i=1}^{m}\\left(y^{(i)}-h_{w}\\left(x^{(i)}\\right)\\right) x_{j}^{(i)}$ 批量梯度下降的算法执行过程如下图： ② 随机梯度下降（Stochastic Gradient Descent, SGD）：（每次找一个样本，迭代速度快，但不一定每次都朝着收敛的方向，而是震荡的方式趋向极小点） $w_{j}^{\\prime}=w_{j}+α\\left(y^{(i)}-h_{w}\\left(x^{(i)}\\right)\\right) x_{j}^{(i)}$ SGD的优点是： 高效 容易实现 SGD的缺点是： SGD需要许多超参数：比如正则项参数、迭代数。 SGD对于特征标准化是敏感的。 ③ 小批量梯度下降法（Mini-batch gradient descent）：（每次更新选择一小部分数据来算，实用！ $w_{j}^{\\prime}=w_{j}+α\\frac{1}{10} \\sum_{k=i}^{i+9}\\left(y^{(k)}-h_{w}\\left(x^{(k)}\\right)\\right) x_{j}^{(k)}$ ④随机平均梯度法(Stochasitc Average Gradient)：（由于收敛的速度太慢，有人提出SAG等基于梯度下降的算法） Scikit-learn：SGDRegressor、岭回归、逻辑回归等当中都会有SAG优化 欠拟合与过拟合 欠拟合（模型过于简单） 过拟合（模型过于复杂） 解决办法 欠拟合解决办法 增加新的特征，可以考虑加入进特征组合、高次特征，来增大假设空间 采用非线性模型，比如核SVM 、决策树、DNN等模型 Boosting，Boosting 往往会有较小的 Bias，比如 Gradient Boosting 等 如果已正则化，尝试减少正则化程度$λ$ 过拟合解决办法 交叉检验，通过交叉检验得到较优的模型参数 特征选择，减少特征数或使用较少的特征组合，对于按区间离散化的特征，增大划分的区间 正则化，常用的有 L1、L2 正则 如果已正则化，尝试增大正则化程度λ 增加训练数据可以有限的避免过拟合 Bagging，将多个弱学习器Bagging 一下效果会好很多，比如随机森林等 正则化类别 L1正则化作用：可以使其中一些W的值直接为0，删除这个特征的影响。LOSSO回归 L2正则化作用：可以使得其中一些W的值都很小，都接近于0，削弱某个特征的影响。优点：越小的参数说明模型越简单，越简单的模型则越不容易产生过拟合现象。Ridge回归（岭回归）。加入L2正则化后的损失函数： $J(w)=\\frac{1}{2m} \\sum_{i=1}^{m}\\left(y^{(i)}-h_w(x^{(i)})\\right)^{2}+λ\\sum_{j=1}^{n}w_j^2$ 其中m为样本数，n为特征数 岭回归岭回归，其实也是一种线性回归。只不过在算法建立回归方程时候，加上正则化（L2）的限制，从而达到解决过拟合的效果。 参数推导线性回归模型的目标函数:$J(β)=\\sum\\left(y-Xβ\\right)^{2}$ 为了保证回归系数$β$可求，岭回归模型在目标函数上加了一个L2范数的惩罚项 $J(β)=\\sum\\left(y-Xβ\\right)^{2}+λ||β||_2^2=\\sum\\left(y-Xβ\\right)^{2}+\\sumλβ^2$ 其中$λ$为非负数，$λ$越大，则为了使$J(β)$最小，回归系数$β$就越小。推导过程： $\\begin{array}{c}J(\\beta)=(y-X \\beta)^{T}(y-X \\beta)+\\lambda \\beta^{T} \\beta \\ =y^{T} y-y^{T} X \\beta-\\beta^{T} X^{T} y+\\beta^{T} X^{T} X \\beta+\\lambda \\beta^{T} \\beta \\令 \\frac{\\partial J(\\beta)}{\\partial \\beta}=0 \\\\Rightarrow 0-X^{T} y-X^{T} y+2 X^{T} X \\beta+2 \\lambda \\beta=0 \\\\Rightarrow \\beta=\\left(X^{T} X+\\lambda I\\right)^{-1} X^{T} y\\end{array}$ L2范数惩罚项的加入使得$(X^{T}X+λI)$满秩，保证了可逆，但是也由于惩罚项的加$λ$，使得回归系数$β$的估计不再是无偏估计。所以岭回归是以放弃无偏性、降低精度为代价解决病态矩阵问题的回归方法。单位矩阵$I$的对角线上全是1，像一条山岭一样，这也是岭回归名称的由来。 $λ$的选择模型的方差：回归系数的方差模型的偏差：预测值和真实值的差异随着模型复杂度的提升，在训练集上的效果就越好，即模型的偏差就越小；但是同时模型的方差就越大。对于岭回归的$λ$而言，随着$λ$的增大，$(X^{T}X+λI)$就越大，$(X^{T}X+λI)^{-1}$就越小，模型的方差就越小；而$λ$越大使得$β$的估计值更加偏离真实值，模型的偏差就越大。所以岭回归的关键是找到一个合理的$λ$值来平衡模型的方差和偏差。根据凸优化，可以将岭回归模型的目标函数$J(β)$最小化问题等价于$\\left{\\begin{array}{l}\\operatorname{argmin}\\left{\\Sigma\\left(y-X \\beta^{2}\\right)\\right} \\ \\Sigma \\beta^{2} \\leq t\\end{array}\\right.$ 其中$t$为一个常数。以最简单的二维为例，即$β(β_1,β_2)$其几何图形是: 抛物面代表的是$\\sum(y-X\\beta)^2$的部分，圆柱体代表的是$β_1^{1}+β_2^{2}≤t$的部分。最小二乘解是抛物面的中心，岭回归解是抛物面与圆柱体的交点。岭回归的惩罚项$∑λβ^2$是关于回归系数$β$的二次函数，对目标函数求偏导时会保留$β$，抛物面与圆柱体很难相交于轴上使某个变量的回归系数为0，因此岭回归不能实现变量的剔除。 （1）岭迹法确定$λ$值由$β=(X^TX+λI)^{−1}X^Ty$ 可知$β$是$λ$的函数，当$λ∈[0,∞)$时，在平面直角坐标系中的$β−λ$曲线称为岭迹曲线。当$β$趋于稳定的点就是所要寻找的$λ$值。代码： 1234567891011121314151617181920212223242526272829import pandas as pdimport numpy as npfrom sklearn.model_selection import train_test_splitfrom sklearn.linear_model import Ridgeimport matplotlib.pyplot as pltdata=pd.read_csv(&#x27;diabetes.csv&#x27;)x=data.iloc[:,1:-1]y=data[&#x27;Outcome&#x27;]#拆分为训练集和测试集x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=1234)#构造不同的lambda值Lambdas=np.logspace(-5,2,200)#存放偏回归系数ridge_cofficients=[]for Lambda in Lambdas: ridge=Ridge(alpha=Lambda,normalize=True) ridge.fit(x_train,y_train) ridge_cofficients.append(ridge.coef_)#绘制岭迹曲线plt.rcParams[&#x27;font.sans-serif&#x27;]=[&#x27;Microsoft YaHei&#x27;]plt.rcParams[&#x27;axes.unicode_minus&#x27;]=Falseplt.style.use(&#x27;ggplot&#x27;)plt.plot(Lambdas,ridge_cofficients)#x轴做对数处理plt.xscale(&#x27;log&#x27;)plt.xlabel(&#x27;Log(Lambda)&#x27;)plt.ylabel(&#x27;Cofficients&#x27;)plt.show() 输出结果： 正则化力度λ越大，权重系数w越小 正则化力度λ越小，权重系数w越大 书上说在0.01附近大多数回归系数就趋于稳定，这哪看得出？所以定性的方法一般不太靠谱，还是用定量的方法吧！（2）交叉验证法确定$λ$值 交叉验证法的思想是，将数据集拆分为$k$个数据组(每组样本量大体相当)，从$k$组中挑选$k-1$组用于模型的训练，剩下的1组用于模型的测试，则会有$k-1$个训练集和测试集配对，每一种训练集和测试集下都会有对应的一个模型及模型评分（如均方误差），进而可以得到一个平均评分。对于$λ$值则选择平均评分最优的$λ$值。RidgeCV(alphas=(0.1, 1.0, 10.0), fit_intercept=True, normalize=False, scoring=None, cv=None, gcv_mode=None, store_cv_values=False)• lambdas：用于指定多个$λ$值的元组或数组对象，默认包含0.1,1,10三种值。 fit_intercept：bool类型，是否需要拟合截距项，默认为True。 normalize：bool类型，建模时是否对数据集做标准化处理，默认为False。 scoring：指定用于模型评估的度量方法。 cv：指定交叉验证的重数。gcv_mode：指定广义交叉验证的方法。 store_cv_values：bool类型，是否保存每个λ\\lambdaλ下交叉验证的评估信息，默认为False，只有cv为None时有效。 代码： 12345678910111213141516import pandas as pdimport numpy as npfrom sklearn.model_selection import train_test_splitfrom sklearn.linear_model import RidgeCVimport matplotlib.pyplot as pltdata=pd.read_csv(&#x27;diabetes.csv&#x27;)x=data.iloc[:,1:-1]y=data[&#x27;Outcome&#x27;]#拆分为训练集和测试集x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=1234)#构造不同的lambda值Lambdas=np.logspace(-5,2,200)#设置交叉验证的参数，使用均方误差评估ridge_cv=RidgeCV(alphas=Lambdas,normalize=True,scoring=&#x27;neg_mean_squared_error&#x27;,cv=10)ridge_cv.fit(x_train,y_train)print(ridge_cv.alpha_) 输出结果： 10.038720387818125535 代码实现Ridge(alpha=1.0, fit_intercept=True, normalize=False, copy_X=True, max_iter=None, tol=0.001, solver=‘auto’, random_state=None) 具有l2正则化的线性回归 alpha：正则化力度，也叫 λ，默认为1。λ取值：0.1~1 ~10 fit_intercept：bool类型，是否需要拟合截距项，默认为True。 normalize：bool类型，建模时是否对数据集做标准化处理，默认为False。 normalize=False:可以在fit之前调用preprocessing.StandardScaler标准化数据 copy_X：bool类型，是否复制自变量X的数值，默认为True。 max_iter：指定模型的最大迭代次数。 solver：指定模型求解最优化问题的算法，默认为’auto’。 sag:如果数据集、特征都比较大，选择该随机梯度下降优化 random_state：指定随机生成器的种子。 Ridge.coef_:回归权重_ Ridge.intercept_:回归偏置 All last four solvers support both dense and sparse data. However,only ‘sag’ supports sparse input when ‘fit_intercept’is True. Ridge方法相当于SGDRegressor(penalty=’l2’, loss=”squared_loss”),只不过SGDRegressor实现了一个普通的随机梯度下降学习，推荐使用Ridge(实现了SAG) sklearn.linear_model.RidgeCV(BaseRidgeCV, RegressorMixin) 具有l2正则化的线性回归，可以进行交叉验证 coef:回归系数 12345class _BaseRidgeCV(LinearModel): def __init__(self, alphas=(0.1, 1.0, 10.0), fit_intercept=True, normalize=False, scoring=None, cv=None, gcv_mode=None, store_cv_values=False): 代码： 123456789101112131415161718192021222324252627import pandas as pdimport numpy as npfrom sklearn.model_selection import train_test_splitfrom sklearn.linear_model import Ridge,RidgeCVdata=pd.read_csv(&#x27;diabetes.csv&#x27;)x=data.iloc[:,1:-1]y=data[&#x27;Outcome&#x27;]#拆分为训练集和测试集x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=1234)#构造不同的lambda值Lambdas=np.logspace(-5,2,200)#设置交叉验证的参数，使用均方误差评估ridge_cv=RidgeCV(alphas=Lambdas,normalize=True,scoring=&#x27;neg_mean_squared_error&#x27;,cv=10)ridge_cv.fit(x_train,y_train)print(ridge_cv.alpha_)#基于最佳lambda值建模ridge=Ridge(alpha=ridge_cv.alpha_,normalize=True)ridge.fit(x_train,y_train)#打印回归系数print(pd.Series(index=[&#x27;Intercept&#x27;]+x_train.columns.tolist(), data=[ridge.intercept_]+ridge.coef_.tolist()))#模型评估ridge_pred=ridge.predict(x_test)#均方误差from sklearn.metrics import mean_squared_errorMSE=mean_squared_error(y_test,ridge_pred)print(MSE) 输出结果： 12345678910110.038720387818125535Intercept -0.829372Glucose 0.005658BloodPressure -0.002019SkinThickness -0.000805Insulin -0.000016BMI 0.012776DiabetesPedigreeFunction 0.158436Age 0.004989dtype: float640.16870817670347735 LASSO回归参数推导岭回归无法剔除变量，而LASSO回归模型，将惩罚项由L2范数变为L1范数，可以将一些不重要的回归系数缩减为0，达到剔除变量的目的。 $J(β)=\\sum\\left(y-Xβ\\right)^{2}+λ||β||_1=\\sum\\left(y-Xβ\\right)^{2}+\\sumλ|β|=ESS（β）+λl_1(β)$ $λ$的选择直接使用交叉验证法 LassoCV(eps=0.001, n_alphas=100, alphas=None, fit_intercept=True, normalize=False, precompute=‘auto’, max_iter=1000, tol=0.0001, copy_X=True, cv=None, verbose=False, n_jobs=1, positive=False, random_state=None, selection=‘cyclic’) eps：指代λ\\lambdaλ最小值与最大值的商，默认为0.001。 n_alphas：指定λ\\lambdaλ的个数，默认为100个。 alphas：指定具体的λ\\lambdaλ列表用于模型的运算。 fit_intercept：bool类型，是否需要拟合截距项，默认为True。 normalize：bool类型，建模时是否对数据集做标准化处理，默认为False。 precompute：bool类型，是否在建模前计算Gram矩阵提升运算速度，默认为False。 max_iter：指定模型的最大迭代次数。 tol：指定模型收敛的阈值，默认为0.0001。 copy_X：bool类型，是否复制自变量X的数值，默认为True。 cv：指定交叉验证的重数。 verbose：bool类型，是否返回模型运行的详细信息，默认为False。 n_jobs：指定使用的CPU数量，默认为1，如果为-1表示所有CPU用于交叉验证的运算。 positive：bool类型，是否将回归系数强制为正数，默认为False。 random_state：指定随机生成器的种子。 selection：指定每次迭代选择的回归系数，如果为’random’，表示每次迭代中将随机更新回归系数；如果为’cyclic’，则每次迭代时回归系数的更新都基于上一次运算。 代码： 123456789101112131415import pandas as pdimport numpy as npfrom sklearn.model_selection import train_test_splitfrom sklearn.linear_model import LassoCVdata=pd.read_csv(&#x27;diabetes.csv&#x27;)x=data.iloc[:,1:-1]y=data[&#x27;Outcome&#x27;]#拆分为训练集和测试集x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=1234)#构造不同的lambda值Lambdas=np.logspace(-5,2,200)#设置交叉验证的参数，使用均方误差评估lasso_cv=LassoCV(alphas=Lambdas,normalize=True,cv=10,max_iter=10000)lasso_cv.fit(x_train,y_train)print(lasso_cv.alpha_) 输出结果： 10.00011357333583431052 代码实现Lasso(alpha=1.0, fit_intercept=True, normalize=False, precompute=False, copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, positive=False, random_state=None, selection=‘cyclic’) alphas：指定λ\\lambdaλ值，默认为1。 fit_intercept：bool类型，是否需要拟合截距项，默认为True。 normalize：bool类型，建模时是否对数据集做标准化处理，默认为False。 precompute：bool类型，是否在建模前计算Gram矩阵提升运算速度，默认为False。 copy_X：bool类型，是否复制自变量X的数值，默认为True。 max_iter：指定模型的最大迭代次数。 tol：指定模型收敛的阈值，默认为0.0001。 warm_start：bool类型，是否将前一次训练结果用作后一次的训练，默认为False。 positive：bool类型，是否将回归系数强制为正数，默认为False。 random_state：指定随机生成器的种子。 selection：指定每次迭代选择的回归系数，如果为’random’，表示每次迭代中将随机更新回归系数；如果为’cyclic’，则每次迭代时回归系数的更新都基于上一次运算。 代码： 1234567891011121314151617181920212223242526import pandas as pdimport numpy as npfrom sklearn.model_selection import train_test_splitfrom sklearn.linear_model import Lasso,LassoCVdata=pd.read_csv(&#x27;diabetes.csv&#x27;)x=data.iloc[:,1:-1]y=data[&#x27;Outcome&#x27;]#拆分为训练集和测试集x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=1234)#构造不同的lambda值Lambdas=np.logspace(-5,2,200)#设置交叉验证的参数，使用均方误差评估lasso_cv=LassoCV(alphas=Lambdas,normalize=True,cv=10,max_iter=10000)lasso_cv.fit(x_train,y_train)#基于最佳lambda值建模lasso=Lasso(alpha=lasso_cv.alpha_,normalize=True,max_iter=10000)lasso.fit(x_train,y_train)#打印回归系数print(pd.Series(index=[&#x27;Intercept&#x27;]+x_train.columns.tolist(), data=[lasso.intercept_]+lasso.coef_.tolist()))#模型评估lasso_pred=lasso.predict(x_test)#均方误差from sklearn.metrics import mean_squared_errorMSE=mean_squared_error(y_test,lasso_pred)print(MSE) 输出结果： 12345678910Intercept -0.842415Glucose 0.005801BloodPressure -0.001983SkinThickness -0.000667Insulin -0.000011BMI 0.012670DiabetesPedigreeFunction 0.153236Age 0.004865dtype: float640.16876008361250405 相对于岭回归而言，可以看到LASSO回归剔除了两个变量，降低了模型的复杂度，同时减少了均方误差，提高了模型的拟合效果。 逻辑回归逻辑回归（Logistic Regression）是机器学习中的一种分类模型，逻辑回归是一种分类算法，虽然名字中带有回归，但是它与回归之间有一定的联系。由于算法的简单和高效，在实际中应用非常广泛。 逻辑回归的应用场景 广告点击率 是否为垃圾邮件 是否患病 金融诈骗 虚假账号 目的：分类还是回归？经典的二分类算法！ 逻辑回归的决策边界：可以是非线性的 逻辑回归的原理输入 $h_w(x)=w_1x_1+w_2x_2+w_3x_3…+b=w^Tx+b$ Sigmoid函数 公式：$g(z)=\\frac{1}{1+e^{-z}}$ 自变量取值为任意实数，值域[0,1] 解释：将任意的输入映射到了[0,1]区间，我们在线性回归中可以得到一个预测值，再将该值映射到Sigmoid 函数中这样就完成了由值到概率的转换，也就是分类任务 预测函数：$$h_w(x)=g(w^Tx)=\\frac{1}{1+e^{-w^Tx}}$$分类任务： $P(y=1|x;w)=h_w(x)$ ; $P(y=0|x;w)=1-h_w(x)$ 整合得： $P(y|x;w)=(h_w(x))^y(1-h_w(x))^{1-y}$ 解释：对于二分类任务（0，1），整合后y取0只保留$(1-h_w(x))^{1-y}$，y取1只保留$(h_w(x))^y$ 损失以及优化1、损失逻辑回归的损失，称之为对数似然损失，公式如下： 分开类别： $$\\operatorname{cost}\\left(h_{\\theta}(x), y\\right)=\\left{\\begin{array}{ll}-\\log \\left(h_{\\theta}(x)\\right) &amp; \\text { if } y=1 \\-\\log \\left(1-h_{\\theta}(x)\\right) &amp; \\text { if } y=0\\end{array}\\right.$$ 综合完整损失函数：$$\\operatorname{cost}\\left(h_{\\theta}(x), y\\right)=\\sum_{i=1}^{m}-y_{i} \\log \\left(h_{\\theta}(x)\\right)-\\left(1-y_{i}\\right) \\log \\left(1-h_{\\theta}(x)\\right)$$ 2、优化同样使用梯度下降优化算法，去减少损失函数的值。这样去更新逻辑回归前面对应算法的权重参数，提升原本属于1类别的概率，降低原本是0类别的概率。 逻辑回归算法APIsklearn.linear_model.LogisticRegression(solver=’liblinear’, penalty=‘l2’, C = 1.0) solver:优化求解方式 默认开源的liblinear库实现，内部使用了坐标轴下降法来迭代优化损失函数 sag：根据数据集自动选择，随机平均梯度下降 penalty：正则化的种类 C：正则化力度 默认将类别数量少的当做正例 LogisticRegression方法相当于 SGDClassifier(loss=”log”, penalty=” “),SGDClassifier实现了一个普通的随机梯度下降学习，也支持平均随机梯度下降法（ASGD），可以通过设置average=True。而使用LogisticRegression(实现了SAG)良／恶性乳腺癌肿瘤预测 案例：癌症分类预测良/恶性乳腺癌肿瘤预测 数据描述: （1）699条样本，共11列数据，第一列用语检索的id，后9列分别是与肿瘤 相关的医学特征，最后一列表示肿瘤类型的数值。 （2）包含16个缺失值，用”?”标出。 原始数据的下载地址：https://archive.ics.uci.edu/ml/machine-learning-databases/ 分析: 缺失值处理 标准化处理 逻辑回归预测 代码： 123456789101112131415161718192021222324252627282930313233343536373839#逻辑回归进行癌症预测import numpy as npimport pandas as pdfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import StandardScalerfrom sklearn.linear_model import LogisticRegression# 1、读取数据column_name = [&#x27;Sample code number&#x27;, &#x27;Clump Thickness&#x27;, &#x27;Uniformity of Cell Size&#x27;, &#x27;Uniformity of Cell Shape&#x27;, &#x27;Marginal Adhesion&#x27;, &#x27;Single Epithelial Cell Size&#x27;, &#x27;Bare Nuclei&#x27;, &#x27;Bland Chromatin&#x27;, &#x27;Normal Nucleoli&#x27;, &#x27;Mitoses&#x27;, &#x27;Class&#x27;]data = pd.read_csv(&quot;https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data&quot;, names=column_name)# 2、数据处理—处理缺失值data = data.replace(to_replace=&#x27;?&#x27;, value=np.nan) #1)替换np.nandata = data.dropna() #2)删除缺失值print(data.isnull().any()) #确认不存在缺失值# 取出特征值x = data[column_name[1:10]] #x=data.iloc[:,1:-1]y = data[column_name[10]] #y=data[&#x27;Class&#x27;]#3、分割数据集x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3)#4、特征工程—标准化std = StandardScaler()x_train = std.fit_transform(x_train)x_test = std.transform(x_test)# 5、使用逻辑回归lr = LogisticRegression()lr.fit(x_train, y_train)#逻辑回归的模型参数：回归系数和偏置print(&quot;权重：\\n&quot;, lr.coef_)print(&quot;偏置：\\n&quot;, lr.intercept_)#6、模型评估#方法1：直接比对真实值和预测值y_predict=lr.predict(x_test)print(&#x27;y_predict:\\n&#x27;,y_predict)print(&#x27;直接比对真实值和预测值：\\n&#x27;,y_test y_predict)#方法2：计算准确率score=lr.score(x_test,y_test)print(&#x27;准确率为：\\n&#x27;,score) 输出结果： 123456789101112131415161718192021222324252627282930313233343536373839Sample code number FalseClump Thickness FalseUniformity of Cell Size FalseUniformity of Cell Shape FalseMarginal Adhesion FalseSingle Epithelial Cell Size FalseBare Nuclei FalseBland Chromatin FalseNormal Nucleoli FalseMitoses FalseClass Falsedtype: bool权重： [[1.4449604 0.10902357 0.64529009 1.02979746 0.2544256 1.55064687 0.92516667 0.62683691 0.54739363]]偏置： [-1.08426503]y_predict: #注：2——良性 4——恶性 [4 4 2 4 2 2 4 2 4 4 2 2 4 2 4 2 2 2 2 4 4 2 2 2 4 2 2 4 2 2 2 2 2 2 2 4 2 2 2 4 2 4 2 2 2 4 4 2 2 2 2 4 4 4 2 2 2 4 4 2 4 2 2 4 2 2 2 4 2 4 2 2 4 4 2 2 2 2 2 2 4 4 2 2 2 4 2 4 4 4 4 2 4 2 2 4 2 2 4 2 4 4 2 2 4 2 2 4 2 2 4 2 2 4 2 2 4 2 4 2 2 2 4 4 4 2 4 2 2 2 2 2 4 2 4 2 4 2 4 4 2 4 2 2 2 4 4 2 2 2 2 2 4 2 2 4 4 4 4 2 2 4 4 2 4 4 2 4 2 4 4 4 2 2 4 2 2 2 2 4 2 4 2 4 2 2 4 4 2 4 2 4 2 4 2 2 2 2 4 4 4 4 4 2 4]直接比对真实值和预测值： 221 True266 True4 True183 True341 True ... 55 True132 True15 True597 True479 TrueName: Class, Length: 205, dtype: bool准确率为： 0.9707317073170731 在很多分类场景当中我们不一定只关注预测的准确率！！！！！ 比如以这个癌症举例子！！！我们并不关注预测的准确率，而是关注在所有的样本当中，癌症患者有没有被全部预测（检测）出来。 分类模型的评估准确率、精确率、召回率、f1_score，混淆矩阵，ks，ks曲线，ROC曲线，psi等。 准确率、精确率、召回率、f1_score准确率（Accuracy）的定义是：对于给定的测试集，分类模型正确分类的样本数与总样本数之比； 代码示例： 1234567891011#1、准确率import numpy as npfrom sklearn.metrics import accuracy_scorey_pred = [0, 2, 1, 3,9,9,8,5,8]y_true = [0, 1, 2, 3,2,6,3,5,9]accuracy_score(y_true, y_pred)Out[127]: 0.33333333333333331 accuracy_score(y_true, y_pred, normalize=False) # 类似海明距离，每个类别求准确后，再求微平均Out[128]: 3 精确率(Precision)：预测结果为正例样本中真实为正例的比例（了解） 召回率(Recall)：真实为正例的样本中预测结果为正例的比例（查的全，对正样本的区分能力） 召回率：查得全不全 应用：质量检测 次品 那么怎么更好理解这个两个概念 F1_score：反映了模型的稳健型 在理想情况下，我们希望模型的精确率越高越好，同时召回率也越高越高，但是，现实情况往往事与愿违，在现实情况下，精确率和召回率像是坐在跷跷板上一样，往往出现一个值升高，另一个值降低，那么，有没有一个指标来综合考虑精确率和召回率了，这个指标就是F值。F值的计算公式为： $F=\\frac{(a^2+1)PR}{a^2*(P+R)}$ 式中：$P：Precision$，$R：Recall$，$a$：权重因子。 当$a=1$时，$F$值便是$F1$值，代表精确率和召回率的权重是一样的，是最常用的一种评价指标。 $F 1=\\frac{2 T P}{2 T P+F N+F P}=\\frac{2 \\cdot \\text { Precision } \\cdot \\text { Recall}}{\\text { Precision }+\\text { Recall}}$ 混淆矩阵在分类任务下，预测结果(Predicted Condition)与正确标记(True Condition)之间存在四种不同的组合，构成混淆矩阵(适用于多分类) 混淆矩阵一级指标（最底层的）： 真实值是positive，模型认为是positive的数量（True Positive=TP）； 真实值是positive，模型认为是negative的数量（False Negative=FN）：这就是统计学上的第一类错误（Type I Error）； 真实值是negative，模型认为是positive的数量（False Positive=FP）：这就是统计学上的第二类错误（Type II Error）； 真实值是negative，模型认为是negative的数量（True Negative=TN） 示例及实现代码 123456789101112# 假如有一个模型在测试集上得到的预测结果为：y_true=[1,0,0,2,1,0,3,3,3]# 实际的类别 y_pred=[1,1,0,2,1,0,1,3,3]# 模型预测的类别 #使用sklearn模块计算混淆矩阵from sklearn.metrics import confusion_matrixconfusion_mat=confusion_matrix(y_true,y_pred)print(confusion_mat) #看看混淆矩阵长啥样 [[2 1 0 0] [0 2 0 0] [0 0 1 0] [0 1 0 2]] 混淆矩阵可视化 12345678910111213import matplotlib.pyplot as pltimport numpy as npdef plot_confusion_matrix(confusion_mat): #将混淆矩阵画图并显示出来&#x27;&#x27;&#x27; plt.title(&#x27;Confusion matrix&#x27;) plt.imshow(confusion_mat,interpolation=&#x27;nearest&#x27;,cmap=plt.cm.gray) plt.colorbar() tick_marks=np.arange(confusion_mat.shape[0]) plt.xticks(tick_marks,tick_marks) plt.yticks(tick_marks,tick_marks) plt.ylabel(&#x27;True label&#x27;) plt.xlabel(&#x27;Predicted label&#x27;) plt.show()plot_confusion_matrix(confusion_mat) 分类评估报告API sklearn.metrics.classification_report(y_true, y_pred, labels=[], target_names=None ) y_true：真实目标值 y_pred：估计器预测目标值 labels:指定类别对应的数字 target_names：目标类别名称 return：每个类别精确率与召回率 上文逻辑回归进行癌症预测代码+下文代码： 123from sklearn.metrics import classification_reportreport=classification_report(y_test, lr.predict(x_test), labels=[2, 4], target_names=[&#x27;良性&#x27;, &#x27;恶性&#x27;])print(&quot;精确率和召回率为：&quot;,report) 输出结果： 12345678精确率和召回率为： precision recall f1-score support 良性 0.98 0.98 0.98 132 恶性 0.96 0.97 0.97 73 accuracy 0.98 205 macro avg 0.97 0.97 0.97 205weighted avg 0.98 0.98 0.98 205 二级指标 混淆矩阵里面统计的是个数，有时候面对大量的数据，光凭算个数，很难衡量模型的优劣。因此混淆矩阵在基本的统计结果上又延伸了如下4个指标，我称他们是二级指标（通过最底层指标加减乘除得到的）： 准确率（Accuracy）—— 针对整个模型 精确率（Precision） 灵敏度（Sensitivity）：就是召回率（Recall） 特异度（Specificity） 例： 假设这样一个情况，如果99个样本癌症，1个样本非癌症，不管怎样我全都预测正例(默认癌症为正例),准确率就为99%但是这样效果并不好，这就是样本不均衡下的评估问题 解：准确率：99% 召回率：99/99=100% 精确率：99% F1—score：2*99%/199%=99.497% AUC：0.5 —不好的模型 AUC下文介绍 TPR=100% FPR=1/1=100% 问题：如何衡量样本不均衡下的评估？ ROC曲线和AUC计算计算ROC值 1234567import numpy as npfrom sklearn.metrics import roc_auc_scorey_true = np.array([0, 0, 1, 1])y_scores = np.array([0.1, 0.4, 0.35, 0.8])print(roc_auc_score(y_true, y_scores))0.75 TPR = TP / (TP + FN) 所有真实类别为1的样本中，预测类别为1的比例 FPR = FP / (FP + FN) 所有真实类别为0的样本中，预测类别为1的比例 ROC曲线 ROC曲线的横轴就是FPR，纵轴就是TPR，当二者相等时，表示的意义则是：对于不论真实类别是1还是0的样本，分类器预测为1的概率是相等的，此时AUC为0.5 AUC指标 AUC(AUC值是一个概率值)的概率意义是随机取一对正负样本，正样本得分大于负样本的概率AUC的最小值为0.5，最大值为1，取值越高越好AUC=1，完美分类器，采用这个预测模型时，不管设定什么阈值都能得出完美预测。绝大多数预测的场合，不存在完美分类器。0.5&lt;AUC&lt;1，优于随机猜测。这个分类器（模型）妥善设定阈值的话，能有预测价值。 AUC就是ROC 曲线下的面积，通常情况下数值介于0.5-1之间，可以评价分类器的好坏，数值越大说明越好。 AUC计算API from sklearn.metrics import roc_auc_score sklearn.metrics.roc_auc_score(y_true, y_score) 计算ROC曲线面积，即AUC值 y_true:每个样本的真实类别，必须为0(反例),1(正例)标记 y_score:每个样本预测的概率值 代码（接上文代码）： 1234567891011121314151617181920212223242526print(y_test.head())#y_true:每个样本的真实类别，必须为0（反例），1（正例）标记#将y_test 转换成0和1y_test = np.where(y_test &gt; 2.5, 1, 0) #y_test数值大于2.5设置为1，不大于设置为0from sklearn.metrics import roc_auc_scoreprint(&quot;AUC指标：&quot;, roc_auc_score(y_test, lr.predict(x_test)))# ROC曲线# y_score为模型预测正例的概率y_score = lr.predict_proba(x_test)[:, 1]# 计算不同阈值下，fpr和tpr的组合之，fpr表示1-Specificity，tpr表示Sensitivityfrom sklearn import metricsfpr, tpr, threshold = metrics.roc_curve(y_test, y_score)# 计算AUCroc_auc = metrics.auc(fpr, tpr)# 绘制面积图import matplotlib.pyplot as pltplt.stackplot(fpr, tpr, color=&#x27;steelblue&#x27;, alpha=0.5, edgecolor=&#x27;black&#x27;)# 添加ROC曲线的轮廓plt.plot(fpr, tpr, color=&#x27;black&#x27;, lw=1)# 添加对角线作为参考线plt.plot([0, 1], [0, 1], color=&#x27;red&#x27;, linestyle=&#x27;--&#x27;)plt.text(0.5, 0.3, &#x27;ROC curve (area=%0.2f)&#x27; % roc_auc)plt.xlabel(&#x27;1-Specificity&#x27;)plt.ylabel(&#x27;Sensitivity&#x27;)plt.show() 输出结果： 123456753 4503 2434 2495 2595 2Name: Class, dtype: int64AUC指标： 0.9556899934597778 总结 AUC只能用来评价二分类 AUC非常适合评价样本不平衡中的分类器性能 LIft和gainLift图衡量的是，与不利用模型相比，模型的预测能力“变好”了多少，lift(提升指数)越大，模型的运行效果越好。Gain图是描述整体精准度的指标。计算公式如下： $Lift=\\frac{\\frac{TP}{TP+FP}}{\\frac{P}{P+N}}$ $Gain=\\frac{TP}{TP+FP}$ 作图步骤：（1） 根据学习器的预测结果（注意，是正例的概率值，非0/1变量）对样本进行排序（从大到小）—–这就是截断点依次选取的顺序；（2） 按顺序选取截断点，并计算Lift和Gain —也可以只选取n个截断点，分别在1/n，2/n，3/n等位置 回归模型的评估主要有以下方法： 指标 描述 metrics方法 Mean Absolute Error (MAE) 平均绝对误差 from sklearn.metrics import mean_absolute_error Mean Square Error(MSE) 平均方差 from sklearn.metrics import mean_squared_error R-Squared R平方值 from sklearn.metrics import r2_score 代码： 12345678#sklearn的调用from sklearn.metrics import mean_absolute_errorfrom sklearn.metrics import mean_squared_error from sklearn.metrics import r2_score mean_absolute_error(y_test,y_predict)mean_squared_error(y_test,y_predict)r2_score(y_test,y_predict) （一）平均绝对误差（Mean Absolute Error，MAE） 平均绝对误差就是指预测值与真实值之间平均相差多大 ： $MAE=\\frac{1}{m}\\sum_{i=1}^{m}|f_i-y_i|$ 平均绝对误差能更好地反映预测值误差的实际情况. （二）均方误差（Mean Squared Error，MSE） 观测值与真值偏差的平方和与观测次数的比值： $MSE=\\frac{1}{m}\\sum_{i=1}^{m}(f_i-y_i)^2$ 这也是线性回归中最常用的损失函数，线性回归过程中尽量让该损失函数最小。那么模型之间的对比也可以用它来比较。MSE可以评价数据的变化程度，MSE的值越小，说明预测模型描述实验数据具有更好的精确度。 （三）R-square(决定系数) $R^2=1-\\frac{\\sum(Y_actual-Y_predict)^2}{\\sum(Y_actual-Y_mean)^2}$ 123456#以线性回归为例lr = LinearRegression()lr.fit(x_train,y_train)y_ = lr.predict(X_test)# R2 决定系数lr.score(X_test,y_test) #与用from sklearn.metrics import r2_score相同 数学理解： 分母理解为原始数据的离散程度，分子为预测数据和原始数据的误差，二者相除可以消除原始数据离散程度的影响 其实“决定系数”是通过数据的变化来表征一个拟合的好坏。 理论上取值范围$ （-∞，1] $, 正常取值范围为[0 1] ——实际操作中通常会选择拟合较好的曲线计算R²，因此很少出现$-∞$ 越接近1，表明方程的变量对y的解释能力越强，这个模型对数据拟合的也较好 越接近0，表明模型拟合的越差 经验值：&gt;0.4， 拟合效果好 缺点：数据集的样本越大，R²越大，因此，不同数据集的模型结果比较会有一定的误差 （四）Adjusted R-Square (校正决定系数） $R^2_adjusted=1-\\frac{(1-R^2)(n-1)}{n-p-1}$ $n$为样本数量，$p$为特征数量 消除了样本数量和特征数量的影响 （五）交叉验证（Cross-Validation） 交叉验证，有的时候也称作循环估计（Rotation Estimation），是一种统计学上将数据样本切割成较小子集的实用方法，该理论是由Seymour Geisser提出的。在给定的建模样本中，拿出大部分样本进行建模型，留小部分样本用刚建立的模型进行预报，并求这小部分样本的预报误差，记录它们的平方加和。这个过程一直进行，直到所有的样本都被预报了一次而且仅被预报一次。把每个样本的预报误差平方加和，称为PRESS(predicted Error Sum of Squares)。 交叉验证的基本思想是把在某种意义下将原始数据(dataset)进行分组,一部分做为训练集(train set)，另一部分做为验证集(validation set or test set)。首先用训练集对分类器进行训练，再利用验证集来测试训练得到的模型(model)，以此来做为评价分类器的性能指标。 无论分类还是回归模型，都可以利用交叉验证，进行模型评估，示例代码： 123from sklearn.cross_validation import cross_val_scoreprint(cross_val_score(knn, X_train, y_train, cv=4))print(cross_cal_score(lr, X, y, cv=2)) 模型保存和加载当训练或者计算好一个模型之后，那么如果别人需要我们提供结果预测，就需要保存模型（主要是保存算法的参数） sklearn模型的保存和加载APIfrom sklearn.externals import joblib 保存：joblib.dump(rf, ‘test.pkl’) 加载：estimator = joblib.load(‘test.pkl’) 线性回归的模型保存加载案例保存 1234567#使用线性模型进行预测#使用正规方程求解lr = LinearRegression()#此时在干什么？lr.fit(x_train, y_train)#保存训练完结束的模型joblib.dump(lr, &quot;test.pkl&quot;) 加载 1234567891011#使用线性模型进行预测#使用正规方程求解#lr = LinearRegression()#此时在干什么？#lr.fit(x_train, y_train)#保存训练完结束的模型#joblib.dump(lr, &quot;test.pkl&quot;)#当上步模型已经保存好了就不需要训练fit和保存模型了#通过已有的模型去预测房价model = joblib.load(&quot;test.pkl&quot;)print(&quot;从文件加载进来的模型预测房价的结果：&quot;, std_y.inverse_transform(model.predict(x_test))) 无监督学习-K-means算法什么是无监督学习没有目标值—无监督学习 例： 一家广告平台需要根据相似的人口学特征和购买习惯将美国人口分成不同的小组，以便广告客户可以通过有关联的广告接触到他们的目标客户。 Airbnb 需要将自己的房屋清单分组成不同的社区，以便用户能更轻松地查阅这些清单。 一个数据科学团队需要降低一个大型数据集的维度的数量，以便简化建模和降低文件大小。 我们可以怎样最有用地对其进行归纳和分组？我们可以怎样以一种压缩格式有效地表征数据？这都是无监督学习的目标，之所以称之为无监督，是因为这是从无标签的数据开始学习的。 无监督学习包含算法 聚类 K-means(K均值聚类) 降维 PCA K-means原理我们先来看一下一个K-means的聚类效果图 K-means聚类步骤1、随机设置K个特征空间内的点作为初始的聚类中心 2、对于其他每个点计算到K个中心的距离，未知的点选择最近的一个聚类中心点作为标记类别 3、接着对着标记的聚类中心之后，重新计算出每个聚类的新中心点（平均值） 4、如果计算得出的新中心点与原中心点一样，那么结束，否则重新进行第二步过程 K—超参数 根据以下确定： 1）看需求；2）调节超参数 我们以一张图来解释效果 K-means算法APIsklearn.cluster.KMeans(n_clusters=8,init=‘k-means++’) k-means聚类 n_clusters:开始的聚类中心数量 init:初始化方法，默认为’k-means ++’ labels_:默认标记的类型，可以和真实值比较（不是值比较） 案例：k-means对Instacart Market用户聚类分析 1、PCA降维数据 2、k-means聚类 3、聚类结果显示 代码： 123456789101112131415161718192021222324252627282930313233343536373839import pandas as pd# 1、获取数据order_products = pd.read_csv(&quot;./instacart/order_products__prior.csv&quot;)products = pd.read_csv(&quot;./instacart/products.csv&quot;)orders = pd.read_csv(&quot;./instacart/orders.csv&quot;)aisles = pd.read_csv(&quot;./instacart/aisles.csv&quot;)# 2、合并表# order_products__prior.csv：订单与商品信息# 字段：order_id, product_id, add_to_cart_order, reordered# products.csv：商品信息# 字段：product_id, product_name, aisle_id, department_id# orders.csv：用户的订单信息# 字段：order_id,user_id,eval_set,order_number,….# aisles.csv：商品所属具体物品类别# 字段： aisle_id, aisle# 合并aisles和products aisle和product_idtab1 = pd.merge(aisles, products, on=[&quot;aisle_id&quot;, &quot;aisle_id&quot;])tab2 = pd.merge(tab1, order_products, on=[&quot;product_id&quot;, &quot;product_id&quot;])tab3 = pd.merge(tab2, orders, on=[&quot;order_id&quot;, &quot;order_id&quot;])#print(tab3.head())# 3、找到user_id和aisle之间的关系table = pd.crosstab(tab3[&quot;user_id&quot;], tab3[&quot;aisle&quot;])data = table[:10000] #缩小数据# 4、PCA降维from sklearn.decomposition import PCA# 1）实例化一个转换器类transfer = PCA(n_components=0.95)# 2）调用fit_transformdata_new = transfer.fit_transform(data)print(data_new.shape)#5、预估器流程from sklearn.cluster import KMeansestimator=KMeans(n_clusters=3)#KMeans(algorithm=&#x27;auto&#x27;, copy_x=True, init=&#x27;k-means++&#x27;, max_iter=300,# n_clusters=3, n_init=10, n_jobs=None, precompute_distances=&#x27;auto&#x27;,# random_state=None, tol=0.0001, verbose=0)estimator.fit(data_new)y_predict=estimator.predict(data_new)print(y_predict[:300]) 输出结果： 12345678910(10000, 42)[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 2 1 0 1 1 1 0 1 1 1 1 1 0 0 0 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 0 1 1 1 1 1 2 1 1 1 2 1 1 1 1 0 1 0 1 2 1 1 1 0 1 1 1 1 2 0 0 1 0 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 0 1 1 1 1 2 0 1 1 1 1 0 1 1 1 1 0] 问题：如何去评估聚类的效果呢？ Kmeans性能评估指标轮廓系数$SC_i=\\frac{b_i-a_i}{max(b_i,a_i)}$ 注：对于每个点 $i$为已聚类数据中的样本 ，$b_i$ 为 $i$ 到其它族群的所有样本的距离最小值，$a_i $为 $i$ 到本身簇的距离平均值。最终计算出所有的样本点的轮廓系数平均值 轮廓系数值分析 分析过程（我们以一个蓝1点为例） 1、计算出蓝1离本身族群所有点的距离的平均值a_i 2、蓝1到其它两个族群的距离计算出平均值红平均，绿平均，取最小的那个距离作为b_i 根据公式：极端值考虑：如果b_i &gt;&gt;a_i: 那么公式结果趋近于1；如果a_i&gt;&gt;&gt;b_i: 那么公式结果趋近于-1 结论如果$b_i&gt;&gt;a_i$:趋近于1效果越好，$ b_i&lt;&lt;a_i$:趋近于-1，效果不好。轮廓系数的值是介于 [-1,1] ，越趋近于1代表内聚度和分离度都相对较优。 轮廓系数APIsklearn.metrics.silhouette_score(X, labels) 计算所有样本的平均轮廓系数 X：特征值 labels：被聚类标记的目标值 用户聚类结果评估代码接上文代码： 123#6、模型评估—轮廓系数from sklearn.metrics import silhouette_scoreprint(silhouette_score(data_new,y_predict)) 输出结果： 10.5396819903993842 K-means总结 特点分析：采用迭代式算法，直观易懂并且非常实用 缺点：容易收敛到局部最优解(多次聚类) 注：聚类一般做在分类之前 SVM算法代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566#SVC算法进行癌症预测import numpy as npimport pandas as pdfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import StandardScaler# 1、读取数据column_name = [&#x27;Sample code number&#x27;, &#x27;Clump Thickness&#x27;, &#x27;Uniformity of Cell Size&#x27;, &#x27;Uniformity of Cell Shape&#x27;, &#x27;Marginal Adhesion&#x27;, &#x27;Single Epithelial Cell Size&#x27;, &#x27;Bare Nuclei&#x27;, &#x27;Bland Chromatin&#x27;, &#x27;Normal Nucleoli&#x27;, &#x27;Mitoses&#x27;, &#x27;Class&#x27;]data = pd.read_csv(&quot;https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data&quot;, names=column_name)# 2、数据处理—处理缺失值data = data.replace(to_replace=&#x27;?&#x27;, value=np.nan) #1)替换np.nandata = data.dropna() #2)删除缺失值print(data.isnull().any()) #确认不存在缺失值# 取出特征值x = data[column_name[1:10]] #x=data.iloc[:,1:-1]y = data[column_name[10]] #y=data[&#x27;Class&#x27;]#3、分割数据集x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3)#4、特征工程—标准化std = StandardScaler()x_train = std.fit_transform(x_train)x_test = std.transform(x_test)# 5、使用SVC算法from sklearn import svm #支持向量机#module = svm.SVC()module.fit(x_train, y_train)#6、模型评估#方法1：直接比对真实值和预测值y_predict=module.predict(x_test)print(&#x27;y_predict:\\n&#x27;,y_predict)print(&#x27;直接比对真实值和预测值：\\n&#x27;,y_test y_predict)#方法2：计算准确率score=module.score(x_test,y_test)print(&#x27;准确率为：\\n&#x27;,score)from sklearn.metrics import classification_reportreport=classification_report(y_test, module.predict(x_test), labels=[2, 4], target_names=[&#x27;良性&#x27;, &#x27;恶性&#x27;])print(&quot;精确率和召回率为：&quot;,report)print(y_test.head())#y_true:每个样本的真实类别，必须为0（反例），1（正例）标记#将y_test 转换成0和1y_test = np.where(y_test &gt; 2.5, 1, 0) #y_test数值大于2.5设置为1，不大于设置为0from sklearn.metrics import roc_auc_scoreprint(&quot;AUC指标：&quot;, roc_auc_score(y_test, module.predict(x_test)))# ROC曲线# y_score为模型预测正例的概率###通过decision_function()计算得到的y_score的值，用在roc_curve()函数中y_score = module.fit(x_train, y_train).decision_function(x_test)# 计算不同阈值下，fpr和tpr的组合之，fpr表示1-Specificity，tpr表示Sensitivityfrom sklearn import metricsfpr, tpr, threshold = metrics.roc_curve(y_test, y_score)# 计算AUCroc_auc = metrics.auc(fpr, tpr)# 绘制面积图import matplotlib.pyplot as pltplt.stackplot(fpr, tpr, color=&#x27;steelblue&#x27;, alpha=0.5, edgecolor=&#x27;black&#x27;)# 添加ROC曲线的轮廓plt.plot(fpr, tpr, color=&#x27;black&#x27;, lw=1)# 添加对角线作为参考线plt.plot([0, 1], [0, 1], color=&#x27;red&#x27;, linestyle=&#x27;--&#x27;)plt.text(0.5, 0.3, &#x27;ROC curve (area=%0.2f)&#x27; % roc_auc)plt.xlabel(&#x27;1-Specificity&#x27;)plt.ylabel(&#x27;Sensitivity&#x27;)plt.show() 输出结果： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950Sample code number FalseClump Thickness FalseUniformity of Cell Size FalseUniformity of Cell Shape FalseMarginal Adhesion FalseSingle Epithelial Cell Size FalseBare Nuclei FalseBland Chromatin FalseNormal Nucleoli FalseMitoses FalseClass Falsedtype: booly_predict: [2 4 2 2 2 2 2 2 2 4 2 2 4 4 2 2 2 2 2 2 4 4 2 2 2 4 4 4 2 4 4 4 2 2 2 2 2 4 2 2 2 2 4 2 2 2 4 2 4 2 2 2 2 4 4 4 2 4 2 2 2 2 4 2 4 2 2 4 2 2 2 2 4 2 2 2 2 2 2 2 2 4 2 2 2 2 2 4 2 2 2 2 4 2 4 2 2 4 2 2 2 4 4 2 2 2 2 4 4 2 4 2 4 2 4 4 2 2 2 4 4 4 2 4 2 2 2 2 2 2 4 2 2 2 2 2 2 4 2 4 4 2 4 2 2 2 4 2 4 4 2 4 4 2 2 2 2 2 2 2 4 4 2 2 2 2 4 2 4 2 2 4 2 2 4 4 2 2 4 2 2 4 4 2 4 4 2 4 4 2 2 2 4 2 2 2 2 4 2 4 2 4 4 4 2]直接比对真实值和预测值： 492 True236 True208 True518 True81 True ... 653 True434 False196 False210 True137 TrueName: Class, Length: 205, dtype: bool准确率为： 0.9658536585365853精确率和召回率为： precision recall f1-score support 良性 0.99 0.96 0.97 139 恶性 0.92 0.98 0.95 66 accuracy 0.97 205 macro avg 0.95 0.97 0.96 205weighted avg 0.97 0.97 0.97 205492 2236 4208 2518 281 2Name: Class, dtype: int64AUC指标： 0.9708415086112929 GBDT算法代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566#GBDT进行癌症预测import numpy as npimport pandas as pdfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import StandardScaler# 1、读取数据column_name = [&#x27;Sample code number&#x27;, &#x27;Clump Thickness&#x27;, &#x27;Uniformity of Cell Size&#x27;, &#x27;Uniformity of Cell Shape&#x27;, &#x27;Marginal Adhesion&#x27;, &#x27;Single Epithelial Cell Size&#x27;, &#x27;Bare Nuclei&#x27;, &#x27;Bland Chromatin&#x27;, &#x27;Normal Nucleoli&#x27;, &#x27;Mitoses&#x27;, &#x27;Class&#x27;]data = pd.read_csv(&quot;https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data&quot;, names=column_name)# 2、数据处理—处理缺失值data = data.replace(to_replace=&#x27;?&#x27;, value=np.nan) #1)替换np.nandata = data.dropna() #2)删除缺失值print(data.isnull().any()) #确认不存在缺失值# 取出特征值x = data[column_name[1:10]] #x=data.iloc[:,1:-1]y = data[column_name[10]] #y=data[&#x27;Class&#x27;]#3、分割数据集x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3)#4、特征工程—标准化std = StandardScaler()x_train = std.fit_transform(x_train)x_test = std.transform(x_test)# 5、使用GBDTfrom sklearn.ensemble import GradientBoostingClassifier #Gradient Boosting 和 AdaBoost算法#from sklearn.ensemble import GradientBoostingRegressormodule = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=1, random_state=0)module.fit(x_train, y_train)#6、模型评估#方法1：直接比对真实值和预测值y_predict=module.predict(x_test)print(&#x27;y_predict:\\n&#x27;,y_predict)print(&#x27;直接比对真实值和预测值：\\n&#x27;,y_test y_predict)#方法2：计算准确率score=module.score(x_test,y_test)print(&#x27;准确率为：\\n&#x27;,score)from sklearn.metrics import classification_reportreport=classification_report(y_test, module.predict(x_test), labels=[2, 4], target_names=[&#x27;良性&#x27;, &#x27;恶性&#x27;])print(&quot;精确率和召回率为：&quot;,report)print(y_test.head())#y_true:每个样本的真实类别，必须为0（反例），1（正例）标记#将y_test 转换成0和1y_test = np.where(y_test &gt; 2.5, 1, 0) #y_test数值大于2.5设置为1，不大于设置为0from sklearn.metrics import roc_auc_scoreprint(&quot;AUC指标：&quot;, roc_auc_score(y_test, module.predict(x_test)))# ROC曲线# y_score为模型预测正例的概率y_score = module.predict_proba(x_test)[:, 1]# 计算不同阈值下，fpr和tpr的组合之，fpr表示1-Specificity，tpr表示Sensitivityfrom sklearn import metricsfpr, tpr, threshold = metrics.roc_curve(y_test, y_score)# 计算AUCroc_auc = metrics.auc(fpr, tpr)# 绘制面积图import matplotlib.pyplot as pltplt.stackplot(fpr, tpr, color=&#x27;steelblue&#x27;, alpha=0.5, edgecolor=&#x27;black&#x27;)# 添加ROC曲线的轮廓plt.plot(fpr, tpr, color=&#x27;black&#x27;, lw=1)# 添加对角线作为参考线plt.plot([0, 1], [0, 1], color=&#x27;red&#x27;, linestyle=&#x27;--&#x27;)plt.text(0.5, 0.3, &#x27;ROC curve (area=%0.2f)&#x27; % roc_auc)plt.xlabel(&#x27;1-Specificity&#x27;)plt.ylabel(&#x27;Sensitivity&#x27;)plt.show()","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://qikaile.us/volantis/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://qikaile.us/volantis/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"Hexo之volantis主题标签插件合集","slug":"Volantis-hashtags","date":"2020-11-18T01:11:52.000Z","updated":"2020-11-20T04:43:29.273Z","comments":true,"path":"posts/61254.html","link":"","permalink":"https://qikaile.us/volantis/posts/61254.html","excerpt":"","text":"此文主要记录volantis主题内置标签插件的用法，以及常用的md语法，方便写文章时查阅 1 文本类1.1 行内文本样式下划线 1&#123;% u 下划线 %&#125; 带着重号的文本 1带&#123;% emp 着重号 %&#125;的文本 带 波浪线 的文本 1带 &#123;% wavy 波浪线 %&#125; 的文本 带 删除线 的文本 1带 &#123;% del 删除线 %&#125; 的文本 键盘样式的文本 command + D 1键盘样式的文本 &#123;% kbd command %&#125; + &#123;% kbd D %&#125; 密码样式的文本：这里没有验证码 1密码样式的文本：&#123;% psw 这里没有验证码 %&#125; 1.2 行内文本彩色文字：红色、黄色、绿色、青色、蓝色、灰色 1&#123;% span red, 红色 %&#125;、&#123;% span yellow, 黄色 %&#125;、&#123;% span green, 绿色 %&#125;、&#123;% span cyan, 青色 %&#125;、&#123;% span blue, 蓝色 %&#125;、&#123;% span gray, 灰色 %&#125; 超大号文字：Volantis 1&#123;% span center logo large, Volantis %&#125; 可以支持的参数样式参数位置可以写颜色、大小和对齐方向，多个样式参数用空格隔开。字体 1logo, code 颜色 1red, yellow, green, cyan, blue, gray 大小 1small, h4, h3, h2, h1, large, huge, ultra 对齐方向 1left, center, right 1.3 引用可以在配置文件中设置默认样式，为简单的一句话提供最的简便写法。 1&#123;% note, 可以在配置文件中设置默认样式，为简单的一句话提供最的简便写法。 %&#125; note quote 适合引用一段话 1&#123;% note quote, note quote 适合引用一段话 %&#125; note info 默认主题色，适合中性的信息 1&#123;% note info, note info 默认主题色，适合中性的信息 %&#125; note warning 默认黄色，适合警告性的信息 1&#123;% note warning, note warning 默认黄色，适合警告性的信息 %&#125; note error/danger 默认红色，适合危险性的信息 1&#123;% note danger, note error&#x2F;danger 默认红色，适合危险性的信息 %&#125; note done/success 默认绿色，适合正确操作的信息 1&#123;% note success, note done&#x2F;success 默认绿色，适合正确操作的信息 %&#125; note radiation 默认样式 1&#123;% note radiation, note radiation 默认样式 %&#125; note radiation yellow 可以加上颜色 1&#123;% note radiation yellow, note radiation yellow 可以加上颜色 %&#125; note bug red 说明还存在的一些故障 1&#123;% note bug red, note bug red 说明还存在的一些故障 %&#125; note link green 可以放置一些链接 1&#123;% note link green, note link green 可以放置一些链接 %&#125; note paperclip blue 放置一些附件链接 1&#123;% note paperclip blue, note paperclip blue 放置一些附件链接 %&#125; note todo 待办事项 1&#123;% note todo, note todo 待办事项 %&#125; note guide clear 可以加上一段向导 1&#123;% note guide clear, note guide clear 可以加上一段向导 %&#125; note download 可以放置下载链接 1&#123;% note download, note download 可以放置下载链接 %&#125;&#123;% note message gray, note message gray 一段消息 %&#125; note message gray 一段消息 1&#123;% note message gray, note message gray 一段消息 %&#125; note up 可以说明如何进行更新 1&#123;% note up, note up 可以说明如何进行更新 %&#125; note undo light 可以说明如何撤销或者回退 1&#123;% note undo light, note undo light 可以说明如何撤销或者回退 %&#125; 1.4 note绿色 红色 黄色 灰色 蓝色 12345&lt;p class&#x3D;&#39;div-border green&#39;&gt;绿色&lt;&#x2F;p&gt;&lt;p class&#x3D;&#39;div-border red&#39;&gt;红色&lt;&#x2F;p&gt;&lt;p class&#x3D;&#39;div-border yellow&#39;&gt;黄色&lt;&#x2F;p&gt;&lt;p class&#x3D;&#39;div-border grey&#39;&gt;灰色&lt;&#x2F;p&gt;&lt;p class&#x3D;&#39;div-border blue&#39;&gt;蓝色&lt;&#x2F;p&gt; 1.5 小tag标签红色小标签绿色小标签蓝色小标签黄色小标签灰色小标签 12345&lt;span class&#x3D;&quot;inline-tag red&quot;&gt;红色小标签&lt;&#x2F;span&gt;&lt;span class&#x3D;&quot;inline-tag green&quot;&gt;绿色小标签&lt;&#x2F;span&gt;&lt;span class&#x3D;&quot;inline-tag blue&quot;&gt;蓝色小标签&lt;&#x2F;span&gt;&lt;span class&#x3D;&quot;inline-tag yellow&quot;&gt;黄色小标签&lt;&#x2F;span&gt;&lt;span class&#x3D;&quot;inline-tag grey&quot;&gt;灰色小标签&lt;&#x2F;span&gt; 1.6 引用块可以在区块中放置一些复杂的结构，支持嵌套。 标题（可选）Windows 10不是為所有人設計,而是為每個人設計嵌套测试： 请坐和放宽，我正在帮你搞定一切… Folding 测试： 点击查看更多 不要说我们没有警告过你我们都有不顺利的时候 1234567891011121314151617&#123;% noteblock, 标题（可选） %&#125;Windows 10不是為所有人設計,而是為每個人設計&#123;% noteblock done %&#125;嵌套测试： 请坐和放宽，我正在帮你搞定一切...&#123;% endnoteblock %&#125;&#123;% folding yellow, Folding 测试： 点击查看更多 %&#125;&#123;% note warning, 不要说我们没有警告过你 %&#125;&#123;% noteblock bug red %&#125;我们都有不顺利的时候&#123;% endnoteblock %&#125;&#123;% endfolding %&#125;&#123;% endnoteblock %&#125; 1.7 复选列表 纯文本测试 支持简单的 markdown 语法 支持自定义颜色 绿色 + 默认选中 黄色 + 默认选中 青色 + 默认选中 蓝色 + 默认选中 增加 减少 叉 12345678910&#123;% checkbox 纯文本测试 %&#125;&#123;% checkbox checked, 支持简单的 [markdown](https:&#x2F;&#x2F;guides.github.com&#x2F;features&#x2F;mastering-markdown&#x2F;) 语法 %&#125;&#123;% checkbox red, 支持自定义颜色 %&#125;&#123;% checkbox green checked, 绿色 + 默认选中 %&#125;&#123;% checkbox yellow checked, 黄色 + 默认选中 %&#125;&#123;% checkbox cyan checked, 青色 + 默认选中 %&#125;&#123;% checkbox blue checked, 蓝色 + 默认选中 %&#125;&#123;% checkbox plus green checked, 增加 %&#125;&#123;% checkbox minus yellow checked, 减少 %&#125;&#123;% checkbox times red checked, 叉 %&#125; 1.8 单选列表 纯文本测试 支持简单的 markdown 语法 支持自定义颜色 绿色 黄色 青色 蓝色 1234567&#123;% radio 纯文本测试 %&#125;&#123;% radio checked, 支持简单的 [markdown](https:&#x2F;&#x2F;guides.github.com&#x2F;features&#x2F;mastering-markdown&#x2F;) 语法 %&#125;&#123;% radio red, 支持自定义颜色 %&#125;&#123;% radio green, 绿色 %&#125;&#123;% radio yellow, 黄色 %&#125;&#123;% radio cyan, 青色 %&#125;&#123;% radio blue, 蓝色 %&#125; 1.9 时间线时间线标题（可选） 时间节点（标题） 正文内容 时间节点（标题） 正文内容 123456789101112131415&#123;% timeline 时间线标题（可选） %&#125;&#123;% timenode 时间节点（标题） %&#125;正文内容&#123;% endtimenode %&#125;&#123;% timenode 时间节点（标题） %&#125;正文内容&#123;% endtimenode %&#125;&#123;% endtimeline %&#125; 2 链接、按钮、菜单2.1 链接卡片如何参与项目https://volantis.js.org/contributors/ 1&#123;% link 如何参与项目, https:&#x2F;&#x2F;volantis.js.org&#x2F;contributors&#x2F;, https:&#x2F;&#x2F;cdn.jsdelivr.net&#x2F;gh&#x2F;xaoxuu&#x2F;cdn-assets@master&#x2F;logo&#x2F;256&#x2F;safari.png %&#125; 2.2 按钮2.2.1 行内按钮不设置任何参数的 按钮 适合融入段落中。 1不设置任何参数的 &#123;% btn 按钮, &#x2F; %&#125; 适合融入段落中。 2.2.2 空心按钮示例博客 1&#123;% btn regular, 示例博客, https:&#x2F;&#x2F;xaoxuu.com, fas fa-play-circle %&#125; 2.2.3 居中显示开始使用 1&#123;% btn center large, 开始使用, https:&#x2F;&#x2F;volantis.js.org&#x2F;v3&#x2F;getting-started&#x2F;, fas fa-download %&#125; 2.2.4 富文本按钮如果需要显示类似「团队成员」之类的一组含有头像的链接： xaoxuu xaoxuu xaoxuu xaoxuu xaoxuu 1234567&#123;% btns circle grid5 %&#125;&#123;% cell xaoxuu, https:&#x2F;&#x2F;xaoxuu.com, https:&#x2F;&#x2F;cdn.jsdelivr.net&#x2F;gh&#x2F;xaoxuu&#x2F;cdn-assets&#x2F;avatar&#x2F;avatar.png %&#125;&#123;% cell xaoxuu, https:&#x2F;&#x2F;xaoxuu.com, https:&#x2F;&#x2F;cdn.jsdelivr.net&#x2F;gh&#x2F;xaoxuu&#x2F;cdn-assets&#x2F;avatar&#x2F;avatar.png %&#125;&#123;% cell xaoxuu, https:&#x2F;&#x2F;xaoxuu.com, https:&#x2F;&#x2F;cdn.jsdelivr.net&#x2F;gh&#x2F;xaoxuu&#x2F;cdn-assets&#x2F;avatar&#x2F;avatar.png %&#125;&#123;% cell xaoxuu, https:&#x2F;&#x2F;xaoxuu.com, https:&#x2F;&#x2F;cdn.jsdelivr.net&#x2F;gh&#x2F;xaoxuu&#x2F;cdn-assets&#x2F;avatar&#x2F;avatar.png %&#125;&#123;% cell xaoxuu, https:&#x2F;&#x2F;xaoxuu.com, https:&#x2F;&#x2F;cdn.jsdelivr.net&#x2F;gh&#x2F;xaoxuu&#x2F;cdn-assets&#x2F;avatar&#x2F;avatar.png %&#125;&#123;% endbtns %&#125; 或者含有图标的按钮： 下载源码 查看文档 1234&#123;% btns rounded grid5 %&#125;&#123;% cell 下载源码, &#x2F;, fas fa-download %&#125;&#123;% cell 查看文档, &#x2F;, fas fa-book-open %&#125;&#123;% endbtns %&#125; 圆形图标 + 标题 + 描述 + 图片 + 网格5列 + 居中 心率管家 专业版 心率管家 免费版 1234567891011121314&#123;% btns circle center grid5 %&#125;&lt;a href&#x3D;&#39;https:&#x2F;&#x2F;apps.apple.com&#x2F;cn&#x2F;app&#x2F;heart-mate-pro-hrm-utility&#x2F;id1463348922?ls&#x3D;1&#39;&gt; &lt;i class&#x3D;&#39;fab fa-apple&#39;&gt;&lt;&#x2F;i&gt; &lt;b&gt;心率管家&lt;&#x2F;b&gt; &#123;% p red, 专业版 %&#125; &lt;img src&#x3D;&#39;https:&#x2F;&#x2F;cdn.jsdelivr.net&#x2F;gh&#x2F;xaoxuu&#x2F;cdn-assets&#x2F;qrcode&#x2F;heartmate_pro.png&#39;&gt;&lt;&#x2F;a&gt;&lt;a href&#x3D;&#39;https:&#x2F;&#x2F;apps.apple.com&#x2F;cn&#x2F;app&#x2F;heart-mate-lite-hrm-utility&#x2F;id1475747930?ls&#x3D;1&#39;&gt; &lt;i class&#x3D;&#39;fab fa-apple&#39;&gt;&lt;&#x2F;i&gt; &lt;b&gt;心率管家&lt;&#x2F;b&gt; &#123;% p green, 免费版 %&#125; &lt;img src&#x3D;&#39;https:&#x2F;&#x2F;cdn.jsdelivr.net&#x2F;gh&#x2F;xaoxuu&#x2F;cdn-assets&#x2F;qrcode&#x2F;heartmate_lite.png&#39;&gt;&lt;&#x2F;a&gt;&#123;% endbtns %&#125; 2.3 GitHub卡片2.3.1 用户信息卡片 1&#123;% ghcard xaoxuu %&#125; 2.3.2 仓库信息卡片 12345| &#123;% ghcard volantis-x&#x2F;hexo-theme-volantis %&#125; | &#123;% ghcard volantis-x&#x2F;hexo-theme-volantis, theme&#x3D;vue %&#125; || -- | -- || &#123;% ghcard volantis-x&#x2F;hexo-theme-volantis, theme&#x3D;buefy %&#125; | &#123;% ghcard volantis-x&#x2F;hexo-theme-volantis, theme&#x3D;solarized-light %&#125; || &#123;% ghcard volantis-x&#x2F;hexo-theme-volantis, theme&#x3D;onedark %&#125; | &#123;% ghcard volantis-x&#x2F;hexo-theme-volantis, theme&#x3D;solarized-dark %&#125; || &#123;% ghcard volantis-x&#x2F;hexo-theme-volantis, theme&#x3D;algolia %&#125; | &#123;% ghcard volantis-x&#x2F;hexo-theme-volantis, theme&#x3D;calm %&#125; | 2.4 网站卡片xaoxuu简约风格 inkss这是一段关于这个网站的描述文字 MHuiG这是一段关于这个网站的描述文字 Colsrch这是一段关于这个网站的描述文字 Linhk1606这是一段关于这个网站的描述文字 2.5 下拉菜单 下拉菜单 主题源码 更新日志 有疑问？ 看 FAQ 看 本站源码 提 Issue 12345678910&#123;% menu 下拉菜单 %&#125;&#123;% menuitem 主题源码, https:&#x2F;&#x2F;github.com&#x2F;volantis-x&#x2F;hexo-theme-volantis&#x2F;, fas fa-file-code %&#125;&#123;% menuitem 更新日志, https:&#x2F;&#x2F;github.com&#x2F;volantis-x&#x2F;hexo-theme-volantis&#x2F;releases&#x2F;, fas fa-clipboard-list %&#125;&#123;% menuitem hr %&#125;&#123;% submenu 有疑问？, fas fa-question-circle %&#125;&#123;% menuitem 看 FAQ, &#x2F;faqs&#x2F; %&#125;&#123;% menuitem 看 本站源码, https:&#x2F;&#x2F;github.com&#x2F;volantis-x&#x2F;volantis-docs&#x2F; %&#125;&#123;% menuitem 提 Issue, https:&#x2F;&#x2F;github.com&#x2F;volantis-x&#x2F;hexo-theme-volantis&#x2F;issues&#x2F; %&#125;&#123;% endsubmenu %&#125;&#123;% endmenu %&#125; 3 容器类3.1 分栏标签名标签名。。。！！！ 12345678&#123;% tabs tab-id %&#125;&lt;!-- tab 标签名 --&gt;。。。&lt;!-- endtab --&gt;&lt;!-- tab 标签名 --&gt;！！！&lt;!-- endtab --&gt;&#123;% endtabs %&#125; 3.2 折叠框 查看图片测试 12345&#123;% folding 查看图片测试 %&#125;![](https:&#x2F;&#x2F;cdn.jsdelivr.net&#x2F;gh&#x2F;chuchendjs&#x2F;picgo&#x2F;picgo&#x2F;2.jpg)&#123;% endfolding %&#125; 查看默认打开的折叠框 这是一个默认打开的折叠框。 12345&#123;% folding cyan open, 查看默认打开的折叠框 %&#125;这是一个默认打开的折叠框。&#123;% endfolding %&#125; 查看代码测试 hello world! 123&#123;% folding green, 查看代码测试 %&#125; hello world!&#123;% endfolding %&#125; 查看列表测试 helloworld 123456&#123;% folding yellow, 查看列表测试 %&#125;- hello- world&#123;% endfolding %&#125; 查看嵌套测试 查看嵌套测试2 查看嵌套测试3 hahaha 12345678910111213&#123;% folding red, 查看嵌套测试 %&#125;&#123;% folding blue, 查看嵌套测试2 %&#125;&#123;% folding 查看嵌套测试3 %&#125;hahaha &#123;% endfolding %&#125;&#123;% endfolding %&#125;&#123;% endfolding %&#125; 4 多媒体类4.1 行内图片这是 一段话。 这又是 一段话。 123这是 &#123;% inlineimage https:&#x2F;&#x2F;cdn.jsdelivr.net&#x2F;gh&#x2F;volantis-x&#x2F;cdn-emoji&#x2F;aru-l&#x2F;0000.gif %&#125; 一段话。这又是 &#123;% inlineimage https:&#x2F;&#x2F;cdn.jsdelivr.net&#x2F;gh&#x2F;volantis-x&#x2F;cdn-emoji&#x2F;aru-l&#x2F;5150.gif, height&#x3D;40px %&#125; 一段话。 4.2 单张图片添加描述： 指定宽度： 指定宽度并添加描述： 设置占位背景色： 123456789101112131415添加描述：&#123;% image https:&#x2F;&#x2F;cdn.jsdelivr.net&#x2F;gh&#x2F;volantis-x&#x2F;cdn-wallpaper-minimalist&#x2F;2020&#x2F;025.jpg, alt&#x3D;每天下课回宿舍的路，没有什么故事。 %&#125;指定宽度：&#123;% image https:&#x2F;&#x2F;cdn.jsdelivr.net&#x2F;gh&#x2F;volantis-x&#x2F;cdn-wallpaper-minimalist&#x2F;2020&#x2F;025.jpg, width&#x3D;400px %&#125;指定宽度并添加描述：&#123;% image https:&#x2F;&#x2F;cdn.jsdelivr.net&#x2F;gh&#x2F;volantis-x&#x2F;cdn-wallpaper-minimalist&#x2F;2020&#x2F;025.jpg, width&#x3D;400px, alt&#x3D;每天下课回宿舍的路，没有什么故事。 %&#125;设置占位背景色：&#123;% image https:&#x2F;&#x2F;cdn.jsdelivr.net&#x2F;gh&#x2F;volantis-x&#x2F;cdn-wallpaper-minimalist&#x2F;2020&#x2F;025.jpg, width&#x3D;400px, bg&#x3D;#1D0C04, alt&#x3D;优化不同宽度浏览的观感 %&#125; 4.3 相册、组图4.3.1 一行一个图片 123&#123;% gallery %&#125;![图片描述](https:&#x2F;&#x2F;cdn.jsdelivr.net&#x2F;gh&#x2F;volantis-x&#x2F;cdn-wallpaper&#x2F;abstract&#x2F;41F215B9-261F-48B4-80B5-4E86E165259E.jpeg)&#123;% endgallery %&#125; 4.3.2 一行多个图片（不换行） 12345678910&#123;% gallery %&#125;![](https:&#x2F;&#x2F;cdn.jsdelivr.net&#x2F;gh&#x2F;volantis-x&#x2F;cdn-wallpaper&#x2F;abstract&#x2F;B951AE18-D431-417F-B3FE-A382403FF21B.jpeg)![](https:&#x2F;&#x2F;cdn.jsdelivr.net&#x2F;gh&#x2F;volantis-x&#x2F;cdn-wallpaper&#x2F;landscape&#x2F;AEB33F9D-7294-4CF1-B8C5-3020748A9D45.jpeg)![](https:&#x2F;&#x2F;cdn.jsdelivr.net&#x2F;gh&#x2F;volantis-x&#x2F;cdn-wallpaper&#x2F;landscape&#x2F;250662D4-5A21-4AAA-BB63-CD25CF97CFF1.jpeg)![](https:&#x2F;&#x2F;cdn.jsdelivr.net&#x2F;gh&#x2F;volantis-x&#x2F;cdn-wallpaper&#x2F;landscape&#x2F;10A0FCE5-36A1-4AD0-8CF0-019259A89E03.jpeg)![](https:&#x2F;&#x2F;cdn.jsdelivr.net&#x2F;gh&#x2F;volantis-x&#x2F;cdn-wallpaper&#x2F;abstract&#x2F;B951AE18-D431-417F-B3FE-A382403FF21B.jpeg)![](https:&#x2F;&#x2F;cdn.jsdelivr.net&#x2F;gh&#x2F;volantis-x&#x2F;cdn-wallpaper&#x2F;landscape&#x2F;AEB33F9D-7294-4CF1-B8C5-3020748A9D45.jpeg)![](https:&#x2F;&#x2F;cdn.jsdelivr.net&#x2F;gh&#x2F;volantis-x&#x2F;cdn-wallpaper&#x2F;landscape&#x2F;250662D4-5A21-4AAA-BB63-CD25CF97CFF1.jpeg)![](https:&#x2F;&#x2F;cdn.jsdelivr.net&#x2F;gh&#x2F;volantis-x&#x2F;cdn-wallpaper&#x2F;landscape&#x2F;10A0FCE5-36A1-4AD0-8CF0-019259A89E03.jpeg)&#123;% endgallery %&#125; 4.3.3 多行多个图片（每行2～8个图片） 12345678910&#123;% gallery stretch, 4 %&#125;![](https:&#x2F;&#x2F;cdn.jsdelivr.net&#x2F;gh&#x2F;volantis-x&#x2F;cdn-wallpaper&#x2F;abstract&#x2F;B951AE18-D431-417F-B3FE-A382403FF21B.jpeg)![](https:&#x2F;&#x2F;cdn.jsdelivr.net&#x2F;gh&#x2F;volantis-x&#x2F;cdn-wallpaper&#x2F;landscape&#x2F;AEB33F9D-7294-4CF1-B8C5-3020748A9D45.jpeg)![](https:&#x2F;&#x2F;cdn.jsdelivr.net&#x2F;gh&#x2F;volantis-x&#x2F;cdn-wallpaper&#x2F;landscape&#x2F;250662D4-5A21-4AAA-BB63-CD25CF97CFF1.jpeg)![](https:&#x2F;&#x2F;cdn.jsdelivr.net&#x2F;gh&#x2F;volantis-x&#x2F;cdn-wallpaper&#x2F;landscape&#x2F;10A0FCE5-36A1-4AD0-8CF0-019259A89E03.jpeg)![](https:&#x2F;&#x2F;cdn.jsdelivr.net&#x2F;gh&#x2F;volantis-x&#x2F;cdn-wallpaper&#x2F;abstract&#x2F;B951AE18-D431-417F-B3FE-A382403FF21B.jpeg)![](https:&#x2F;&#x2F;cdn.jsdelivr.net&#x2F;gh&#x2F;volantis-x&#x2F;cdn-wallpaper&#x2F;landscape&#x2F;AEB33F9D-7294-4CF1-B8C5-3020748A9D45.jpeg)![](https:&#x2F;&#x2F;cdn.jsdelivr.net&#x2F;gh&#x2F;volantis-x&#x2F;cdn-wallpaper&#x2F;landscape&#x2F;250662D4-5A21-4AAA-BB63-CD25CF97CFF1.jpeg)![](https:&#x2F;&#x2F;cdn.jsdelivr.net&#x2F;gh&#x2F;volantis-x&#x2F;cdn-wallpaper&#x2F;landscape&#x2F;10A0FCE5-36A1-4AD0-8CF0-019259A89E03.jpeg)&#123;% endgallery %&#125; 4.4 音频Your browser does not support the audio tag. 1&#123;% audio https:&#x2F;&#x2F;github.com&#x2F;volantis-x&#x2F;volantis-docs&#x2F;releases&#x2F;download&#x2F;assets&#x2F;Lumia1020.mp3 %&#125; 4.5 视频4.5.1 100%宽度Your browser does not support the video tag. 1&#123;% video https:&#x2F;&#x2F;github.com&#x2F;xaoxuu&#x2F;volantis-docs&#x2F;releases&#x2F;download&#x2F;assets&#x2F;IMG_0341.mov %&#125; 4.5.2 50%宽度Your browser does not support the video tag. Your browser does not support the video tag. Your browser does not support the video tag. Your browser does not support the video tag. 123456&#123;% videos, 2 %&#125;&#123;% video https:&#x2F;&#x2F;github.com&#x2F;xaoxuu&#x2F;volantis-docs&#x2F;releases&#x2F;download&#x2F;assets&#x2F;IMG_0341.mov %&#125;&#123;% video https:&#x2F;&#x2F;github.com&#x2F;xaoxuu&#x2F;volantis-docs&#x2F;releases&#x2F;download&#x2F;assets&#x2F;IMG_0341.mov %&#125;&#123;% video https:&#x2F;&#x2F;github.com&#x2F;xaoxuu&#x2F;volantis-docs&#x2F;releases&#x2F;download&#x2F;assets&#x2F;IMG_0341.mov %&#125;&#123;% video https:&#x2F;&#x2F;github.com&#x2F;xaoxuu&#x2F;volantis-docs&#x2F;releases&#x2F;download&#x2F;assets&#x2F;IMG_0341.mov %&#125;&#123;% endvideos %&#125; 4.5.3 25%宽度Your browser does not support the video tag. Your browser does not support the video tag. Your browser does not support the video tag. Your browser does not support the video tag. Your browser does not support the video tag. Your browser does not support the video tag. Your browser does not support the video tag. Your browser does not support the video tag. 12345678910&#123;% videos, 4 %&#125;&#123;% video https:&#x2F;&#x2F;github.com&#x2F;xaoxuu&#x2F;volantis-docs&#x2F;releases&#x2F;download&#x2F;assets&#x2F;IMG_0341.mov %&#125;&#123;% video https:&#x2F;&#x2F;github.com&#x2F;xaoxuu&#x2F;volantis-docs&#x2F;releases&#x2F;download&#x2F;assets&#x2F;IMG_0341.mov %&#125;&#123;% video https:&#x2F;&#x2F;github.com&#x2F;xaoxuu&#x2F;volantis-docs&#x2F;releases&#x2F;download&#x2F;assets&#x2F;IMG_0341.mov %&#125;&#123;% video https:&#x2F;&#x2F;github.com&#x2F;xaoxuu&#x2F;volantis-docs&#x2F;releases&#x2F;download&#x2F;assets&#x2F;IMG_0341.mov %&#125;&#123;% video https:&#x2F;&#x2F;github.com&#x2F;xaoxuu&#x2F;volantis-docs&#x2F;releases&#x2F;download&#x2F;assets&#x2F;IMG_0341.mov %&#125;&#123;% video https:&#x2F;&#x2F;github.com&#x2F;xaoxuu&#x2F;volantis-docs&#x2F;releases&#x2F;download&#x2F;assets&#x2F;IMG_0341.mov %&#125;&#123;% video https:&#x2F;&#x2F;github.com&#x2F;xaoxuu&#x2F;volantis-docs&#x2F;releases&#x2F;download&#x2F;assets&#x2F;IMG_0341.mov %&#125;&#123;% video https:&#x2F;&#x2F;github.com&#x2F;xaoxuu&#x2F;volantis-docs&#x2F;releases&#x2F;download&#x2F;assets&#x2F;IMG_0341.mov %&#125;&#123;% endvideos %&#125;","categories":[],"tags":[{"name":"hexo","slug":"hexo","permalink":"https://qikaile.us/volantis/tags/hexo/"}]},{"title":"PS功能精通课","slug":"PS-note","date":"2020-11-17T12:13:45.000Z","updated":"2020-11-17T13:47:21.207Z","comments":true,"path":"posts/48699.html","link":"","permalink":"https://qikaile.us/volantis/posts/48699.html","excerpt":"","text":"我用双手成就你的梦想课程结构精通14节+提升22节+实战36节 交作业M站：m.dapengjiaoyu.com 上传图片格式：JPG 单张作业不可大过2M、最多上传9张、9张不可大过9M 认识软件美国Adobe公司开发 中文：奥多比 PS：位图的处理软件 位图：图片缩小拉大之后图像会失真、色彩表现力好，文件大 PS的使用方向：摄影后期、效果图的后期处理、图像合成、电影海报、电商、网页设计、UI设计、等等…很多方面，较为广泛 Ai：矢量图的处理软件 矢量图：图片缩小拉大之后图像不会失真、色彩表现不如ps细腻、层次感不强、文件小 Ai的使用方向：印刷（名片）、包装盒的设计制作、企业VI手册设计（企业形象识别系统）、LOGO设计等 ID：专业书籍排版软件 ID的使用方向：杂志、广告设计、目录、零售商设计工作室和报纸出版方案等 PS基本操作1.新建画布（快捷键**Ctrl+N**） ①分辨率：打印时用300分辨率(大尺寸设计稿分比率会根据具体情况变小)；屏幕显示时用72分辨率 ②颜色模式：显示时用RGB RGB代表红绿蓝 ​ 打印时用CMYK CMYK代表青、品红、黄、黑 2.打开（在工作区域外） ①文件——打开（快捷键**Ctrl+O**） ②拖拽打开：在计算机里选中需要打开的文件点击并拖拽至**ps图标**上，然后移动到菜单栏或属性栏 3.置入（在工作区域内）：拖拽到画布中，需确定置入命令，可直接按**回车Enter**确定 4.保存（快捷键**Ctrl+S**）：格式psd留给设计师自己的源文件可再次修改， Jpg图片格式 不可修改， png透底图 背景透明 5.另存为：快捷键**Ctrl+Shift+S** 6.存储为web所用格式：快捷键**Ctrl+Shift+Alt+S**（可以更改素材质量大小，优化文件大小） 7.调整ps：编辑—首选项—暂存盘 快捷键**Ctrl+K**勾选所有硬盘可缓解卡顿 快捷键整理 新建画布：Ctrl+N 打开：Ctrl+O 保存：Ctrl+S 另存为：Ctrl+Shift+S 存储为web所用格式：Ctrl+Shift+Alt+S 揭开ps的神秘面纱画布操作1.放大缩小画布：**Alt+鼠标滚轮 或 Ctrl+加减键 或缩放工具（Alt缩小）** 2.百分之百显示：Ctrl+1 根据显示器显示：Ctrl+0（零） 3.移动画板：按住空格键可临时切换为抓手工具鼠标左键拖拽即可 4.撤销一步：**Ctrl+Z** 撤销多步：Ctrl+Alt+Z 还原多步：Ctrl+Shift+Z 5.自由变换：**Ctrl+T** ①普通变换：通过对角点拉伸，鼠标放在对角（出现弯箭头）旋转。按**Alt（中心点）和Shift（等比）以中心点等比例缩放** 图层基本操作1.解锁背景图层：点击图层后方锁头解锁 2.显示隐藏图层：点击该图层前的眼睛图标 3.删除图层：选中图层按delete键删除 或选中图层后点击下方垃圾桶图标 或选中图层拖拽至垃圾桶图标上 4.创建新图层：点击垃圾桶左侧的图标（快捷键Ctrl+Alt+Shift+N） 5.上色（前景色与背景色，需要自行添加新图层） 6.恢复默认前景色和背景色（黑白）：按D 7.切换前景色与背景色：点击弯箭头或按X 填充前景色：**Alt+Delete** 填充背景色：**Ctrl+Delete** 移动工具（快捷键V） 移动工具可以移动，可以复制，还可以跨画布复制 对齐：必须有**两个**及以上图层 分布：必须有**三个**及以上图层 快速选择图层：移动工具下按住ctrl可以临时切换到自动选择模式，同时按住Shift可以进行加选或减选 画笔（快捷键B）1.画笔加号情况：可能开了大写或画笔过大 2.画笔工具用于涂抹前景色 3.画笔大小更改：P后面【】 画笔硬度更改：Shift+P后面【】 快捷键整理 适应显示器显示：Ctrl+0 百分百显示：Ctrl+1 撤销：Ctrl+Z 自由变换：Ctrl+T 还原多步：Ctrl+Shift+Z 撤销多步：Ctrl+Alt+Z 画笔（快捷键：B） 新建图层：Ctrl+Alt+Shift+N 填充前景色：Alt+Delete 填充背景色：Ctrl+Delete 成为大神前的第一步图层操作1.复制图层 ①移动工具下，按住Alt点击并拖拽可以复制，按住Shift可以控制复制的图层水平或垂直方向平移 ②原位复制，快捷键**Ctrl+J** 2.编组：**Ctrl+G** 取消编组：Ctrl+Shift+G 3.合并图层：**Ctrl+E**（尽量少用） 4.分离图像：**Ctrl+Shift+J**把选中的内容剪切出来并放在原位 选框工具（快捷键M，切换工具Shift+M）1.选框：流动的虚线（蚂蚁线） 2.绘制方法：点击并拖拽（点击鼠标不要松开） 按住Shift绘制正图形选区，按住Alt以鼠标为中心绘制选区，按Shift+Alt以鼠标为中心绘制正图形选区 3.取消选区：**Ctrl+D** 4.羽化：制作边缘虚化的效果 属性栏处进行羽化时**选择羽化后填充**（选框绘画之前） 羽化需要选择—修改—羽化快捷键**Shift+F6**进行羽化(选框画完之后) 5.载入选区：按住Ctrl点击该图层的**图层缩览图** 选区的操作1.新选区：新绘制的选区每次都形成一个新选区 2.添加到选区**：新绘制的选区与之前绘制的选区进行相加（快捷方法先按住Shift**再绘制新选区） 3.从选区减去**：新绘制的选区与之前绘制的选区进行相减（快捷方法先按住Alt**再绘制新选区） 4.与选区交叉**：新绘制的选区与之前绘制的选区相交部分留下（快捷方法先按住Shift+Alt**再绘制新选区） 5.绘制时移动：绘制时按住空格键可以移动选区 快捷键整理 编组：Ctrl+G （原位）复制：Ctrl+J 合并图层：Ctrl+E 取消编组：Ctrl+Shift+G 填充背景色：Ctrl+Delete 填充前景色：Alt+Delete 取消选区：Ctrl+D 分离图像：Ctrl+Shift+J 羽化：Shift+F6 神奇的布尔运算矢量工具（快捷键U，切换工具Shift+U）1.**形状模式**：在绘制过程中会自动新建图层，默认自动填充前景色 2.**颜色填充**：纯色填充、渐变填充、图案填充 3.**图形描边**：纯色填充、渐变填充、图案填充；描边大小；描边选项 4.图形大小**：属性栏处可以精确调整大小或Ctrl+T** 5.图形绘制**：按住Shift可以绘制正图形，按住Alt键可以以鼠标为中心点绘制图形，按住Shift+Alt**可以以鼠标为中心点绘制正图形 6.圆角矩形**：绘制的时候先设置半径**，高版本可以在属性栏中修改(CS版本的就不好意思啦，不能在属性栏调整，因为没有这个功能，哈哈哈！！！) 7.多边形**：绘制的时候先设置多边形的边数**与平滑星星等 8.直线**：绘制的时候先改变线的粗细**，按住shift可以成角度约束 9.**自定形状**：软件预设好的形状，方便使用，还可以追加。 10.**自定形状追加**：设置—全部—追加 11.**定义自定形状**：选择想要定义的图层右击选择定义自定形状（必须有路径的图层） 12.布尔运算（通过形状层的加、减、交得到新的图形）：同选区操作 13.矢量图形计算后：**必须要合并形状组件** 小黑小白1.**小黑（A）**：移动和复制路径，单独选中图形 2.**小白（A）**：选择和移动路径的上锚点，以及调节控制手柄，按住Shift可以加选锚点 渐变工具在属性栏处选择一种渐变类型，并设置渐变颜色和其他属性等，创建渐变 快捷键整理 填充前景色：Alt+Delete 填充背景色：Ctrl+Delete 分离图像：Ctrl+Shift+J 取消选区：Ctrl+D 祖传抠图技法套索工具1.套索工具：大致框选，不适合精确抠图 2.多边形套索工具：适合抠有棱角的图片，直线（回车可快速成选区） 3.磁性套索工具：具有磁性，可以识别物体边缘（边缘清晰），操作发生偏移可以通过Delete进行点的删除（回车可快速成选区） a 宽度：该值决定了以光标中心为基准，其**周围有多少个像素**能够被工具检测到。边界清晰时数值高 b 对比度：设置工具感应图像边缘的**灵敏度**，图像清晰时数值高 c 频率：决定产生的**锚点数量**。数值越高，捕捉的边界越准确 快速选择工具（属于画笔类）1.调成大小：P后面的【】可以调节画笔大小 2.可以移动十字光标，快速连续选择相近的图像，会自动识别边缘（创建选框） 3.选区的反向选择：**Ctrl+Shift+I** 魔棒工具1.可以创建选区，选择颜色相近的范围 2.**容差值**越大，选择颜色相似的范围越大，5~35之间 3.不勾选连续时，主体物与背景颜色相近时，主体物也会选中 4.魔棒工具抠图适用情况：背景是纯色，或背景与主体物颜色差距大 色彩范围1.选择—色彩范围 选择改为**取样颜色** 图形下面选择**选择范围** 2.根据图像的颜色范围，进行创建选区 白色为被选中的，黑色没被选中的，灰色透明的 橡皮擦工具1.橡皮擦工具：直接擦涂，删掉不需要的图像 2.魔术橡皮擦：**直接删掉**颜色相近的区域，容差值与魔棒相同 其他辅助操作1.图层顺序：下/上移一层图层 Ctrl+【】 ​ 置底/置顶图层 Ctrl+Shift+【】 2.收缩（选择—修改）：在原有基础上进行缩小选区 快捷键整理 反向选则：Ctrl+Shift+I 下/上移动图层：Ctrl+【】 置底/置顶：Ctrl+Shift+【】 高效修图法污点修复画笔1.调节大小：P后面的【】进行调节 2.类型：**内容识别**（常用）/创建纹理/近似匹配 3.内容识别：点击需要修复的区域。软件会自动在他的周围进行取样，通过计算对其进行光线和明暗的匹配，并进行**羽化融合** 4.创建纹理：可以创建纹理，纹理为ps自带不可修改 5.近似匹配：使用工具边缘的像素来修补图像 扩散数值为画笔附近几像素的范围。（可以自动调节明暗） 修复画笔工具1.调节大小：P后面的【】进行调节 2.取样：在需要修复的**区域四周，找到颜色相似的区域，按住Alt键，鼠标点击进行取样，然后在需要修复的区域点击**或涂抹，（在修复时，修复画笔尽量要比修复的区域大，否则，修复效果不是很好。） 3.对齐：勾选对齐后吸取点**跟随**修复点移动，不勾选每次单击修复都是用同一吸取点去修复 4.图案：直接涂抹即可，不需要取样，类似图案叠加 修补工具1.源：选区位置**被鼠标停留位置覆盖** 2.目标：选区位置**覆盖**鼠标停留位置 内容感知移动工具可以移动画面当中物体的位置，移动 之后可以自动填充。可以在需要修改的位置绘制选区，移动选区到画布外，**留一小部分**选区再画布当中，来用于修补水印 红眼工具可以修复相机在光线昏暗的情况下，产生的红眼效果，点击红眼部位，会自动修复。（了解即可） 仿制图章1.使用方法同修复画笔一致 2.仿制图章工具与修复画笔工具的区别： ①仿制图章是**无损仿制**，取样什么颜色/皮肤，仿制的就是什么样子 ②修复画笔有一个运算过程，在涂抹当中将取样图像和目标位置**融合**，自动适应周围环境 图案图章工具选择图案可以涂背景，类似图案添加 液化（快捷键：**Ctrl+Shift+X**） 如果液化点不开或者灰色的，首选项-性能-使用图形处理器勾选上 1.位置：滤镜—液化 2.向前变形：可以制作瘦身瘦脸效果 3.重建工具：可以恢复之前的变形 4.顺时针旋转扭曲工具：按住alt键点击可以逆时针旋转 5.褶皱工具（挤压、褶皱效果） 6.膨胀工具（与褶皱工具相反） 7.左推工具（从上往下是往左推，从下往上是往右推） 8.冻结蒙版工具（保护图层） 9.解冻蒙版工具（取消冻结的蒙版） 10.人脸识别 11.移动工具、放大缩小 内容识别（快捷键：Shift+F5）通过绘制选区选择 需要修复的区域，软件会自动识别与画面不匹配的区域，进行修复图像 快捷键整理 液化 Ctrl+Shift+X 内容识别 Shift+F5 玩转钢笔钢笔工具（快捷键P）1.钢笔工具： ①绘制直线的方法：在起始点位置点击定点，连续**点击，按住Shift**键，可以绘制成角度的直线 ②绘制曲线的方法：在起始点位置点击定点，在下一点处**点击并拖拽鼠标，拉出弧线，会出现控制手柄，再一次绘制时，需要按住Alt**键取消一侧手柄 ③自动添加删除：可以直接在路径上点击添加锚点或者点击锚点删除锚点 ④临时切换：按住**Ctrl**键可以临时切换到小白工具进行锚点移动（自带控制手柄，可以调节弧度大小） ⑤将路径转换为选区：右击，选择建立选区、或**Ctrl+Enter回车**、或在路径面板下，Ctrl+路径缩览图 ⑥Delete键删除最后一个锚点的同时会结束钢笔工具这一次路径的绘制 2.自由钢笔工具：点击拖拽鼠标可以画出流畅的线条路径。右击路径，选择画笔勾选**模拟压力**（需先设置好画笔大小、硬度等） 3.转换点工具：点击曲线位置的点，可以将其变成直线。点击直线位置的点，选中并拖拽，可以出现控制手柄，调节弧度 路径面板1.路径面板可以实现选区与路径的互相转换 2.储存为jpg,psd时，路径面板可以储存路径，类似图层，便于抠图便于工作 画笔（快捷键：B）1.载入画笔：设置中找到载入画笔，找到画笔点击载入，右键可以删除画笔 2.画笔面板（快捷键F5）：形状动态、散布、颜色动态 3.定义画笔预设：编辑—定义画笔预设 画笔只认黑白灰，黑（实色颜色）、白（没有颜色）、灰（半透明） 多元化的文字文字工具（推荐：www.qiuziti.com来找字体） 1.横排文字蒙版（直排文字蒙版）工具：点击就会出现红色蒙版，输入文字确定后不会新建图层，并且文字会变为选区 2.横排文字（竖排文字）工具：点击会自动新建文字图层，可以再属性栏处更改文字属性 3.确定文字输入：属性栏的对勾 或Ctrl+Enter回车 或**小键盘**下的Enter 4.全选：Ctrl+A或双击文字图层缩览图 5.调节字间距：Alt+左右箭头 6.调节行间距：Alt+上下箭头 7.点文字：不会自动换行，换行需要手动回车进行换行，适合做标题文字 8.段文字（区域文字）：在画布上点击并拖拽拉出文本框，会自动换行，文字溢出时下方有加号提示，适合做说明文字 9.路径文字：用钢笔或者形状工具，绘制一段路径，将文字工具的光标放在路径上，点击输入文字。用小白调节文字形态 图层样式+图层混合模式混合模式（27个） 1.使用要求：必须**两个或两个以上**的图层才能进行混合 2.混合模式分组： A.组合模式：需要降低图层的不透明度才能产生作用 B.加深混合组：可以使图像**变暗**，将下方图层中的亮色被上方较暗的像素替代 C.减淡混合组：与加深混合组相反，可以使图像**变亮**，将下方图层中的暗色被上方较亮的像素替代 D.对比混合组：50%的灰色完全消失，高于50%灰的像素会使底图**变亮，低于50%灰的像素会使底图变暗** E.比较混合组：相同的区域显示为黑色，不同的区域显示为灰度层次或彩色。当图层中包含白色，白色区域会使底层图像反相，而黑色不会对底层图像产生影响。 F.色彩混合组：将色彩的色相、饱和度和亮度，替换给下方图层 3.重要的混合模式选项（4个） ①加深混合组：**正片叠底**（去白留黑） ②减淡混合组：**滤色**（去黑留白） ③比较混合组：**叠加**，使你的颜色跟下方图层进行有机的的叠加，同时修改下方图层的本身的亮度和明暗程度，比较柔和的效果 ④**柔光** ，效果更好，画面更融合 图层样式1.添加图层样式： ①双击图层缩览图的后方，弹出对话框 ②点击图层面板下方Fx按钮，添加图层样式 ③图层菜单中选择 ④在画布区域右击弹出**混合选项** 选择（移动工具、抓手工具、放大镜工具不可） 2.复制图层样式：按住Alt键点击图层样式Fx进行拖拽到需要复制的图层 或在图层上右击鼠标选择**拷贝图层样式** 在需要复制的图层上右击选择**粘贴图层样式** 3.填充：可以将颜色降低透明度，图层样式不变 蒙版带你领略台前幕后的故事快速蒙版（快捷键Q） 快速蒙版是一种**选区工具** 结合画笔工具使用，常用与影楼。双击快速蒙版，可以更改快速蒙版建立的选区形式 剪贴蒙版（上图下形）1.原理是将上层图层置于下层图层内，他们必须是**上下层关系** 2.下方图层可以是形状、图层、画笔、文字、智能对象 3.上图层右击选择创建剪贴蒙版，或按住Alt键，在上下图层之间移动，出现方框带箭头形状，单击鼠标左键，或**Ctrl+Alt+G **创建/释放 4.剪切蒙版可以**同时多个图层** 进行剪贴蒙版 图层蒙版（黑隐藏白显示灰色半透明）1.蒙版颜色表示的意义：**黑色：隐藏图像、白色：显示图像、灰色半透明** 蒙版只认黑白灰，除了黑白其他颜色都是不同程度的灰 2.可以在蒙版上添加颜色的方式：画笔、渐变、填充等 3.暂停蒙版使用：按住Shift点击图层蒙版缩览图 4.使用蒙版时容易出现的问题： ①在使用时出现涂抹颜色的情况，多数是没有添加或选择蒙版缩览图。 ②在使用蒙版时，涂抹无效果，看下当前前景色是否是白色 ③在使用蒙版时，涂抹无效果，看下画笔的透明度或流量是否是1% 通道1.作用：用于储存颜色信息，相当于颜色银行 2.第一个通道为复合通道，不同的通道会显示不同的颜色信息 3.单色通道中黑白灰的意义：白色表示颜色值最高255，黑色0，灰色0-255 4.Alpha通道的作用：可以储存和制作选区，黑色非选区，白色选区，灰色半透明选区 快捷键整理 创建/释放剪贴蒙版 Ctrl+Alt+G 打造滤镜下的艺术效果滤镜1.转换为智能滤镜：可以将普通的位图转为**智能对象** 2.滤镜使用规则：▼ RGB模式下**滤镜都可以使用** ▼ CMYK Lab 模式下有部分滤镜不能使用 ▼ 索引模式下滤镜不能使用. 3.智能滤镜的优点：**自带蒙版，可编辑性强**，可以对滤镜的效果单独进行多次修改或调整 4.上次滤镜操作：快捷键Ctrl+F可以再次执行**上次**的滤镜操作 5.渐隐：快捷键Ctrl+Shift+F 编辑—渐隐 对普通图层滤镜效果再编辑，可以调整不透明度和混合模式（不常用） 6.图像中有选区时，滤镜效果只对**选区内有效**，没有选区时，对整体图像有效 7.滤镜库：里面有现成的滤镜效果，可以通过**调成参数**从而改变滤镜的效果，通过下方新建按钮可以创建多个滤镜效果 8.滤镜库—素描：**多个**滤镜应用前/背景色，亮部应用背景色，暗部应用前景色 9.▷ 自适应广角：可较正广角导致的变形图像 ▷ camera raw滤镜：可以调整图像颜色等 ▷ 镜头较正：可制作鱼眼镜头拍摄产生的效果，可以对照片的畸变和暗角进行一定程度的矫正 ▷ 液化：可通过平移、旋转、进行像素变形（瘦身瘦脸等） ▷ 消失点：可以通过内置透视网格、进行图像修图（透视：近大远小效果） ▷ 模糊系列：可以根据参数对图像进行各种形式整体模糊处理。局部模糊可以通过选框进行控制。 ▷ 扭曲系列：根据不同方式对图像进行整体像素变形 ▷ 锐化系列：对图像进行整体边缘对比强化，使图片整体更加清晰。如果参数过大图像会损坏。 ▷ 杂色：添加杂色：可以添加颗粒杂色 ▷ 其他滤镜：高反差保留：可以保留细节与叠加柔光一起使用 10.渲染—云彩：可以应用没有像素的区域（空白图层），应用的是前/背景色 快捷键整理 重复上次滤镜 Ctrl+Alt＋F 低版本重复上次滤镜 Ctrl＋F 渐隐 Ctrl+Shift+F 让我们换一种颜色看世界（调色一）色彩模式1.RGB：**光学三原色**，也是调色运用最多的一种颜色模式 2.CMYK：印刷用的颜色 青、洋红、黄、黑 3.灰度模式：图像不包含颜色，只有黑白灰三种颜色，并影响之后的颜色使用 4.去色（**Ctrl+Shift+U **）：把图像的饱和度降到最低，不影响色彩模式，对于之后的颜色使用没有影响 5.更改模式：菜单栏—图像—模式 调色1.调整面板：点击效果，直接新建图层，自带图层蒙版，可以多次调解，只对下方图层起作用 2.亮度/对比度：亮度、添加/减少图像明暗程度 ​ 对比度、增加/降低图像明暗对比程度 3.色相/饱和度**Ctrl+U **：色相、色彩的相貌 ​ 饱和度、颜色鲜艳程度 ​ 明度、颜色的明暗程度 ​ 单色制作时勾选着色 4.三原色：红绿蓝 间色：原色+原色=间色 黄、洋红、青 互补色（反色）：可以互相抵消的颜色、180°的颜色、相对的颜色 三对互补色：**红色与青色 蓝色与黄色 绿色与洋红 ** 5.色彩平衡（**Ctrl+B **）：可以根据颜色的色相来调节 6.渐变映射：一般结合混合模式和不透明度来使用 通俗的说就是用你所设定的颜色，对应到原图的色彩上。用这种效果可以达到某些非常夸张的色彩搭配效果。 7.可选颜色：对单一颜色进行调整 “相对”比较柔和 “绝对”比较犀利 相对运用的较多 8.替换颜色：图像—调整—替换颜色 拾取一种颜色，选择另一种颜色替换（可以用添加吸管添加颜色） 快捷键整理 去色 Ctrl+Shift+U 从昏暗到光明（调色二）色阶（快捷键Ctrl+L）1.输入色阶——数字图像本来的色阶范围，通过调节改变**黑白灰范围** 输出色阶——是指为打印机指定最小的暗调色阶和最大的高光色阶，调整时调整的是**整体的明暗度 ** 2.可以调整图像的阴影、中间调和高光的强度级别，校正色调范围和色彩平衡 曲线（快捷键Ctrl+M）它整合了“色阶”、“亮度/对比度”等多个命令的功能。曲线上可以添加**14个**控点，移动这些控制点可以对色彩和色调进行非常精确的调整 a 按Shift 点击可以选中并控制多个控制点 b 点击Delete键可以删掉控制点 照片滤镜可用于矫正照片的颜色 颜色查找查询颜色，形成滤镜效果 快捷键总结","categories":[{"name":"photoshop","slug":"photoshop","permalink":"https://qikaile.us/volantis/categories/photoshop/"}],"tags":[{"name":"photoshop","slug":"photoshop","permalink":"https://qikaile.us/volantis/tags/photoshop/"}]},{"title":"markdown笔记","slug":"markdown-note","date":"2020-11-17T10:50:06.000Z","updated":"2020-11-17T10:52:28.341Z","comments":true,"path":"posts/30600.html","link":"","permalink":"https://qikaile.us/volantis/posts/30600.html","excerpt":"","text":"1. 欢迎使用 Cmd Markdown 编辑阅读器 将以下代码，icon图标由markdown就会显示网站地址 我们理解您需要更便捷更高效的工具记录思想，整理笔记、知识，并将其中承载的价值传播给他人，Cmd Markdown 是我们给出的答案 —— 我们为记录思想和分享知识提供更专业的工具。 您可以使用 Cmd Markdown： 整理知识，学习笔记 发布日记，杂文，所见所想 撰写发布技术文稿（代码支持） 撰写发布学术论文 一些常用的 Emoji 符号（可直接复制） 🐌：云同步速度让人失望，用户体验感不佳📱：对苹果系统用户不友好，移动端使用感差😭：学习门槛高，初学者不易上手♍：笔记排版混乱，始终不知道该怎样对齐✨：功能。🌼 🎷🍀月之影✨🌙✨ 介绍：🎵乐随心动......🌙✨🌼🍃 ✌🈚☑☒🔘★☀✅文本图标链接📖打开的书 📚图书 🔖书签 📓笔记本 $$J(\\theta) = \\frac 1 2 \\sum_{i=1}^m (h_\\theta(x^{(i)})-y^{(i)})^2$$ 换行和段落如果需要换行，您应该在当前行末尾添加两个空格，然后换行。VNote提供快捷键Shift+Enter来辅助用户输入两个空格并换行。如果需要一个新的段落，您应该先插入一个空行然后才输入新的段落的文本。 一般来说，您应该在一个块元素（例如代码块、列表和块引用）后面插入一个空行来显式结束该元素。该指南参考了 Mastering Markdown 除了您现在看到的这个 Cmd Markdown 在线版本，您还可以前往以下网址下载： 1.1.1. Windows/Mac/Linux 全平台客户端 请保留此份 Cmd Markdown 的欢迎稿兼使用说明，如需撰写新稿件，点击顶部工具栏右侧的 新文稿 或者使用快捷键 Ctrl+Alt+N。 1.2. 什么是 MarkdownMarkdown 是一种方便记忆、书写的纯文本标记语言，用户可以使用这些标记符号以最小的输入代价生成极富表现力的文档：譬如您正在阅读的这份文档。它使用简单的符号标记不同的标题，分割不同的段落，粗体 或者 斜体 某些文字，更棒的是，它还可以 1.2.1. 制作一份待办事宜 Todo 列表 支持以 PDF 格式导出文稿 改进 Cmd 渲染算法，使用局部渲染技术提高渲染效率 新增 Todo 列表功能 修复 LaTex 公式渲染问题 新增 LaTex 公式编号功能 1.2.2. 书写一个质能守恒公式[^LaTeX]This is the 1^st^ superscript.上下标 $$E=mc^2$$ ![](https://i.loli.net/2020/05/02/xR6vMhCgNrc7Uu2.jpg =400x300) 颜色的标记红色 ::: alert-primary 这是一个基础文本。 ::: ::: alert-success 这是一个成功文本。 ::: ::: alert-info 这是一个信息文本。 ::: ::: alert-danger 这是一个危险文本。 ::: 1.2.3. 高亮一段代码[^code]1234567@requires_authorizationclass SomeClass: passif __name__ == &#x27;__main__&#x27;: # A comment print &#x27;hello world&#x27; 1.2.4. 高效绘制 流程图123456789st&#x3D;&gt;start: 开始框op&#x3D;&gt;operation: 处理框cond&#x3D;&gt;condition: 判断框(是或否?)sub1&#x3D;&gt;subroutine: 子流程io&#x3D;&gt;inputoutput: 输入输出框e&#x3D;&gt;end: 结束框st-&gt;op-&gt;condcond(yes)-&gt;io-&gt;econd(no)-&gt;sub1(right)-&gt;op 123456graph TDA[模块A] --&gt;|A1| B(模块B)B --&gt; C&#123;判断条件C&#125;C --&gt;|条件C1| D&gt;模块D]C --&gt;|条件C2| E((模块E))C --&gt;|条件C3| F[&quot;模块F(引号可转义特殊字符)&quot;] 1.2.5. 高效绘制 序列图12345%% 时序图例子,-&gt; 直线，--&gt;虚线，-&gt;&gt;实线箭头sequenceDiagramAlice-&gt;&gt;Bob: Hello Bob, how are you?Note right of Bob: Bob thinksBob--&gt;&gt;Alice: I am good thanks! 1234@startumlBob -[#red]&gt; Alice : helloAlice -[#0000FF]-&gt;Bob : ok@enduml 123456789101112131415@startuml!include &lt;cloudinsight&#x2F;tomcat&gt;!include &lt;cloudinsight&#x2F;kafka&gt;!include &lt;cloudinsight&#x2F;java&gt;!include &lt;cloudinsight&#x2F;cassandra&gt;title Cloudinsight sprites exampleskinparam monochrome truerectangle &quot;&lt;$tomcat&gt;\\nwebapp&quot; as webappqueue &quot;&lt;$kafka&gt;&quot; as kafkarectangle &quot;&lt;$java&gt;\\ndaemon&quot; as daemondatabase &quot;&lt;$cassandra&gt;&quot; as cassandrawebapp -&gt; kafkakafka -&gt; daemondaemon --&gt; cassandra@enduml 1.2.6. 高效绘制 甘特图1234567891011121314gantt title 项目开发流程 section 项目确定 需求分析 :a1, 2016-06-22, 5d 可行性报告 :after a1, 5d 概念验证 : 5d section 项目实施 概要设计 :2016-07-05 , 5d 详细设计 :2016-07-08, 10d 编码 :2016-07-15, 10d 测试 :2016-07-22, 5d section 发布验收 发布: 2d 验收: 3d 1.2.7. 绘制表格 项目 价格 数量 计算机 $1600 5 手机 $12 12 管线 $1 234 1.2.8. 数字时序图1234567&#123;signal: [ &#123;name: &#39;clk&#39;, wave: &#39;p.....|...&#39;&#125;, &#123;name: &#39;dat&#39;, wave: &#39;x.345x|&#x3D;.x&#39;, data: [&#39;head&#39;, &#39;body&#39;, &#39;tail&#39;, &#39;data&#39;]&#125;, &#123;name: &#39;req&#39;, wave: &#39;0.1..0|1.0&#39;&#125;, &#123;&#125;, &#123;name: &#39;ack&#39;, wave: &#39;1.....|01.&#39;&#125;]&#125; 1.2.9. 饼图123456pie title 动物数 &quot;Dogs&quot; : 356 &quot;Cats&quot; : 85 &quot;Rats&quot; : 150 &quot;Cows&quot; : 150 1.2.10. 更详细语法说明想要查看更详细的语法说明，可以参考我们准备的 Cmd Markdown 简明语法手册，进阶用户可以参考 Cmd Markdown 高阶语法手册 了解更多高级功能。 总而言之，不同于其它 所见即所得 的编辑器：你只需使用键盘专注于书写文本内容，就可以生成印刷级的排版格式，省却在键盘和工具栏之间来回切换，调整内容和格式的麻烦。Markdown 在流畅的书写和印刷级的阅读体验之间找到了平衡。 目前它已经成为世界上最大的技术分享网站 GitHub 和 技术问答网站 StackOverFlow 的御用书写格式。 1.3. 什么是 Cmd Markdown您可以使用很多工具书写 Markdown，但是 Cmd Markdown 是这个星球上我们已知的、最好的 Markdown 工具——没有之一 ：）因为深信文字的力量，所以我们和你一样，对流畅书写，分享思想和知识，以及阅读体验有极致的追求，我们把对于这些诉求的回应整合在 Cmd Markdown，并且一次，两次，三次，乃至无数次地提升这个工具的体验，最终将它演化成一个 编辑/发布/阅读 Markdown 的在线平台——您可以在任何地方，任何系统/设备上管理这里的文字。 1.3.1. 实时同步预览我们将 Cmd Markdown 的主界面一分为二，左边为编辑区，右边为预览区，在编辑区的操作会实时地渲染到预览区方便查看最终的版面效果，并且如果你在其中一个区拖动滚动条，我们有一个巧妙的算法把另一个区的滚动条同步到等价的位置，超酷！ 1.3.2. 编辑工具栏也许您还是一个 Markdown 语法的新手，在您完全熟悉它之前，我们在 编辑区 的顶部放置了一个如下图所示的工具栏，您可以使用鼠标在工具栏上调整格式，不过我们仍旧鼓励你使用键盘标记格式，提高书写的流畅度。 1.3.3. 编辑模式完全心无旁骛的方式编辑文字：点击 编辑工具栏 最右侧的拉伸按钮或者按下 Ctrl + M，将 Cmd Markdown 切换到独立的编辑模式，这是一个极度简洁的写作环境，所有可能会引起分心的元素都已经被挪除，超清爽！ 1.3.4. 实时的云端文稿为了保障数据安全，Cmd Markdown 会将您每一次击键的内容保存至云端，同时在 编辑工具栏 的最右侧提示 已保存 的字样。无需担心浏览器崩溃，机器掉电或者地震，海啸——在编辑的过程中随时关闭浏览器或者机器，下一次回到 Cmd Markdown 的时候继续写作。 1.3.5. 离线模式在网络环境不稳定的情况下记录文字一样很安全！在您写作的时候，如果电脑突然失去网络连接，Cmd Markdown 会智能切换至离线模式，将您后续键入的文字保存在本地，直到网络恢复再将他们传送至云端，即使在网络恢复前关闭浏览器或者电脑，一样没有问题，等到下次开启 Cmd Markdown 的时候，她会提醒您将离线保存的文字传送至云端。简而言之，我们尽最大的努力保障您文字的安全。 1.3.6. 管理工具栏为了便于管理您的文稿，在 预览区 的顶部放置了如下所示的 管理工具栏： 通过管理工具栏可以： 发布：将当前的文稿生成固定链接，在网络上发布，分享 新建：开始撰写一篇新的文稿 删除：删除当前的文稿 导出：将当前的文稿转化为 Markdown 文本或者 Html 格式，并导出到本地 列表：所有新增和过往的文稿都可以在这里查看、操作 模式：切换 普通/Vim/Emacs 编辑模式 1.3.7. 阅读工具栏 通过 预览区 右上角的 阅读工具栏，可以查看当前文稿的目录并增强阅读体验。 工具栏上的五个图标依次为： 目录：快速导航当前文稿的目录结构以跳转到感兴趣的段落 视图：互换左边编辑区和右边预览区的位置 主题：内置了黑白两种模式的主题，试试 黑色主题，超炫！ 阅读：心无旁骛的阅读模式提供超一流的阅读体验 全屏：简洁，简洁，再简洁，一个完全沉浸式的写作和阅读环境 1.3.8. 阅读模式在 阅读工具栏 点击 或者按下 Ctrl+Alt+M 随即进入独立的阅读模式界面，我们在版面渲染上的每一个细节：字体，字号，行间距，前背景色都倾注了大量的时间，努力提升阅读的体验和品质。 1.3.9. 标签、分类和搜索在编辑区任意行首位置输入以下格式的文字可以标签当前文档： 标签： 未分类 标签以后的文稿在【文件列表】（Ctrl+Alt+F）里会按照标签分类，用户可以同时使用键盘或者鼠标浏览查看，或者在【文件列表】的搜索文本框内搜索标题关键字过滤文稿，如下图所示： 1.3.10. 文稿发布和分享在您使用 Cmd Markdown 记录，创作，整理，阅读文稿的同时，我们不仅希望它是一个有力的工具，更希望您的思想和知识通过这个平台，连同优质的阅读体验，将他们分享给有相同志趣的人，进而鼓励更多的人来到这里记录分享他们的思想和知识，尝试点击 (Ctrl+Alt+P) 发布这份文档给好友吧！ 再一次感谢您花费时间阅读这份欢迎稿，点击 (Ctrl+Alt+N) 开始撰写新的文稿吧！祝您在这里记录、阅读、分享愉快！ 作者 @ghosert2016 年 07月 07日 [^LaTeX]: 支持 LaTeX 编辑显示支持，例如：$\\sum_{i=1}^n a_i=0$， 访问 MathJax 参考更多使用方法。[^code]: 代码高亮功能支持包括 Java, Python, JavaScript 在内的，四十一种主流编程语言。 【微信】 💥 【变化】 🐞 【错误】 📖 【文档】 🌀 【改变】 🌍 【本地化】 http://emojihomepage.com/ 📘 ℹ","categories":[],"tags":[{"name":"markdown","slug":"markdown","permalink":"https://qikaile.us/volantis/tags/markdown/"}]},{"title":"Pandas中文手册","slug":"pandas-manual","date":"2020-11-17T10:47:46.000Z","updated":"2020-11-17T12:07:16.076Z","comments":true,"path":"posts/43716.html","link":"","permalink":"https://qikaile.us/volantis/posts/43716.html","excerpt":"","text":"如果你想学习Pandas，建议先看两个网站。 （1）官网：Python Data Analysis Library （2）十分钟入门Pandas：10 Minutes to pandas 关键缩写和包导入在这个速查手册中，我们使用如下缩写： df：任意的Pandas DataFrame对象 s：任意的Pandas Series对象 同时我们需要做如下的引入： import pandas as pd import numpy as np 导入数据 pd.read_csv(filename)：从CSV文件导入数据 pd.read_table(filename)：从限定分隔符的文本文件导入数据 pd.read_excel(filename)：从Excel文件导入数据 pd.read_sql(query, connection_object)：从SQL表/库导入数据 pd.read_json(json_string)：从JSON格式的字符串导入数据 pd.read_html(url)：解析URL、字符串或者HTML文件，抽取其中的tables表格 pd.read_clipboard()：从你的粘贴板获取内容，并传给read_table() pd.DataFrame(dict)：从字典对象导入数据，Key是列名，Value是数据 导出数据 df.to_csv(filename)：导出数据到CSV文件 df.to_excel(filename)：导出数据到Excel文件 df.to_sql(table_name, connection_object)：导出数据到SQL表 df.to_json(filename)：以Json格式导出数据到文本文件 创建测试对象 pd.DataFrame(np.random.rand(20,5))：创建20行5列的随机数组成的DataFrame对象 pd.Series(my_list)：从可迭代对象my_list创建一个Series对象 df.index = pd.date_range(‘1900/1/30’, periods=df.shape[0])：增加一个日期索引 查看、检查数据 df.head(n)：查看DataFrame对象的前n行 df.tail(n)：查看DataFrame对象的最后n行 df.shape()：查看行数和列数 http://df.info()：查看索引、数据类型和内存信息 df.describe()：查看数值型列的汇总统计 s.value_counts(dropna=False)：查看Series对象的唯一值和计数 df.apply(pd.Series.value_counts)：查看DataFrame对象中每一列的唯一值和计数 数据选取 df[col]：根据列名，并以Series的形式返回列 df[[col1, col2]]：以DataFrame形式返回多列 s.iloc[0]：按位置选取数据 s.loc[‘index_one’]：按索引选取数据 df.iloc[0,:]：返回第一行 df.iloc[0,0]：返回第一列的第一个元素 数据清理 df.columns = [‘a’,’b’,’c’]：重命名列名 pd.isnull()：检查DataFrame对象中的空值，并返回一个Boolean数组 pd.notnull()：检查DataFrame对象中的非空值，并返回一个Boolean数组 df.dropna()：删除所有包含空值的行 df.dropna(axis=1)：删除所有包含空值的列 df.dropna(axis=1,thresh=n)：删除所有小于n个非空值的行 df.fillna(x)：用x替换DataFrame对象中所有的空值 s.astype(float)：将Series中的数据类型更改为float类型 s.replace(1,’one’)：用‘one’代替所有等于1的值 s.replace([1,3],[‘one’,’three’])：用’one’代替1，用’three’代替3 df.rename(columns=lambda x: x + 1)：批量更改列名 df.rename(columns={‘old_name’: ‘new_ name’})：选择性更改列名 df.set_index(‘column_one’)：更改索引列 df.rename(index=lambda x: x + 1)：批量重命名索引 数据处理：Filter 、Sort 和 GroupBy df[df[col] &gt; 0.5]：选择col列的值大于0.5的行 df.sort_values(col1)：按照列col1排序数据，默认升序排列 df.sort_values(col2, ascending=False)：按照列col1降序排列数据 df.sort_values([col1,col2], ascending=[True,False])：先按列col1升序排列，后按col2降序排列数据 df.groupby(col)：返回一个按列col进行分组的Groupby对象 df.groupby([col1,col2])：返回一个按多列进行分组的Groupby对象 df.groupby(col1)[col2]：返回按列col1进行分组后，列col2的均值 df.pivot_table(index=col1, values=[col2,col3], aggfunc=max)：创建一个按列col1进行分组，并计算col2和col3的最大值的数据透视表 df.groupby(col1).agg(np.mean)：返回按列col1分组的所有列的均值 data.apply(np.mean)：对DataFrame中的每一列应用函数np.mean data.apply(np.max,axis=1)：对DataFrame中的每一行应用函数np.max 数据合并 df1.append(df2)：将df2中的行添加到df1的尾部 df.concat([df1, df2],axis=1)：将df2中的列添加到df1的尾部 df1.join(df2,on=col1,how=’inner’)：对df1的列和df2的列执行SQL形式的join 数据统计 df.describe()：查看数据值列的汇总统计 df.mean()：返回所有列的均值 df.corr()：返回列与列之间的相关系数 df.count()：返回每一列中的非空值的个数 df.max()：返回每一列的最大值 df.min()：返回每一列的最小值 df.median()：返回每一列的中位数 df.std()：返回每一列的标准差","categories":[{"name":"Pandas","slug":"Pandas","permalink":"https://qikaile.us/volantis/categories/Pandas/"}],"tags":[{"name":"pandas","slug":"pandas","permalink":"https://qikaile.us/volantis/tags/pandas/"}]},{"title":"十分钟搞定 pandas","slug":"10-Minutes-to-pandas","date":"2020-11-17T10:28:58.000Z","updated":"2020-11-17T13:30:09.257Z","comments":true,"path":"posts/6936.html","link":"","permalink":"https://qikaile.us/volantis/posts/6936.html","excerpt":"","text":"官方网站上《10 Minutes to pandas》的一个简单的翻译，原文在这里。这篇文章是对 pandas 的一个简单的介绍，详细的介绍请参考：秘籍 。习惯上，我们会按下面格式引入所需要的包： 12345In [1]: import pandas as pdIn [2]: import numpy as npIn [3]: import matplotlib.pyplot as plt 一、 创建对象可以通过 数据结构入门 来查看有关该节内容的详细信息。 1、可以通过传递一个list对象来创建一个Series，pandas 会默认创建整型索引： 1234567891011In [4]: s = pd.Series([1,3,5,np.nan,6,8])In [5]: sOut[5]: 0 1.01 3.02 5.03 NaN4 6.05 8.0dtype: float64 2、通过传递一个 numpyarray，时间索引以及列标签来创建一个DataFrame： 12345678910111213141516171819In [6]: dates = pd.date_range(&#x27;20130101&#x27;, periods=6)In [7]: datesOut[7]: DatetimeIndex([&#x27;2013-01-01&#x27;, &#x27;2013-01-02&#x27;, &#x27;2013-01-03&#x27;, &#x27;2013-01-04&#x27;, &#x27;2013-01-05&#x27;, &#x27;2013-01-06&#x27;], dtype=&#x27;datetime64[ns]&#x27;, freq=&#x27;D&#x27;)In [8]: df = pd.DataFrame(np.random.randn(6,4), index=dates, columns=list(&#x27;ABCD&#x27;))In [9]: dfOut[9]: A B C D2013-01-01 0.469112 -0.282863 -1.509059 -1.1356322013-01-02 1.212112 -0.173215 0.119209 -1.0442362013-01-03 -0.861849 -2.104569 -0.494929 1.0718042013-01-04 0.721555 -0.706771 -1.039575 0.2718602013-01-05 -0.424972 0.567020 0.276232 -1.0874012013-01-06 -0.673690 0.113648 -1.478427 0.524988 3、通过传递一个能够被转换成类似序列结构的字典对象来创建一个DataFrame： 123456789101112131415In [10]: df2 = pd.DataFrame(&#123; &#x27;A&#x27; : 1., ....: &#x27;B&#x27; : pd.Timestamp(&#x27;20130102&#x27;), ....: &#x27;C&#x27; : pd.Series(1,index=list(range(4)),dtype=&#x27;float32&#x27;), ....: &#x27;D&#x27; : np.array([3] * 4,dtype=&#x27;int32&#x27;), ....: &#x27;E&#x27; : pd.Categorical([&quot;test&quot;,&quot;train&quot;,&quot;test&quot;,&quot;train&quot;]), ....: &#x27;F&#x27; : &#x27;foo&#x27; &#125;) ....: In [11]: df2Out[11]: A B C D E F0 1.0 2013-01-02 1.0 3 test foo1 1.0 2013-01-02 1.0 3 train foo2 1.0 2013-01-02 1.0 3 test foo3 1.0 2013-01-02 1.0 3 train foo 4、查看不同列的数据类型： 123456789In [12]: df2.dtypesOut[12]: A float64B datetime64[ns]C float32D int32E categoryF objectdtype: object 5、如果你使用的是 IPython，使用 Tab 自动补全功能会自动识别所有的属性以及自定义的列，下图中是所有能够被自动识别的属性的一个子集： 123456789101112131415161718192021222324In [13]: df2.&lt;TAB&gt;df2.A df2.boxplotdf2.abs df2.Cdf2.add df2.clipdf2.add_prefix df2.clip_lowerdf2.add_suffix df2.clip_upperdf2.align df2.columnsdf2.all df2.combinedf2.any df2.combineAdddf2.append df2.combine_firstdf2.apply df2.combineMultdf2.applymap df2.compounddf2.as_blocks df2.consolidatedf2.asfreq df2.convert_objectsdf2.as_matrix df2.copydf2.astype df2.corrdf2.at df2.corrwithdf2.at_time df2.countdf2.axes df2.covdf2.B df2.cummaxdf2.between_time df2.cummindf2.bfill df2.cumproddf2.blocks df2.cumsumdf2.bool df2.D 二、 查看数据详情请参阅：基础。 1、 查看DataFrame中头部和尾部的行： 123456789101112131415In [14]: df.head()Out[14]: A B C D2013-01-01 0.469112 -0.282863 -1.509059 -1.1356322013-01-02 1.212112 -0.173215 0.119209 -1.0442362013-01-03 -0.861849 -2.104569 -0.494929 1.0718042013-01-04 0.721555 -0.706771 -1.039575 0.2718602013-01-05 -0.424972 0.567020 0.276232 -1.087401In [15]: df.tail(3)Out[15]: A B C D2013-01-04 0.721555 -0.706771 -1.039575 0.2718602013-01-05 -0.424972 0.567020 0.276232 -1.0874012013-01-06 -0.673690 0.113648 -1.478427 0.524988 2、 显示索引、列和底层的 numpy 数据： 1234567891011121314151617In [16]: df.indexOut[16]: DatetimeIndex([&#x27;2013-01-01&#x27;, &#x27;2013-01-02&#x27;, &#x27;2013-01-03&#x27;, &#x27;2013-01-04&#x27;, &#x27;2013-01-05&#x27;, &#x27;2013-01-06&#x27;], dtype=&#x27;datetime64[ns]&#x27;, freq=&#x27;D&#x27;)In [17]: df.columnsOut[17]: Index([u&#x27;A&#x27;, u&#x27;B&#x27;, u&#x27;C&#x27;, u&#x27;D&#x27;], dtype=&#x27;object&#x27;)In [18]: df.valuesOut[18]: array([[ 0.4691, -0.2829, -1.5091, -1.1356], [ 1.2121, -0.1732, 0.1192, -1.0442], [-0.8618, -2.1046, -0.4949, 1.0718], [ 0.7216, -0.7068, -1.0396, 0.2719], [-0.425 , 0.567 , 0.2762, -1.0874], [-0.6737, 0.1136, -1.4784, 0.525 ]]) 3、 describe()函数对于数据的快速统计汇总： 1234567891011In [19]: df.describe()Out[19]: A B C Dcount 6.000000 6.000000 6.000000 6.000000mean 0.073711 -0.431125 -0.687758 -0.233103std 0.843157 0.922818 0.779887 0.973118min -0.861849 -2.104569 -1.509059 -1.13563225% -0.611510 -0.600794 -1.368714 -1.07661050% 0.022070 -0.228039 -0.767252 -0.38618875% 0.658444 0.041933 -0.034326 0.461706max 1.212112 0.567020 0.276232 1.071804 4、 对数据的转置： 1234567In [20]: df.TOut[20]: 2013-01-01 2013-01-02 2013-01-03 2013-01-04 2013-01-05 2013-01-06A 0.469112 1.212112 -0.861849 0.721555 -0.424972 -0.673690B -0.282863 -0.173215 -2.104569 -0.706771 0.567020 0.113648C -1.509059 0.119209 -0.494929 -1.039575 0.276232 -1.478427D -1.135632 -1.044236 1.071804 0.271860 -1.087401 0.524988 5、 按轴进行排序 123456789In [21]: df.sort_index(axis=1, ascending=False)Out[21]: D C B A2013-01-01 -1.135632 -1.509059 -0.282863 0.4691122013-01-02 -1.044236 0.119209 -0.173215 1.2121122013-01-03 1.071804 -0.494929 -2.104569 -0.8618492013-01-04 0.271860 -1.039575 -0.706771 0.7215552013-01-05 -1.087401 0.276232 0.567020 -0.4249722013-01-06 0.524988 -1.478427 0.113648 -0.673690 6、 按值进行排序 123456789In [22]: df.sort_values(by=&#x27;B&#x27;)Out[22]: A B C D2013-01-03 -0.861849 -2.104569 -0.494929 1.0718042013-01-04 0.721555 -0.706771 -1.039575 0.2718602013-01-01 0.469112 -0.282863 -1.509059 -1.1356322013-01-02 1.212112 -0.173215 0.119209 -1.0442362013-01-06 -0.673690 0.113648 -1.478427 0.5249882013-01-05 -0.424972 0.567020 0.276232 -1.087401 三、 选择虽然标准的 Python/Numpy 的选择和设置表达式都能够直接派上用场，但是作为工程使用的代码，我们推荐使用经过优化的 pandas 数据访问方式： .at, .iat, .loc, .iloc 和 .ix。详情请参阅索引和选取数据 和 多重索引/高级索引。 获取1、 选择一个单独的列，这将会返回一个Series，等同于df.A： 123456789In [23]: df[&#x27;A&#x27;]Out[23]: 2013-01-01 0.4691122013-01-02 1.2121122013-01-03 -0.8618492013-01-04 0.7215552013-01-05 -0.4249722013-01-06 -0.673690Freq: D, Name: A, dtype: float64 2、 通过[]进行选择，这将会对行进行切片 12345678910111213In [24]: df[0:3]Out[24]: A B C D2013-01-01 0.469112 -0.282863 -1.509059 -1.1356322013-01-02 1.212112 -0.173215 0.119209 -1.0442362013-01-03 -0.861849 -2.104569 -0.494929 1.071804In [25]: df[&#x27;20130102&#x27;:&#x27;20130104&#x27;]Out[25]: A B C D2013-01-02 1.212112 -0.173215 0.119209 -1.0442362013-01-03 -0.861849 -2.104569 -0.494929 1.0718042013-01-04 0.721555 -0.706771 -1.039575 0.271860 通过标签选择1、 使用标签来获取一个交叉的区域 1234567In [26]: df.loc[dates[0]]Out[26]: A 0.469112B -0.282863C -1.509059D -1.135632Name: 2013-01-01 00:00:00, dtype: float64 2、 通过标签来在多个轴上进行选择 123456789In [27]: df.loc[:,[&#x27;A&#x27;,&#x27;B&#x27;]]Out[27]: A B2013-01-01 0.469112 -0.2828632013-01-02 1.212112 -0.1732152013-01-03 -0.861849 -2.1045692013-01-04 0.721555 -0.7067712013-01-05 -0.424972 0.5670202013-01-06 -0.673690 0.113648 3、 标签切片 123456In [28]: df.loc[&#x27;20130102&#x27;:&#x27;20130104&#x27;,[&#x27;A&#x27;,&#x27;B&#x27;]]Out[28]: A B2013-01-02 1.212112 -0.1732152013-01-03 -0.861849 -2.1045692013-01-04 0.721555 -0.706771 4、 对于返回的对象进行维度缩减 12345In [29]: df.loc[&#x27;20130102&#x27;,[&#x27;A&#x27;,&#x27;B&#x27;]]Out[29]: A 1.212112B -0.173215Name: 2013-01-02 00:00:00, dtype: float64 5、 获取一个标量 12In [30]: df.loc[dates[0],&#x27;A&#x27;]Out[30]: 0.46911229990718628 6、 快速访问一个标量（与上一个方法等价） 12In [31]: df.at[dates[0],&#x27;A&#x27;]Out[31]: 0.46911229990718628 通过位置选择1、 通过传递数值进行位置选择（选择的是行） 1234567In [32]: df.iloc[3]Out[32]: A 0.721555B -0.706771C -1.039575D 0.271860Name: 2013-01-04 00:00:00, dtype: float64 2、 通过数值进行切片，与 numpy/python 中的情况类似 12345In [33]: df.iloc[3:5,0:2]Out[33]: A B2013-01-04 0.721555 -0.7067712013-01-05 -0.424972 0.567020 3、 通过指定一个位置的列表，与 numpy/python 中的情况类似 123456In [34]: df.iloc[[1,2,4],[0,2]]Out[34]: A C2013-01-02 1.212112 0.1192092013-01-03 -0.861849 -0.4949292013-01-05 -0.424972 0.276232 4、 对行进行切片 12345In [35]: df.iloc[1:3,:]Out[35]: A B C D2013-01-02 1.212112 -0.173215 0.119209 -1.0442362013-01-03 -0.861849 -2.104569 -0.494929 1.071804 5、 对列进行切片 123456789In [36]: df.iloc[:,1:3]Out[36]: B C2013-01-01 -0.282863 -1.5090592013-01-02 -0.173215 0.1192092013-01-03 -2.104569 -0.4949292013-01-04 -0.706771 -1.0395752013-01-05 0.567020 0.2762322013-01-06 0.113648 -1.478427 6、 获取特定的值 12In [37]: df.iloc[1,1]Out[37]: -0.17321464905330858 快速访问标量（等同于前一个方法）： 12In [38]: df.iat[1,1]Out[38]: -0.17321464905330858 布尔索引1、 使用一个单独列的值来选择数据： 123456In [39]: df[df.A &gt; 0]Out[39]: A B C D2013-01-01 0.469112 -0.282863 -1.509059 -1.1356322013-01-02 1.212112 -0.173215 0.119209 -1.0442362013-01-04 0.721555 -0.706771 -1.039575 0.271860 2、 使用where操作来选择数据： 123456789In [40]: df[df &gt; 0]Out[40]: A B C D2013-01-01 0.469112 NaN NaN NaN2013-01-02 1.212112 NaN 0.119209 NaN2013-01-03 NaN NaN NaN 1.0718042013-01-04 0.721555 NaN NaN 0.2718602013-01-05 NaN 0.567020 0.276232 NaN2013-01-06 NaN 0.113648 NaN 0.524988 3、 使用isin()方法来过滤： 12345678910111213141516171819In [41]: df2 = df.copy()In [42]: df2[&#x27;E&#x27;] = [&#x27;one&#x27;, &#x27;one&#x27;,&#x27;two&#x27;,&#x27;three&#x27;,&#x27;four&#x27;,&#x27;three&#x27;]In [43]: df2Out[43]: A B C D E2013-01-01 0.469112 -0.282863 -1.509059 -1.135632 one2013-01-02 1.212112 -0.173215 0.119209 -1.044236 one2013-01-03 -0.861849 -2.104569 -0.494929 1.071804 two2013-01-04 0.721555 -0.706771 -1.039575 0.271860 three2013-01-05 -0.424972 0.567020 0.276232 -1.087401 four2013-01-06 -0.673690 0.113648 -1.478427 0.524988 threeIn [44]: df2[df2[&#x27;E&#x27;].isin([&#x27;two&#x27;,&#x27;four&#x27;])]Out[44]: A B C D E2013-01-03 -0.861849 -2.104569 -0.494929 1.071804 two2013-01-05 -0.424972 0.567020 0.276232 -1.087401 four 设置1、 设置一个新的列： 12345678910111213In [45]: s1 = pd.Series([1,2,3,4,5,6], index=pd.date_range(&#x27;20130102&#x27;, periods=6))In [46]: s1Out[46]: 2013-01-02 12013-01-03 22013-01-04 32013-01-05 42013-01-06 52013-01-07 6Freq: D, dtype: int64In [47]: df[&#x27;F&#x27;] = s1 2、 通过标签设置新的值： 1In [48]: df.at[dates[0],&#x27;A&#x27;] = 0 3、 通过位置设置新的值： 1In [49]: df.iat[0,1] = 0 4、 通过一个numpy数组设置一组新值： 1In [50]: df.loc[:,&#x27;D&#x27;] = np.array([5] * len(df)) 上述操作结果如下： 123456789In [51]: dfOut[51]: A B C D F2013-01-01 0.000000 0.000000 -1.509059 5 NaN2013-01-02 1.212112 -0.173215 0.119209 5 1.02013-01-03 -0.861849 -2.104569 -0.494929 5 2.02013-01-04 0.721555 -0.706771 -1.039575 5 3.02013-01-05 -0.424972 0.567020 0.276232 5 4.02013-01-06 -0.673690 0.113648 -1.478427 5 5.0 5、 通过where操作来设置新的值： 12345678910111213In [52]: df2 = df.copy()In [53]: df2[df2 &gt; 0] = -df2In [54]: df2Out[54]: A B C D F2013-01-01 0.000000 0.000000 -1.509059 -5 NaN2013-01-02 -1.212112 -0.173215 -0.119209 -5 -1.02013-01-03 -0.861849 -2.104569 -0.494929 -5 -2.02013-01-04 -0.721555 -0.706771 -1.039575 -5 -3.02013-01-05 -0.424972 -0.567020 -0.276232 -5 -4.02013-01-06 -0.673690 -0.113648 -1.478427 -5 -5.0 四、 缺失值处理在 pandas 中，使用np.nan来代替缺失值，这些值将默认不会包含在计算中，详情请参阅：缺失的数据。 1、 reindex()方法可以对指定轴上的索引进行改变/增加/删除操作，这将返回原始数据的一个拷贝： 1234567891011In [55]: df1 = df.reindex(index=dates[0:4], columns=list(df.columns) + [&#x27;E&#x27;])In [56]: df1.loc[dates[0]:dates[1],&#x27;E&#x27;] = 1In [57]: df1Out[57]: A B C D F E2013-01-01 0.000000 0.000000 -1.509059 5 NaN 1.02013-01-02 1.212112 -0.173215 0.119209 5 1.0 1.02013-01-03 -0.861849 -2.104569 -0.494929 5 2.0 NaN2013-01-04 0.721555 -0.706771 -1.039575 5 3.0 NaN 2、 去掉包含缺失值的行： 1234In [58]: df1.dropna(how=&#x27;any&#x27;)Out[58]: A B C D F E2013-01-02 1.212112 -0.173215 0.119209 5 1.0 1.0 3、 对缺失值进行填充： 1234567In [59]: df1.fillna(value=5)Out[59]: A B C D F E2013-01-01 0.000000 0.000000 -1.509059 5 5.0 1.02013-01-02 1.212112 -0.173215 0.119209 5 1.0 1.02013-01-03 -0.861849 -2.104569 -0.494929 5 2.0 5.02013-01-04 0.721555 -0.706771 -1.039575 5 3.0 5.0 4、 对数据进行布尔填充： 1234567n [60]: pd.isnull(df1)Out[60]: A B C D F E2013-01-01 False False False False True False2013-01-02 False False False False False False2013-01-03 False False False False False True2013-01-04 False False False False False True 五、 相关操作详情请参与 基本的二进制操作 统计（相关操作通常情况下不包括缺失值）1、 执行描述性统计： 12345678In [61]: df.mean()Out[61]: A -0.004474B -0.383981C -0.687758D 5.000000F 3.000000dtype: float64 2、 在其他轴上进行相同的操作： 123456789In [62]: df.mean(1)Out[62]: 2013-01-01 0.8727352013-01-02 1.4316212013-01-03 0.7077312013-01-04 1.3950422013-01-05 1.8836562013-01-06 1.592306Freq: D, dtype: float64 3、 对于拥有不同维度，需要对齐的对象进行操作。Pandas 会自动的沿着指定的维度进行广播： 123456789101112131415161718192021In [63]: s = pd.Series([1,3,5,np.nan,6,8], index=dates).shift(2)In [64]: sOut[64]: 2013-01-01 NaN2013-01-02 NaN2013-01-03 1.02013-01-04 3.02013-01-05 5.02013-01-06 NaNFreq: D, dtype: float64In [65]: df.sub(s, axis=&#x27;index&#x27;)Out[65]: A B C D F2013-01-01 NaN NaN NaN NaN NaN2013-01-02 NaN NaN NaN NaN NaN2013-01-03 -1.861849 -3.104569 -1.494929 4.0 1.02013-01-04 -2.278445 -3.706771 -4.039575 2.0 0.02013-01-05 -5.424972 -4.432980 -4.723768 0.0 -1.02013-01-06 NaN NaN NaN NaN NaN Apply1、 对数据应用函数： 123456789101112131415161718In [66]: df.apply(np.cumsum)Out[66]: A B C D F2013-01-01 0.000000 0.000000 -1.509059 5 NaN2013-01-02 1.212112 -0.173215 -1.389850 10 1.02013-01-03 0.350263 -2.277784 -1.884779 15 3.02013-01-04 1.071818 -2.984555 -2.924354 20 6.02013-01-05 0.646846 -2.417535 -2.648122 25 10.02013-01-06 -0.026844 -2.303886 -4.126549 30 15.0In [67]: df.apply(lambda x: x.max() - x.min())Out[67]: A 2.073961B 2.671590C 1.785291D 0.000000F 4.000000dtype: float64 直方图具体请参照：直方图和离散化。 1234567891011121314151617181920212223In [68]: s = pd.Series(np.random.randint(0, 7, size=10))In [69]: sOut[69]: 0 41 22 13 24 65 46 47 68 49 4dtype: int64In [70]: s.value_counts()Out[70]: 4 56 22 21 1dtype: int64 字符串方法Series对象在其str属性中配备了一组字符串处理方法，可以很容易的应用到数组中的每个元素，如下段代码所示。更多详情请参考：字符串向量化方法。 1234567891011121314In [71]: s = pd.Series([&#x27;A&#x27;, &#x27;B&#x27;, &#x27;C&#x27;, &#x27;Aaba&#x27;, &#x27;Baca&#x27;, np.nan, &#x27;CABA&#x27;, &#x27;dog&#x27;, &#x27;cat&#x27;])In [72]: s.str.lower()Out[72]: 0 a1 b2 c3 aaba4 baca5 NaN6 caba7 dog8 catdtype: object 六、 合并Pandas 提供了大量的方法能够轻松的对Series，DataFrame和Panel对象进行各种符合各种逻辑关系的合并操作。具体请参阅：合并。 Concat1234567891011121314151617181920212223242526272829303132In [73]: df = pd.DataFrame(np.random.randn(10, 4))In [74]: dfOut[74]: 0 1 2 30 -0.548702 1.467327 -1.015962 -0.4830751 1.637550 -1.217659 -0.291519 -1.7455052 -0.263952 0.991460 -0.919069 0.2660463 -0.709661 1.669052 1.037882 -1.7057754 -0.919854 -0.042379 1.247642 -0.0099205 0.290213 0.495767 0.362949 1.5481066 -1.131345 -0.089329 0.337863 -0.9458677 -0.932132 1.956030 0.017587 -0.0166928 -0.575247 0.254161 -1.143704 0.2158979 1.193555 -0.077118 -0.408530 -0.862495# break it into piecesIn [75]: pieces = [df[:3], df[3:7], df[7:]]In [76]: pd.concat(pieces)Out[76]: 0 1 2 30 -0.548702 1.467327 -1.015962 -0.4830751 1.637550 -1.217659 -0.291519 -1.7455052 -0.263952 0.991460 -0.919069 0.2660463 -0.709661 1.669052 1.037882 -1.7057754 -0.919854 -0.042379 1.247642 -0.0099205 0.290213 0.495767 0.362949 1.5481066 -1.131345 -0.089329 0.337863 -0.9458677 -0.932132 1.956030 0.017587 -0.0166928 -0.575247 0.254161 -1.143704 0.2158979 1.193555 -0.077118 -0.408530 -0.862495 Join类似于 SQL 类型的合并，具体请参阅：数据库风格的连接 1234567891011121314151617181920212223In [77]: left = pd.DataFrame(&#123;&#x27;key&#x27;: [&#x27;foo&#x27;, &#x27;foo&#x27;], &#x27;lval&#x27;: [1, 2]&#125;)In [78]: right = pd.DataFrame(&#123;&#x27;key&#x27;: [&#x27;foo&#x27;, &#x27;foo&#x27;], &#x27;rval&#x27;: [4, 5]&#125;)In [79]: leftOut[79]: key lval0 foo 11 foo 2In [80]: rightOut[80]: key rval0 foo 41 foo 5In [81]: pd.merge(left, right, on=&#x27;key&#x27;)Out[81]: key lval rval0 foo 1 41 foo 1 52 foo 2 43 foo 2 5 另一个例子： 123456789101112131415161718192021In [82]: left = pd.DataFrame(&#123;&#x27;key&#x27;: [&#x27;foo&#x27;, &#x27;bar&#x27;], &#x27;lval&#x27;: [1, 2]&#125;)In [83]: right = pd.DataFrame(&#123;&#x27;key&#x27;: [&#x27;foo&#x27;, &#x27;bar&#x27;], &#x27;rval&#x27;: [4, 5]&#125;)In [84]: leftOut[84]: key lval0 foo 11 bar 2In [85]: rightOut[85]: key rval0 foo 41 bar 5In [86]: pd.merge(left, right, on=&#x27;key&#x27;)Out[86]: key lval rval0 foo 1 41 bar 2 5 Append将一行连接到一个DataFrame上，具体请参阅附加： 12345678910111213141516171819202122232425262728In [87]: df = pd.DataFrame(np.random.randn(8, 4), columns=[&#x27;A&#x27;,&#x27;B&#x27;,&#x27;C&#x27;,&#x27;D&#x27;])In [88]: dfOut[88]: A B C D0 1.346061 1.511763 1.627081 -0.9905821 -0.441652 1.211526 0.268520 0.0245802 -1.577585 0.396823 -0.105381 -0.5325323 1.453749 1.208843 -0.080952 -0.2646104 -0.727965 -0.589346 0.339969 -0.6932055 -0.339355 0.593616 0.884345 1.5914316 0.141809 0.220390 0.435589 0.1924517 -0.096701 0.803351 1.715071 -0.708758In [89]: s = df.iloc[3]In [90]: df.append(s, ignore_index=True)Out[90]: A B C D0 1.346061 1.511763 1.627081 -0.9905821 -0.441652 1.211526 0.268520 0.0245802 -1.577585 0.396823 -0.105381 -0.5325323 1.453749 1.208843 -0.080952 -0.2646104 -0.727965 -0.589346 0.339969 -0.6932055 -0.339355 0.593616 0.884345 1.5914316 0.141809 0.220390 0.435589 0.1924517 -0.096701 0.803351 1.715071 -0.7087588 1.453749 1.208843 -0.080952 -0.264610 七、 分组对于”group by”操作，我们通常是指以下一个或多个操作步骤： （Splitting）按照一些规则将数据分为不同的组； （Applying）对于每组数据分别执行一个函数； （Combining）将结果组合到一个数据结构中； 详情请参阅：Grouping section 12345678910111213141516171819In [91]: df = pd.DataFrame(&#123;&#x27;A&#x27; : [&#x27;foo&#x27;, &#x27;bar&#x27;, &#x27;foo&#x27;, &#x27;bar&#x27;, ....: &#x27;foo&#x27;, &#x27;bar&#x27;, &#x27;foo&#x27;, &#x27;foo&#x27;], ....: &#x27;B&#x27; : [&#x27;one&#x27;, &#x27;one&#x27;, &#x27;two&#x27;, &#x27;three&#x27;, ....: &#x27;two&#x27;, &#x27;two&#x27;, &#x27;one&#x27;, &#x27;three&#x27;], ....: &#x27;C&#x27; : np.random.randn(8), ....: &#x27;D&#x27; : np.random.randn(8)&#125;) ....: In [92]: dfOut[92]: A B C D0 foo one -1.202872 -0.0552241 bar one -1.814470 2.3959852 foo two 1.018601 1.5528253 bar three -0.595447 0.1665994 foo two 1.395433 0.0476095 bar two -0.392670 -0.1364736 foo one 0.007207 -0.5617577 foo three 1.928123 -1.623033 1、 分组并对每个分组执行sum函数： 123456In [93]: df.groupby(&#x27;A&#x27;).sum()Out[93]: C DA bar -2.802588 2.42611foo 3.146492 -0.63958 2、 通过多个列进行分组形成一个层次索引，然后执行函数： 12345678910In [94]: df.groupby([&#x27;A&#x27;,&#x27;B&#x27;]).sum()Out[94]: C DA B bar one -1.814470 2.395985 three -0.595447 0.166599 two -0.392670 -0.136473foo one -1.195665 -0.616981 three 1.928123 -1.623033 two 2.414034 1.600434 八、 改变形状详情请参阅 层次索引 和 改变形状。 Stack1234567891011121314151617181920In [95]: tuples = list(zip(*[[&#x27;bar&#x27;, &#x27;bar&#x27;, &#x27;baz&#x27;, &#x27;baz&#x27;, ....: &#x27;foo&#x27;, &#x27;foo&#x27;, &#x27;qux&#x27;, &#x27;qux&#x27;], ....: [&#x27;one&#x27;, &#x27;two&#x27;, &#x27;one&#x27;, &#x27;two&#x27;, ....: &#x27;one&#x27;, &#x27;two&#x27;, &#x27;one&#x27;, &#x27;two&#x27;]])) ....: In [96]: index = pd.MultiIndex.from_tuples(tuples, names=[&#x27;first&#x27;, &#x27;second&#x27;])In [97]: df = pd.DataFrame(np.random.randn(8, 2), index=index, columns=[&#x27;A&#x27;, &#x27;B&#x27;])In [98]: df2 = df[:4]In [99]: df2Out[99]: A Bfirst second bar one 0.029399 -0.542108 two 0.282696 -0.087302baz one -1.575170 1.771208 two 0.816482 1.100230 1234567891011121314In [100]: stacked = df2.stack()In [101]: stackedOut[101]: first second bar one A 0.029399 B -0.542108 two A 0.282696 B -0.087302baz one A -1.575170 B 1.771208 two A 0.816482 B 1.100230dtype: float64 1234567891011121314151617181920212223242526In [102]: stacked.unstack()Out[102]: A Bfirst second bar one 0.029399 -0.542108 two 0.282696 -0.087302baz one -1.575170 1.771208 two 0.816482 1.100230In [103]: stacked.unstack(1)Out[103]: second one twofirst bar A 0.029399 0.282696 B -0.542108 -0.087302baz A -1.575170 0.816482 B 1.771208 1.100230In [104]: stacked.unstack(0)Out[104]: first bar bazsecond one A 0.029399 -1.575170 B -0.542108 1.771208two A 0.282696 0.816482 B -0.087302 1.100230 数据透视表详情请参阅：数据透视表. 12345678910111213141516171819202122In [105]: df = pd.DataFrame(&#123;&#x27;A&#x27; : [&#x27;one&#x27;, &#x27;one&#x27;, &#x27;two&#x27;, &#x27;three&#x27;] * 3, .....: &#x27;B&#x27; : [&#x27;A&#x27;, &#x27;B&#x27;, &#x27;C&#x27;] * 4, .....: &#x27;C&#x27; : [&#x27;foo&#x27;, &#x27;foo&#x27;, &#x27;foo&#x27;, &#x27;bar&#x27;, &#x27;bar&#x27;, &#x27;bar&#x27;] * 2, .....: &#x27;D&#x27; : np.random.randn(12), .....: &#x27;E&#x27; : np.random.randn(12)&#125;) .....: In [106]: dfOut[106]: A B C D E0 one A foo 1.418757 -0.1796661 one B foo -1.879024 1.2918362 two C foo 0.536826 -0.0096143 three A bar 1.006160 0.3921494 one B bar -0.029716 0.2645995 one C bar -1.146178 -0.0574096 two A foo 0.100900 -1.4256387 three B foo -1.035018 1.0240988 one C foo 0.314665 -0.1060629 one A bar -0.773723 1.82437510 two B bar -1.170653 0.59597411 three C bar 0.648740 1.167115 可以从这个数据中轻松的生成数据透视表： 12345678910111213In [107]: pd.pivot_table(df, values=&#x27;D&#x27;, index=[&#x27;A&#x27;, &#x27;B&#x27;], columns=[&#x27;C&#x27;])Out[107]: C bar fooA B one A -0.773723 1.418757 B -0.029716 -1.879024 C -1.146178 0.314665three A 1.006160 NaN B NaN -1.035018 C 0.648740 NaNtwo A NaN 0.100900 B -1.170653 NaN C NaN 0.536826 九、 时间序列Pandas 在对频率转换进行重新采样时拥有简单、强大且高效的功能（如将按秒采样的数据转换为按5分钟为单位进行采样的数据）。这种操作在金融领域非常常见。具体参考：时间序列。 12345678In [108]: rng = pd.date_range(&#x27;1/1/2012&#x27;, periods=100, freq=&#x27;S&#x27;)In [109]: ts = pd.Series(np.random.randint(0, 500, len(rng)), index=rng)In [110]: ts.resample(&#x27;5Min&#x27;).sum()Out[110]: 2012-01-01 25083Freq: 5T, dtype: int64 1、 时区表示： 1234567891011121314151617181920212223In [111]: rng = pd.date_range(&#x27;3/6/2012 00:00&#x27;, periods=5, freq=&#x27;D&#x27;)In [112]: ts = pd.Series(np.random.randn(len(rng)), rng)In [113]: tsOut[113]: 2012-03-06 0.4640002012-03-07 0.2273712012-03-08 -0.4969222012-03-09 0.3063892012-03-10 -2.290613Freq: D, dtype: float64In [114]: ts_utc = ts.tz_localize(&#x27;UTC&#x27;)In [115]: ts_utcOut[115]: 2012-03-06 00:00:00+00:00 0.4640002012-03-07 00:00:00+00:00 0.2273712012-03-08 00:00:00+00:00 -0.4969222012-03-09 00:00:00+00:00 0.3063892012-03-10 00:00:00+00:00 -2.290613Freq: D, dtype: float64 2、 时区转换： 12345678In [116]: ts_utc.tz_convert(&#x27;US/Eastern&#x27;)Out[116]: 2012-03-05 19:00:00-05:00 0.4640002012-03-06 19:00:00-05:00 0.2273712012-03-07 19:00:00-05:00 -0.4969222012-03-08 19:00:00-05:00 0.3063892012-03-09 19:00:00-05:00 -2.290613Freq: D, dtype: float64 3、 时间跨度转换： 1234567891011121314151617181920212223242526272829303132In [117]: rng = pd.date_range(&#x27;1/1/2012&#x27;, periods=5, freq=&#x27;M&#x27;)In [118]: ts = pd.Series(np.random.randn(len(rng)), index=rng)In [119]: tsOut[119]: 2012-01-31 -1.1346232012-02-29 -1.5618192012-03-31 -0.2608382012-04-30 0.2819572012-05-31 1.523962Freq: M, dtype: float64In [120]: ps = ts.to_period()In [121]: psOut[121]: 2012-01 -1.1346232012-02 -1.5618192012-03 -0.2608382012-04 0.2819572012-05 1.523962Freq: M, dtype: float64In [122]: ps.to_timestamp()Out[122]: 2012-01-01 -1.1346232012-02-01 -1.5618192012-03-01 -0.2608382012-04-01 0.2819572012-05-01 1.523962Freq: MS, dtype: float64 4、 时期和时间戳之间的转换使得可以使用一些方便的算术函数。 1234567891011121314In [123]: prng = pd.period_range(&#x27;1990Q1&#x27;, &#x27;2000Q4&#x27;, freq=&#x27;Q-NOV&#x27;)In [124]: ts = pd.Series(np.random.randn(len(prng)), prng)In [125]: ts.index = (prng.asfreq(&#x27;M&#x27;, &#x27;e&#x27;) + 1).asfreq(&#x27;H&#x27;, &#x27;s&#x27;) + 9In [126]: ts.head()Out[126]: 1990-03-01 09:00 -0.9029371990-06-01 09:00 0.0681591990-09-01 09:00 -0.0578731990-12-01 09:00 -0.3682041991-03-01 09:00 -1.144073Freq: H, dtype: float64 十、 Categorical从 0.15 版本开始，pandas 可以在DataFrame中支持 Categorical 类型的数据，详细 介绍参看：Categorical 简介和API documentation。 1In [127]: df = pd.DataFrame(&#123;&quot;id&quot;:[1,2,3,4,5,6], &quot;raw_grade&quot;:[&#x27;a&#x27;, &#x27;b&#x27;, &#x27;b&#x27;, &#x27;a&#x27;, &#x27;a&#x27;, &#x27;e&#x27;]&#125;) 1、 将原始的grade转换为 Categorical 数据类型： 123456789101112In [128]: df[&quot;grade&quot;] = df[&quot;raw_grade&quot;].astype(&quot;category&quot;)In [129]: df[&quot;grade&quot;]Out[129]: 0 a1 b2 b3 a4 a5 eName: grade, dtype: categoryCategories (3, object): [a, b, e] 2、 将 Categorical 类型数据重命名为更有意义的名称： 1In [130]: df[&quot;grade&quot;].cat.categories = [&quot;very good&quot;, &quot;good&quot;, &quot;very bad&quot;] 3、 对类别进行重新排序，增加缺失的类别： 123456789101112In [131]: df[&quot;grade&quot;] = df[&quot;grade&quot;].cat.set_categories([&quot;very bad&quot;, &quot;bad&quot;, &quot;medium&quot;, &quot;good&quot;, &quot;very good&quot;])In [132]: df[&quot;grade&quot;]Out[132]: 0 very good1 good2 good3 very good4 very good5 very badName: grade, dtype: categoryCategories (5, object): [very bad, bad, medium, good, very good] 4、 排序是按照 Categorical 的顺序进行的而不是按照字典顺序进行： 123456789In [133]: df.sort_values(by=&quot;grade&quot;)Out[133]: id raw_grade grade5 6 e very bad1 2 b good2 3 b good0 1 a very good3 4 a very good4 5 a very good 5、 对 Categorical 列进行排序时存在空的类别： 123456789In [134]: df.groupby(&quot;grade&quot;).size()Out[134]: gradevery bad 1bad 0medium 0good 2very good 3dtype: int64 十一、 画图具体文档参看：绘图文档。 123456In [135]: ts = pd.Series(np.random.randn(1000), index=pd.date_range(&#x27;1/1/2000&#x27;, periods=1000))In [136]: ts = ts.cumsum()In [137]: ts.plot()Out[137]: &lt;matplotlib.axes._subplots.AxesSubplot at 0x7ff2ab2af550&gt; 对于DataFrame来说，plot是一种将所有列及其标签进行绘制的简便方法： 12345678In [138]: df = pd.DataFrame(np.random.randn(1000, 4), index=ts.index, .....: columns=[&#x27;A&#x27;, &#x27;B&#x27;, &#x27;C&#x27;, &#x27;D&#x27;]) .....: In [139]: df = df.cumsum()In [140]: plt.figure(); df.plot(); plt.legend(loc=&#x27;best&#x27;)Out[140]: &lt;matplotlib.legend.Legend at 0x7ff29c8163d0&gt; 十二、 导入和保存数据CSV参考：写入 CSV 文件。 1、 写入 csv 文件： 1In [141]: df.to_csv(&#x27;foo.csv&#x27;) 2、 从 csv 文件中读取： 1234567891011121314151617181920In [142]: pd.read_csv(&#x27;foo.csv&#x27;)Out[142]: Unnamed: 0 A B C D0 2000-01-01 0.266457 -0.399641 -0.219582 1.1868601 2000-01-02 -1.170732 -0.345873 1.653061 -0.2829532 2000-01-03 -1.734933 0.530468 2.060811 -0.5155363 2000-01-04 -1.555121 1.452620 0.239859 -1.1568964 2000-01-05 0.578117 0.511371 0.103552 -2.4282025 2000-01-06 0.478344 0.449933 -0.741620 -1.9624096 2000-01-07 1.235339 -0.091757 -1.543861 -1.084753.. ... ... ... ... ...993 2002-09-20 -10.628548 -9.153563 -7.883146 28.313940994 2002-09-21 -10.390377 -8.727491 -6.399645 30.914107995 2002-09-22 -8.985362 -8.485624 -4.669462 31.367740996 2002-09-23 -9.558560 -8.781216 -4.499815 30.518439997 2002-09-24 -9.902058 -9.340490 -4.386639 30.105593998 2002-09-25 -10.216020 -9.480682 -3.933802 29.758560999 2002-09-26 -11.856774 -10.671012 -3.216025 29.369368[1000 rows x 5 columns] HDF5参考：HDF5 存储 1、 写入 HDF5 存储： 1In [143]: df.to_hdf(&#x27;foo.h5&#x27;,&#x27;df&#x27;) 2、 从 HDF5 存储中读取： 1234567891011121314151617181920In [144]: pd.read_hdf(&#x27;foo.h5&#x27;,&#x27;df&#x27;)Out[144]: A B C D2000-01-01 0.266457 -0.399641 -0.219582 1.1868602000-01-02 -1.170732 -0.345873 1.653061 -0.2829532000-01-03 -1.734933 0.530468 2.060811 -0.5155362000-01-04 -1.555121 1.452620 0.239859 -1.1568962000-01-05 0.578117 0.511371 0.103552 -2.4282022000-01-06 0.478344 0.449933 -0.741620 -1.9624092000-01-07 1.235339 -0.091757 -1.543861 -1.084753... ... ... ... ...2002-09-20 -10.628548 -9.153563 -7.883146 28.3139402002-09-21 -10.390377 -8.727491 -6.399645 30.9141072002-09-22 -8.985362 -8.485624 -4.669462 31.3677402002-09-23 -9.558560 -8.781216 -4.499815 30.5184392002-09-24 -9.902058 -9.340490 -4.386639 30.1055932002-09-25 -10.216020 -9.480682 -3.933802 29.7585602002-09-26 -11.856774 -10.671012 -3.216025 29.369368[1000 rows x 4 columns] Excel参考：MS Excel 1、 写入excel文件： 1In [145]: df.to_excel(&#x27;foo.xlsx&#x27;, sheet_name=&#x27;Sheet1&#x27;) 2、 从excel文件中读取： 1234567891011121314151617181920In [146]: pd.read_excel(&#x27;foo.xlsx&#x27;, &#x27;Sheet1&#x27;, index_col=None, na_values=[&#x27;NA&#x27;])Out[146]: A B C D2000-01-01 0.266457 -0.399641 -0.219582 1.1868602000-01-02 -1.170732 -0.345873 1.653061 -0.2829532000-01-03 -1.734933 0.530468 2.060811 -0.5155362000-01-04 -1.555121 1.452620 0.239859 -1.1568962000-01-05 0.578117 0.511371 0.103552 -2.4282022000-01-06 0.478344 0.449933 -0.741620 -1.9624092000-01-07 1.235339 -0.091757 -1.543861 -1.084753... ... ... ... ...2002-09-20 -10.628548 -9.153563 -7.883146 28.3139402002-09-21 -10.390377 -8.727491 -6.399645 30.9141072002-09-22 -8.985362 -8.485624 -4.669462 31.3677402002-09-23 -9.558560 -8.781216 -4.499815 30.5184392002-09-24 -9.902058 -9.340490 -4.386639 30.1055932002-09-25 -10.216020 -9.480682 -3.933802 29.7585602002-09-26 -11.856774 -10.671012 -3.216025 29.369368[1000 rows x 4 columns] 十三、陷阱如果你尝试某个操作并且看到如下异常： 12345&gt;&gt;&gt; if pd.Series([False, True, False]): print(&quot;I was true&quot;)Traceback ...ValueError: The truth value of an array is ambiguous. Use a.empty, a.any() or a.all(). 解释及处理方式请见比较。 同时请见陷阱。","categories":[{"name":"Pandas","slug":"Pandas","permalink":"https://qikaile.us/volantis/categories/Pandas/"}],"tags":[{"name":"pandas","slug":"pandas","permalink":"https://qikaile.us/volantis/tags/pandas/"}]},{"title":"Hello World","slug":"hello-world","date":"2020-11-17T07:02:43.539Z","updated":"2020-11-17T08:48:15.669Z","comments":true,"path":"posts/16107.html","link":"","permalink":"https://qikaile.us/volantis/posts/16107.html","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]}],"categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://qikaile.us/volantis/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"photoshop","slug":"photoshop","permalink":"https://qikaile.us/volantis/categories/photoshop/"},{"name":"Pandas","slug":"Pandas","permalink":"https://qikaile.us/volantis/categories/Pandas/"}],"tags":[{"name":"Solidworks","slug":"Solidworks","permalink":"https://qikaile.us/volantis/tags/Solidworks/"},{"name":"English","slug":"English","permalink":"https://qikaile.us/volantis/tags/English/"},{"name":"Office","slug":"Office","permalink":"https://qikaile.us/volantis/tags/Office/"},{"name":"Python","slug":"Python","permalink":"https://qikaile.us/volantis/tags/Python/"},{"name":"机器学习","slug":"机器学习","permalink":"https://qikaile.us/volantis/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"hexo","slug":"hexo","permalink":"https://qikaile.us/volantis/tags/hexo/"},{"name":"photoshop","slug":"photoshop","permalink":"https://qikaile.us/volantis/tags/photoshop/"},{"name":"markdown","slug":"markdown","permalink":"https://qikaile.us/volantis/tags/markdown/"},{"name":"pandas","slug":"pandas","permalink":"https://qikaile.us/volantis/tags/pandas/"}]}